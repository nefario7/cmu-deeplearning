{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nefario7/cmu-deeplearning/blob/working-hw1/hw1p2_s22_starter_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "#!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "MIby3J0IWvkY"
      },
      "id": "MIby3J0IWvkY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        # TODO: Please try different architectures\n",
        "        in_size = 13\n",
        "        layers = [\n",
        "            nn.Linear(in_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 40)\n",
        "        ]\n",
        "        self.laysers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.laysers(A0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Qcny8yCsWxmq"
      },
      "id": "Qcny8yCsWxmq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"dev-clean\", csvpath=None):\n",
        "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
        "        self.sample = sample \n",
        "        \n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        self.Y_dir = data_path + \"/\" + partition +\"/transcript/\"\n",
        "        \n",
        "        self.X_names = os.listdir(self.X_dir)\n",
        "        self.Y_names = os.listdir(self.Y_dir)\n",
        "\n",
        "        # using a small part of the dataset to debug\n",
        "        if csvpath:\n",
        "            subset = self.parse_csv(csvpath)\n",
        "            self.X_names = [i for i in self.X_names if i in subset]\n",
        "            self.Y_names = [i for i in self.Y_names if i in subset]\n",
        "        \n",
        "        if shuffle == True:\n",
        "            XY_names = list(zip(self.X_names, self.Y_names))\n",
        "            random.shuffle(XY_names)\n",
        "            self.X_names, self.Y_names = zip(*XY_names)\n",
        "        \n",
        "        assert(len(self.X_names) == len(self.Y_names))\n",
        "        self.length = len(self.X_names)\n",
        "        \n",
        "        self.PHONEMES = [\n",
        "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n",
        "      \n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[1])\n",
        "        return subset[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.length / self.sample))\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n",
        "        \n",
        "        X, Y = [], []\n",
        "        for j in sample_range:\n",
        "            X_path = self.X_dir + self.X_names[j]\n",
        "            Y_path = self.Y_dir + self.Y_names[j]\n",
        "            \n",
        "            label = [self.PHONEMES.index(yy) for yy in np.load(Y_path)][1:-1]\n",
        "\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "            Y.append(np.array(label))\n",
        "            \n",
        "        X, Y = np.concatenate(X), np.concatenate(Y)\n",
        "        return X, Y\n",
        "    \n",
        "class LibriItems(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y, context = 0):\n",
        "        assert(X.shape[0] == Y.shape[0])\n",
        "        \n",
        "        self.length  = X.shape[0]\n",
        "        self.context = context\n",
        "\n",
        "        if context == 0:\n",
        "            self.X, self.Y = X, Y\n",
        "        else:\n",
        "            X = np.pad(X, ((context,context), (0,0)), 'constant', constant_values=(0,0))\n",
        "            self.X, self.Y = X, Y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        if self.context == 0:\n",
        "            xx = self.X[i].flatten()\n",
        "            yy = self.Y[i]\n",
        "        else:\n",
        "            xx = self.X[i:(i + 2*self.context + 1)].flatten()\n",
        "            yy = self.Y[i]\n",
        "        return xx, yy\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "Jr9VJwbGWxrY"
      },
      "id": "Jr9VJwbGWxrY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbb1d57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cbb1d57",
        "outputId": "69aa0e1f-6918-44e4-ce3b-e96129129774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/10391263 (0%)]\tLoss: 3.726638\n",
            "Train Epoch: 1 [51200/10391263 (0%)]\tLoss: 2.075493\n",
            "Train Epoch: 1 [102400/10391263 (1%)]\tLoss: 1.858368\n",
            "Train Epoch: 1 [153600/10391263 (1%)]\tLoss: 1.768054\n",
            "Train Epoch: 1 [204800/10391263 (2%)]\tLoss: 1.796281\n",
            "Train Epoch: 1 [256000/10391263 (2%)]\tLoss: 1.864937\n",
            "Train Epoch: 1 [307200/10391263 (3%)]\tLoss: 1.853778\n",
            "Train Epoch: 1 [358400/10391263 (3%)]\tLoss: 1.690517\n",
            "Train Epoch: 1 [409600/10391263 (4%)]\tLoss: 1.909193\n",
            "Train Epoch: 1 [460800/10391263 (4%)]\tLoss: 1.657735\n",
            "Train Epoch: 1 [512000/10391263 (5%)]\tLoss: 1.733385\n",
            "Train Epoch: 1 [563200/10391263 (5%)]\tLoss: 1.864222\n",
            "Train Epoch: 1 [614400/10391263 (6%)]\tLoss: 1.813726\n",
            "Train Epoch: 1 [665600/10391263 (6%)]\tLoss: 1.794536\n",
            "Train Epoch: 1 [716800/10391263 (7%)]\tLoss: 1.675392\n",
            "Train Epoch: 1 [768000/10391263 (7%)]\tLoss: 1.662641\n",
            "Train Epoch: 1 [819200/10391263 (8%)]\tLoss: 1.756513\n",
            "Train Epoch: 1 [870400/10391263 (8%)]\tLoss: 1.678574\n",
            "Train Epoch: 1 [921600/10391263 (9%)]\tLoss: 1.769909\n",
            "Train Epoch: 1 [972800/10391263 (9%)]\tLoss: 1.735441\n",
            "Train Epoch: 1 [1024000/10391263 (10%)]\tLoss: 1.802359\n",
            "Train Epoch: 1 [1075200/10391263 (10%)]\tLoss: 1.832820\n",
            "Train Epoch: 1 [1126400/10391263 (11%)]\tLoss: 1.643407\n",
            "Train Epoch: 1 [1177600/10391263 (11%)]\tLoss: 1.740640\n",
            "Train Epoch: 1 [1228800/10391263 (12%)]\tLoss: 1.799420\n",
            "Train Epoch: 1 [1280000/10391263 (12%)]\tLoss: 1.764050\n",
            "Train Epoch: 1 [1331200/10391263 (13%)]\tLoss: 1.805743\n",
            "Train Epoch: 1 [1382400/10391263 (13%)]\tLoss: 1.617332\n",
            "Train Epoch: 1 [1433600/10391263 (14%)]\tLoss: 1.736790\n",
            "Train Epoch: 1 [1484800/10391263 (14%)]\tLoss: 1.834690\n",
            "Train Epoch: 1 [1536000/10391263 (15%)]\tLoss: 1.784818\n",
            "Train Epoch: 1 [1587200/10391263 (15%)]\tLoss: 1.909317\n",
            "Train Epoch: 1 [1638400/10391263 (16%)]\tLoss: 1.845628\n",
            "Train Epoch: 1 [1689600/10391263 (16%)]\tLoss: 1.711614\n",
            "Train Epoch: 1 [1740800/10391263 (17%)]\tLoss: 1.790247\n",
            "Train Epoch: 1 [1792000/10391263 (17%)]\tLoss: 1.670179\n",
            "Train Epoch: 1 [1843200/10391263 (18%)]\tLoss: 1.688747\n",
            "Train Epoch: 1 [1894400/10391263 (18%)]\tLoss: 1.690181\n",
            "Train Epoch: 1 [1945600/10391263 (19%)]\tLoss: 1.812031\n",
            "Train Epoch: 1 [1996800/10391263 (19%)]\tLoss: 1.743074\n",
            "Train Epoch: 1 [2048000/10391263 (20%)]\tLoss: 1.668489\n",
            "Train Epoch: 1 [2099200/10391263 (20%)]\tLoss: 1.568839\n",
            "Train Epoch: 1 [2150400/10391263 (21%)]\tLoss: 1.570342\n",
            "Train Epoch: 1 [2201600/10391263 (21%)]\tLoss: 1.713727\n",
            "Train Epoch: 1 [2252800/10391263 (22%)]\tLoss: 1.785349\n",
            "Train Epoch: 1 [2304000/10391263 (22%)]\tLoss: 1.894308\n",
            "Train Epoch: 1 [2355200/10391263 (23%)]\tLoss: 1.627237\n",
            "Train Epoch: 1 [2406400/10391263 (23%)]\tLoss: 1.782201\n",
            "Train Epoch: 1 [2457600/10391263 (24%)]\tLoss: 1.762797\n",
            "Train Epoch: 1 [2508800/10391263 (24%)]\tLoss: 1.681230\n",
            "Train Epoch: 1 [2560000/10391263 (25%)]\tLoss: 1.684363\n",
            "Train Epoch: 1 [2611200/10391263 (25%)]\tLoss: 1.801907\n",
            "Train Epoch: 1 [2662400/10391263 (26%)]\tLoss: 1.669549\n",
            "Train Epoch: 1 [2713600/10391263 (26%)]\tLoss: 1.624808\n",
            "Train Epoch: 1 [2764800/10391263 (27%)]\tLoss: 1.662816\n",
            "Train Epoch: 1 [2816000/10391263 (27%)]\tLoss: 1.591769\n",
            "Train Epoch: 1 [2867200/10391263 (28%)]\tLoss: 1.773782\n",
            "Train Epoch: 1 [2918400/10391263 (28%)]\tLoss: 1.632358\n",
            "Train Epoch: 1 [2969600/10391263 (29%)]\tLoss: 1.633014\n",
            "Train Epoch: 1 [3020800/10391263 (29%)]\tLoss: 1.606931\n",
            "Train Epoch: 1 [3072000/10391263 (30%)]\tLoss: 1.825984\n",
            "Train Epoch: 1 [3123200/10391263 (30%)]\tLoss: 1.744161\n",
            "Train Epoch: 1 [3174400/10391263 (31%)]\tLoss: 1.726727\n",
            "Train Epoch: 1 [3225600/10391263 (31%)]\tLoss: 1.739616\n",
            "Train Epoch: 1 [3276800/10391263 (32%)]\tLoss: 1.813833\n",
            "Train Epoch: 1 [3328000/10391263 (32%)]\tLoss: 1.590069\n",
            "Train Epoch: 1 [3379200/10391263 (33%)]\tLoss: 1.749185\n",
            "Train Epoch: 1 [3430400/10391263 (33%)]\tLoss: 1.714442\n",
            "Train Epoch: 1 [3481600/10391263 (34%)]\tLoss: 1.834885\n",
            "Train Epoch: 1 [3532800/10391263 (34%)]\tLoss: 1.722487\n",
            "Train Epoch: 1 [3584000/10391263 (34%)]\tLoss: 1.712886\n",
            "Train Epoch: 1 [3635200/10391263 (35%)]\tLoss: 1.697823\n",
            "Train Epoch: 1 [3686400/10391263 (35%)]\tLoss: 1.611050\n",
            "Train Epoch: 1 [3737600/10391263 (36%)]\tLoss: 1.632077\n",
            "Train Epoch: 1 [3788800/10391263 (36%)]\tLoss: 1.576979\n",
            "Train Epoch: 1 [3840000/10391263 (37%)]\tLoss: 1.546636\n",
            "Train Epoch: 1 [3891200/10391263 (37%)]\tLoss: 1.574189\n",
            "Train Epoch: 1 [3942400/10391263 (38%)]\tLoss: 1.777691\n",
            "Train Epoch: 1 [3993600/10391263 (38%)]\tLoss: 1.769894\n",
            "Train Epoch: 1 [4044800/10391263 (39%)]\tLoss: 1.585007\n",
            "Train Epoch: 1 [4096000/10391263 (39%)]\tLoss: 1.587214\n",
            "Train Epoch: 1 [4147200/10391263 (40%)]\tLoss: 1.689225\n",
            "Train Epoch: 1 [4198400/10391263 (40%)]\tLoss: 1.579119\n",
            "Train Epoch: 1 [4249600/10391263 (41%)]\tLoss: 1.551014\n",
            "Train Epoch: 1 [4300800/10391263 (41%)]\tLoss: 1.748072\n",
            "Train Epoch: 1 [4352000/10391263 (42%)]\tLoss: 1.532613\n",
            "Train Epoch: 1 [4403200/10391263 (42%)]\tLoss: 1.738097\n",
            "Train Epoch: 1 [4454400/10391263 (43%)]\tLoss: 1.777391\n",
            "Train Epoch: 1 [4505600/10391263 (43%)]\tLoss: 1.693720\n",
            "Train Epoch: 1 [4556800/10391263 (44%)]\tLoss: 1.639487\n",
            "Train Epoch: 1 [4608000/10391263 (44%)]\tLoss: 1.783309\n",
            "Train Epoch: 1 [4659200/10391263 (45%)]\tLoss: 1.804769\n",
            "Train Epoch: 1 [4710400/10391263 (45%)]\tLoss: 1.741014\n",
            "Train Epoch: 1 [4761600/10391263 (46%)]\tLoss: 1.534952\n",
            "Train Epoch: 1 [4812800/10391263 (46%)]\tLoss: 1.618047\n",
            "Train Epoch: 1 [4864000/10391263 (47%)]\tLoss: 1.807162\n",
            "Train Epoch: 1 [4915200/10391263 (47%)]\tLoss: 1.702928\n",
            "Train Epoch: 1 [4966400/10391263 (48%)]\tLoss: 1.715082\n",
            "Train Epoch: 1 [5017600/10391263 (48%)]\tLoss: 1.602532\n",
            "Train Epoch: 1 [5068800/10391263 (49%)]\tLoss: 1.661303\n",
            "Train Epoch: 1 [5120000/10391263 (49%)]\tLoss: 1.645047\n",
            "Train Epoch: 1 [5171200/10391263 (50%)]\tLoss: 1.602629\n",
            "Train Epoch: 1 [5222400/10391263 (50%)]\tLoss: 1.699181\n",
            "Train Epoch: 1 [5273600/10391263 (51%)]\tLoss: 1.641096\n",
            "Train Epoch: 1 [5324800/10391263 (51%)]\tLoss: 1.755107\n",
            "Train Epoch: 1 [5376000/10391263 (52%)]\tLoss: 1.832147\n",
            "Train Epoch: 1 [5427200/10391263 (52%)]\tLoss: 1.629860\n",
            "Train Epoch: 1 [5478400/10391263 (53%)]\tLoss: 1.800217\n",
            "Train Epoch: 1 [5529600/10391263 (53%)]\tLoss: 1.816029\n",
            "Train Epoch: 1 [5580800/10391263 (54%)]\tLoss: 1.604486\n",
            "Train Epoch: 1 [5632000/10391263 (54%)]\tLoss: 1.576970\n",
            "Train Epoch: 1 [5683200/10391263 (55%)]\tLoss: 1.881002\n",
            "Train Epoch: 1 [5734400/10391263 (55%)]\tLoss: 1.600324\n",
            "Train Epoch: 1 [5785600/10391263 (56%)]\tLoss: 1.730900\n",
            "Train Epoch: 1 [5836800/10391263 (56%)]\tLoss: 1.490770\n",
            "Train Epoch: 1 [5888000/10391263 (57%)]\tLoss: 1.931524\n",
            "Train Epoch: 1 [5939200/10391263 (57%)]\tLoss: 1.575988\n",
            "Train Epoch: 1 [5990400/10391263 (58%)]\tLoss: 1.889222\n",
            "Train Epoch: 1 [6041600/10391263 (58%)]\tLoss: 1.664445\n",
            "Train Epoch: 1 [6092800/10391263 (59%)]\tLoss: 1.758896\n",
            "Train Epoch: 1 [6144000/10391263 (59%)]\tLoss: 1.653795\n",
            "Train Epoch: 1 [6195200/10391263 (60%)]\tLoss: 1.606630\n",
            "Train Epoch: 1 [6246400/10391263 (60%)]\tLoss: 1.856061\n",
            "Train Epoch: 1 [6297600/10391263 (61%)]\tLoss: 1.596773\n",
            "Train Epoch: 1 [6348800/10391263 (61%)]\tLoss: 1.918110\n",
            "Train Epoch: 1 [6400000/10391263 (62%)]\tLoss: 1.728862\n",
            "Train Epoch: 1 [6451200/10391263 (62%)]\tLoss: 1.485870\n",
            "Train Epoch: 1 [6502400/10391263 (63%)]\tLoss: 1.678383\n",
            "Train Epoch: 1 [6553600/10391263 (63%)]\tLoss: 1.662290\n",
            "Train Epoch: 1 [6604800/10391263 (64%)]\tLoss: 1.623866\n",
            "Train Epoch: 1 [6656000/10391263 (64%)]\tLoss: 1.727622\n",
            "Train Epoch: 1 [6707200/10391263 (65%)]\tLoss: 1.542019\n",
            "Train Epoch: 1 [6758400/10391263 (65%)]\tLoss: 1.635771\n",
            "Train Epoch: 1 [6809600/10391263 (66%)]\tLoss: 1.600338\n",
            "Train Epoch: 1 [6860800/10391263 (66%)]\tLoss: 1.733433\n",
            "Train Epoch: 1 [6912000/10391263 (67%)]\tLoss: 1.694375\n",
            "Train Epoch: 1 [6963200/10391263 (67%)]\tLoss: 1.545951\n",
            "Train Epoch: 1 [7014400/10391263 (68%)]\tLoss: 1.683681\n",
            "Train Epoch: 1 [7065600/10391263 (68%)]\tLoss: 1.611564\n",
            "Train Epoch: 1 [7116800/10391263 (68%)]\tLoss: 1.546339\n",
            "Train Epoch: 1 [7168000/10391263 (69%)]\tLoss: 1.687300\n",
            "Train Epoch: 1 [7219200/10391263 (69%)]\tLoss: 1.654383\n",
            "Train Epoch: 1 [7270400/10391263 (70%)]\tLoss: 1.624250\n",
            "Train Epoch: 1 [7321600/10391263 (70%)]\tLoss: 1.714810\n",
            "Train Epoch: 1 [7372800/10391263 (71%)]\tLoss: 1.510064\n",
            "Train Epoch: 1 [7424000/10391263 (71%)]\tLoss: 1.656575\n",
            "Train Epoch: 1 [7475200/10391263 (72%)]\tLoss: 1.634346\n",
            "Train Epoch: 1 [7526400/10391263 (72%)]\tLoss: 1.685298\n",
            "Train Epoch: 1 [7577600/10391263 (73%)]\tLoss: 1.843651\n",
            "Train Epoch: 1 [7628800/10391263 (73%)]\tLoss: 1.573869\n",
            "Train Epoch: 1 [7680000/10391263 (74%)]\tLoss: 1.664956\n",
            "Train Epoch: 1 [7731200/10391263 (74%)]\tLoss: 1.761414\n",
            "Train Epoch: 1 [7782400/10391263 (75%)]\tLoss: 1.578559\n",
            "Train Epoch: 1 [7833600/10391263 (75%)]\tLoss: 1.607014\n",
            "Train Epoch: 1 [7884800/10391263 (76%)]\tLoss: 1.710611\n",
            "Train Epoch: 1 [7936000/10391263 (76%)]\tLoss: 1.830767\n",
            "Train Epoch: 1 [7987200/10391263 (77%)]\tLoss: 1.607758\n",
            "Train Epoch: 1 [8038400/10391263 (77%)]\tLoss: 1.763847\n",
            "Train Epoch: 1 [8089600/10391263 (78%)]\tLoss: 1.685859\n",
            "Train Epoch: 1 [8140800/10391263 (78%)]\tLoss: 1.633336\n",
            "Train Epoch: 1 [8192000/10391263 (79%)]\tLoss: 1.673771\n",
            "Train Epoch: 1 [8243200/10391263 (79%)]\tLoss: 1.893299\n",
            "Train Epoch: 1 [8294400/10391263 (80%)]\tLoss: 1.604092\n",
            "Train Epoch: 1 [8345600/10391263 (80%)]\tLoss: 1.733035\n",
            "Train Epoch: 1 [8396800/10391263 (81%)]\tLoss: 1.723568\n",
            "Train Epoch: 1 [8448000/10391263 (81%)]\tLoss: 1.669383\n",
            "Train Epoch: 1 [8499200/10391263 (82%)]\tLoss: 1.597581\n",
            "Train Epoch: 1 [8550400/10391263 (82%)]\tLoss: 1.734054\n",
            "Train Epoch: 1 [8601600/10391263 (83%)]\tLoss: 1.703677\n",
            "Train Epoch: 1 [8652800/10391263 (83%)]\tLoss: 1.672989\n",
            "Train Epoch: 1 [8704000/10391263 (84%)]\tLoss: 1.740601\n",
            "Train Epoch: 1 [8755200/10391263 (84%)]\tLoss: 1.864993\n",
            "Train Epoch: 1 [8806400/10391263 (85%)]\tLoss: 1.607559\n",
            "Train Epoch: 1 [8857600/10391263 (85%)]\tLoss: 1.661143\n",
            "Train Epoch: 1 [8908800/10391263 (86%)]\tLoss: 1.614416\n",
            "Train Epoch: 1 [8960000/10391263 (86%)]\tLoss: 1.731280\n",
            "Train Epoch: 1 [9011200/10391263 (87%)]\tLoss: 1.806628\n",
            "Train Epoch: 1 [9062400/10391263 (87%)]\tLoss: 1.705702\n",
            "Train Epoch: 1 [9113600/10391263 (88%)]\tLoss: 1.620710\n",
            "Train Epoch: 1 [9164800/10391263 (88%)]\tLoss: 1.653126\n",
            "Train Epoch: 1 [9216000/10391263 (89%)]\tLoss: 1.890628\n",
            "Train Epoch: 1 [9267200/10391263 (89%)]\tLoss: 1.842705\n",
            "Train Epoch: 1 [9318400/10391263 (90%)]\tLoss: 1.563026\n",
            "Train Epoch: 1 [9369600/10391263 (90%)]\tLoss: 1.579721\n",
            "Train Epoch: 1 [9420800/10391263 (91%)]\tLoss: 1.737564\n",
            "Train Epoch: 1 [9472000/10391263 (91%)]\tLoss: 1.730460\n",
            "Train Epoch: 1 [9523200/10391263 (92%)]\tLoss: 1.683405\n",
            "Train Epoch: 1 [9574400/10391263 (92%)]\tLoss: 1.612185\n",
            "Train Epoch: 1 [9625600/10391263 (93%)]\tLoss: 1.705080\n",
            "Train Epoch: 1 [9676800/10391263 (93%)]\tLoss: 1.812533\n",
            "Train Epoch: 1 [9728000/10391263 (94%)]\tLoss: 1.772747\n",
            "Train Epoch: 1 [9779200/10391263 (94%)]\tLoss: 1.653934\n",
            "Train Epoch: 1 [9830400/10391263 (95%)]\tLoss: 1.701191\n",
            "Train Epoch: 1 [9881600/10391263 (95%)]\tLoss: 1.456582\n",
            "Train Epoch: 1 [9932800/10391263 (96%)]\tLoss: 1.624718\n",
            "Train Epoch: 1 [9984000/10391263 (96%)]\tLoss: 1.745144\n",
            "Train Epoch: 1 [10035200/10391263 (97%)]\tLoss: 1.829946\n",
            "Train Epoch: 1 [10086400/10391263 (97%)]\tLoss: 1.577916\n",
            "Train Epoch: 1 [10137600/10391263 (98%)]\tLoss: 1.802087\n",
            "Train Epoch: 1 [10188800/10391263 (98%)]\tLoss: 1.857554\n",
            "Train Epoch: 1 [10240000/10391263 (99%)]\tLoss: 1.703509\n",
            "Train Epoch: 1 [10291200/10391263 (99%)]\tLoss: 1.696022\n",
            "Train Epoch: 1 [10342400/10391263 (100%)]\tLoss: 1.807674\n",
            "Dev accuracy  0.48746010314343874\n",
            "Train Epoch: 2 [0/10391263 (0%)]\tLoss: 1.596619\n",
            "Train Epoch: 2 [51200/10391263 (0%)]\tLoss: 1.744579\n",
            "Train Epoch: 2 [102400/10391263 (1%)]\tLoss: 1.683039\n",
            "Train Epoch: 2 [153600/10391263 (1%)]\tLoss: 1.651369\n",
            "Train Epoch: 2 [204800/10391263 (2%)]\tLoss: 1.753997\n",
            "Train Epoch: 2 [256000/10391263 (2%)]\tLoss: 1.601907\n",
            "Train Epoch: 2 [307200/10391263 (3%)]\tLoss: 1.640826\n",
            "Train Epoch: 2 [358400/10391263 (3%)]\tLoss: 1.572291\n",
            "Train Epoch: 2 [409600/10391263 (4%)]\tLoss: 1.680817\n",
            "Train Epoch: 2 [460800/10391263 (4%)]\tLoss: 1.815868\n",
            "Train Epoch: 2 [512000/10391263 (5%)]\tLoss: 1.696988\n",
            "Train Epoch: 2 [563200/10391263 (5%)]\tLoss: 1.703860\n",
            "Train Epoch: 2 [614400/10391263 (6%)]\tLoss: 1.568973\n",
            "Train Epoch: 2 [665600/10391263 (6%)]\tLoss: 1.682860\n",
            "Train Epoch: 2 [716800/10391263 (7%)]\tLoss: 1.795216\n",
            "Train Epoch: 2 [768000/10391263 (7%)]\tLoss: 1.622587\n",
            "Train Epoch: 2 [819200/10391263 (8%)]\tLoss: 1.795785\n",
            "Train Epoch: 2 [870400/10391263 (8%)]\tLoss: 1.596648\n",
            "Train Epoch: 2 [921600/10391263 (9%)]\tLoss: 1.586924\n",
            "Train Epoch: 2 [972800/10391263 (9%)]\tLoss: 1.670648\n",
            "Train Epoch: 2 [1024000/10391263 (10%)]\tLoss: 1.665645\n",
            "Train Epoch: 2 [1075200/10391263 (10%)]\tLoss: 1.691711\n",
            "Train Epoch: 2 [1126400/10391263 (11%)]\tLoss: 1.739375\n",
            "Train Epoch: 2 [1177600/10391263 (11%)]\tLoss: 1.622870\n",
            "Train Epoch: 2 [1228800/10391263 (12%)]\tLoss: 1.723954\n",
            "Train Epoch: 2 [1280000/10391263 (12%)]\tLoss: 1.510508\n",
            "Train Epoch: 2 [1331200/10391263 (13%)]\tLoss: 1.506788\n",
            "Train Epoch: 2 [1382400/10391263 (13%)]\tLoss: 1.561537\n",
            "Train Epoch: 2 [1433600/10391263 (14%)]\tLoss: 1.593916\n",
            "Train Epoch: 2 [1484800/10391263 (14%)]\tLoss: 1.712973\n",
            "Train Epoch: 2 [1536000/10391263 (15%)]\tLoss: 1.724256\n",
            "Train Epoch: 2 [1587200/10391263 (15%)]\tLoss: 1.563340\n",
            "Train Epoch: 2 [1638400/10391263 (16%)]\tLoss: 1.811371\n",
            "Train Epoch: 2 [1689600/10391263 (16%)]\tLoss: 1.574328\n",
            "Train Epoch: 2 [1740800/10391263 (17%)]\tLoss: 1.596165\n",
            "Train Epoch: 2 [1792000/10391263 (17%)]\tLoss: 1.599403\n",
            "Train Epoch: 2 [1843200/10391263 (18%)]\tLoss: 1.545994\n",
            "Train Epoch: 2 [1894400/10391263 (18%)]\tLoss: 1.877110\n",
            "Train Epoch: 2 [1945600/10391263 (19%)]\tLoss: 1.761662\n",
            "Train Epoch: 2 [1996800/10391263 (19%)]\tLoss: 1.669621\n",
            "Train Epoch: 2 [2048000/10391263 (20%)]\tLoss: 1.661670\n",
            "Train Epoch: 2 [2099200/10391263 (20%)]\tLoss: 1.676639\n",
            "Train Epoch: 2 [2150400/10391263 (21%)]\tLoss: 1.573185\n",
            "Train Epoch: 2 [2201600/10391263 (21%)]\tLoss: 1.586726\n",
            "Train Epoch: 2 [2252800/10391263 (22%)]\tLoss: 1.798626\n",
            "Train Epoch: 2 [2304000/10391263 (22%)]\tLoss: 1.578697\n",
            "Train Epoch: 2 [2355200/10391263 (23%)]\tLoss: 1.557167\n",
            "Train Epoch: 2 [2406400/10391263 (23%)]\tLoss: 1.613519\n",
            "Train Epoch: 2 [2457600/10391263 (24%)]\tLoss: 1.724679\n",
            "Train Epoch: 2 [2508800/10391263 (24%)]\tLoss: 1.788645\n",
            "Train Epoch: 2 [2560000/10391263 (25%)]\tLoss: 1.625475\n",
            "Train Epoch: 2 [2611200/10391263 (25%)]\tLoss: 1.845339\n",
            "Train Epoch: 2 [2662400/10391263 (26%)]\tLoss: 1.697906\n",
            "Train Epoch: 2 [2713600/10391263 (26%)]\tLoss: 1.690908\n",
            "Train Epoch: 2 [2764800/10391263 (27%)]\tLoss: 1.637975\n",
            "Train Epoch: 2 [2816000/10391263 (27%)]\tLoss: 1.632758\n",
            "Train Epoch: 2 [2867200/10391263 (28%)]\tLoss: 1.798117\n",
            "Train Epoch: 2 [2918400/10391263 (28%)]\tLoss: 1.487101\n",
            "Train Epoch: 2 [2969600/10391263 (29%)]\tLoss: 1.657048\n",
            "Train Epoch: 2 [3020800/10391263 (29%)]\tLoss: 1.636945\n",
            "Train Epoch: 2 [3072000/10391263 (30%)]\tLoss: 1.695993\n",
            "Train Epoch: 2 [3123200/10391263 (30%)]\tLoss: 1.824394\n",
            "Train Epoch: 2 [3174400/10391263 (31%)]\tLoss: 1.620055\n",
            "Train Epoch: 2 [3225600/10391263 (31%)]\tLoss: 1.646240\n",
            "Train Epoch: 2 [3276800/10391263 (32%)]\tLoss: 1.697805\n",
            "Train Epoch: 2 [3328000/10391263 (32%)]\tLoss: 1.611652\n",
            "Train Epoch: 2 [3379200/10391263 (33%)]\tLoss: 1.786996\n",
            "Train Epoch: 2 [3430400/10391263 (33%)]\tLoss: 1.821608\n",
            "Train Epoch: 2 [3481600/10391263 (34%)]\tLoss: 1.443871\n",
            "Train Epoch: 2 [3532800/10391263 (34%)]\tLoss: 1.568465\n",
            "Train Epoch: 2 [3584000/10391263 (34%)]\tLoss: 1.674643\n",
            "Train Epoch: 2 [3635200/10391263 (35%)]\tLoss: 1.652432\n",
            "Train Epoch: 2 [3686400/10391263 (35%)]\tLoss: 1.646433\n",
            "Train Epoch: 2 [3737600/10391263 (36%)]\tLoss: 1.605828\n",
            "Train Epoch: 2 [3788800/10391263 (36%)]\tLoss: 1.653404\n",
            "Train Epoch: 2 [3840000/10391263 (37%)]\tLoss: 1.718614\n",
            "Train Epoch: 2 [3891200/10391263 (37%)]\tLoss: 1.766863\n",
            "Train Epoch: 2 [3942400/10391263 (38%)]\tLoss: 1.766899\n",
            "Train Epoch: 2 [3993600/10391263 (38%)]\tLoss: 1.777874\n",
            "Train Epoch: 2 [4044800/10391263 (39%)]\tLoss: 1.702139\n",
            "Train Epoch: 2 [4096000/10391263 (39%)]\tLoss: 1.688639\n",
            "Train Epoch: 2 [4147200/10391263 (40%)]\tLoss: 1.670058\n",
            "Train Epoch: 2 [4198400/10391263 (40%)]\tLoss: 1.752070\n",
            "Train Epoch: 2 [4249600/10391263 (41%)]\tLoss: 1.802993\n",
            "Train Epoch: 2 [4300800/10391263 (41%)]\tLoss: 1.693140\n",
            "Train Epoch: 2 [4352000/10391263 (42%)]\tLoss: 1.493505\n",
            "Train Epoch: 2 [4403200/10391263 (42%)]\tLoss: 1.523471\n",
            "Train Epoch: 2 [4454400/10391263 (43%)]\tLoss: 1.699822\n",
            "Train Epoch: 2 [4505600/10391263 (43%)]\tLoss: 1.617606\n",
            "Train Epoch: 2 [4556800/10391263 (44%)]\tLoss: 1.695725\n",
            "Train Epoch: 2 [4608000/10391263 (44%)]\tLoss: 1.663094\n",
            "Train Epoch: 2 [4659200/10391263 (45%)]\tLoss: 1.478473\n",
            "Train Epoch: 2 [4710400/10391263 (45%)]\tLoss: 1.580717\n",
            "Train Epoch: 2 [4761600/10391263 (46%)]\tLoss: 1.700928\n",
            "Train Epoch: 2 [4812800/10391263 (46%)]\tLoss: 1.691839\n",
            "Train Epoch: 2 [4864000/10391263 (47%)]\tLoss: 1.684584\n",
            "Train Epoch: 2 [4915200/10391263 (47%)]\tLoss: 1.818703\n",
            "Train Epoch: 2 [4966400/10391263 (48%)]\tLoss: 1.697692\n",
            "Train Epoch: 2 [5017600/10391263 (48%)]\tLoss: 1.875529\n",
            "Train Epoch: 2 [5068800/10391263 (49%)]\tLoss: 1.685460\n",
            "Train Epoch: 2 [5120000/10391263 (49%)]\tLoss: 1.553746\n",
            "Train Epoch: 2 [5171200/10391263 (50%)]\tLoss: 1.731365\n",
            "Train Epoch: 2 [5222400/10391263 (50%)]\tLoss: 1.528109\n",
            "Train Epoch: 2 [5273600/10391263 (51%)]\tLoss: 1.706846\n",
            "Train Epoch: 2 [5324800/10391263 (51%)]\tLoss: 1.671758\n",
            "Train Epoch: 2 [5376000/10391263 (52%)]\tLoss: 1.708393\n",
            "Train Epoch: 2 [5427200/10391263 (52%)]\tLoss: 1.664444\n",
            "Train Epoch: 2 [5478400/10391263 (53%)]\tLoss: 1.643728\n",
            "Train Epoch: 2 [5529600/10391263 (53%)]\tLoss: 1.703568\n",
            "Train Epoch: 2 [5580800/10391263 (54%)]\tLoss: 1.795404\n",
            "Train Epoch: 2 [5632000/10391263 (54%)]\tLoss: 1.654294\n",
            "Train Epoch: 2 [5683200/10391263 (55%)]\tLoss: 1.709031\n",
            "Train Epoch: 2 [5734400/10391263 (55%)]\tLoss: 1.597990\n",
            "Train Epoch: 2 [5785600/10391263 (56%)]\tLoss: 1.784379\n",
            "Train Epoch: 2 [5836800/10391263 (56%)]\tLoss: 1.706336\n",
            "Train Epoch: 2 [5888000/10391263 (57%)]\tLoss: 1.689435\n",
            "Train Epoch: 2 [5939200/10391263 (57%)]\tLoss: 1.749252\n",
            "Train Epoch: 2 [5990400/10391263 (58%)]\tLoss: 1.677933\n",
            "Train Epoch: 2 [6041600/10391263 (58%)]\tLoss: 1.653767\n",
            "Train Epoch: 2 [6092800/10391263 (59%)]\tLoss: 1.529481\n",
            "Train Epoch: 2 [6144000/10391263 (59%)]\tLoss: 1.517334\n",
            "Train Epoch: 2 [6195200/10391263 (60%)]\tLoss: 1.716097\n",
            "Train Epoch: 2 [6246400/10391263 (60%)]\tLoss: 1.608389\n",
            "Train Epoch: 2 [6297600/10391263 (61%)]\tLoss: 1.694126\n",
            "Train Epoch: 2 [6348800/10391263 (61%)]\tLoss: 1.709732\n",
            "Train Epoch: 2 [6400000/10391263 (62%)]\tLoss: 1.609254\n",
            "Train Epoch: 2 [6451200/10391263 (62%)]\tLoss: 1.723698\n",
            "Train Epoch: 2 [6502400/10391263 (63%)]\tLoss: 1.740387\n",
            "Train Epoch: 2 [6553600/10391263 (63%)]\tLoss: 1.805085\n",
            "Train Epoch: 2 [6604800/10391263 (64%)]\tLoss: 1.599718\n",
            "Train Epoch: 2 [6656000/10391263 (64%)]\tLoss: 1.603946\n",
            "Train Epoch: 2 [6707200/10391263 (65%)]\tLoss: 1.492296\n",
            "Train Epoch: 2 [6758400/10391263 (65%)]\tLoss: 1.476953\n",
            "Train Epoch: 2 [6809600/10391263 (66%)]\tLoss: 1.598998\n",
            "Train Epoch: 2 [6860800/10391263 (66%)]\tLoss: 1.561869\n",
            "Train Epoch: 2 [6912000/10391263 (67%)]\tLoss: 1.623695\n",
            "Train Epoch: 2 [6963200/10391263 (67%)]\tLoss: 1.651571\n",
            "Train Epoch: 2 [7014400/10391263 (68%)]\tLoss: 1.696530\n",
            "Train Epoch: 2 [7065600/10391263 (68%)]\tLoss: 1.659224\n",
            "Train Epoch: 2 [7116800/10391263 (68%)]\tLoss: 1.632127\n",
            "Train Epoch: 2 [7168000/10391263 (69%)]\tLoss: 1.697976\n",
            "Train Epoch: 2 [7219200/10391263 (69%)]\tLoss: 1.661479\n",
            "Train Epoch: 2 [7270400/10391263 (70%)]\tLoss: 1.691868\n",
            "Train Epoch: 2 [7321600/10391263 (70%)]\tLoss: 1.618493\n",
            "Train Epoch: 2 [7372800/10391263 (71%)]\tLoss: 1.853628\n",
            "Train Epoch: 2 [7424000/10391263 (71%)]\tLoss: 1.638617\n",
            "Train Epoch: 2 [7475200/10391263 (72%)]\tLoss: 1.773438\n",
            "Train Epoch: 2 [7526400/10391263 (72%)]\tLoss: 1.754388\n",
            "Train Epoch: 2 [7577600/10391263 (73%)]\tLoss: 1.559196\n",
            "Train Epoch: 2 [7628800/10391263 (73%)]\tLoss: 1.751643\n",
            "Train Epoch: 2 [7680000/10391263 (74%)]\tLoss: 1.574134\n",
            "Train Epoch: 2 [7731200/10391263 (74%)]\tLoss: 1.452224\n",
            "Train Epoch: 2 [7782400/10391263 (75%)]\tLoss: 1.863363\n",
            "Train Epoch: 2 [7833600/10391263 (75%)]\tLoss: 1.799986\n",
            "Train Epoch: 2 [7884800/10391263 (76%)]\tLoss: 1.809672\n",
            "Train Epoch: 2 [7936000/10391263 (76%)]\tLoss: 1.825353\n",
            "Train Epoch: 2 [7987200/10391263 (77%)]\tLoss: 1.596017\n",
            "Train Epoch: 2 [8038400/10391263 (77%)]\tLoss: 1.668221\n",
            "Train Epoch: 2 [8089600/10391263 (78%)]\tLoss: 1.494689\n",
            "Train Epoch: 2 [8140800/10391263 (78%)]\tLoss: 1.653170\n",
            "Train Epoch: 2 [8192000/10391263 (79%)]\tLoss: 1.730028\n",
            "Train Epoch: 2 [8243200/10391263 (79%)]\tLoss: 1.589431\n",
            "Train Epoch: 2 [8294400/10391263 (80%)]\tLoss: 1.579901\n",
            "Train Epoch: 2 [8345600/10391263 (80%)]\tLoss: 1.849905\n",
            "Train Epoch: 2 [8396800/10391263 (81%)]\tLoss: 1.834708\n",
            "Train Epoch: 2 [8448000/10391263 (81%)]\tLoss: 1.729066\n",
            "Train Epoch: 2 [8499200/10391263 (82%)]\tLoss: 1.772132\n",
            "Train Epoch: 2 [8550400/10391263 (82%)]\tLoss: 1.658714\n",
            "Train Epoch: 2 [8601600/10391263 (83%)]\tLoss: 1.729069\n",
            "Train Epoch: 2 [8652800/10391263 (83%)]\tLoss: 1.834998\n",
            "Train Epoch: 2 [8704000/10391263 (84%)]\tLoss: 1.739503\n",
            "Train Epoch: 2 [8755200/10391263 (84%)]\tLoss: 1.573836\n",
            "Train Epoch: 2 [8806400/10391263 (85%)]\tLoss: 1.590985\n",
            "Train Epoch: 2 [8857600/10391263 (85%)]\tLoss: 1.595387\n",
            "Train Epoch: 2 [8908800/10391263 (86%)]\tLoss: 1.726998\n",
            "Train Epoch: 2 [8960000/10391263 (86%)]\tLoss: 1.576791\n",
            "Train Epoch: 2 [9011200/10391263 (87%)]\tLoss: 1.650969\n",
            "Train Epoch: 2 [9062400/10391263 (87%)]\tLoss: 1.736846\n",
            "Train Epoch: 2 [9113600/10391263 (88%)]\tLoss: 1.679157\n",
            "Train Epoch: 2 [9164800/10391263 (88%)]\tLoss: 1.593204\n",
            "Train Epoch: 2 [9216000/10391263 (89%)]\tLoss: 1.730090\n",
            "Train Epoch: 2 [9267200/10391263 (89%)]\tLoss: 1.568954\n",
            "Train Epoch: 2 [9318400/10391263 (90%)]\tLoss: 1.629462\n",
            "Train Epoch: 2 [9369600/10391263 (90%)]\tLoss: 1.717353\n",
            "Train Epoch: 2 [9420800/10391263 (91%)]\tLoss: 1.752316\n",
            "Train Epoch: 2 [9472000/10391263 (91%)]\tLoss: 1.674617\n",
            "Train Epoch: 2 [9523200/10391263 (92%)]\tLoss: 1.889590\n",
            "Train Epoch: 2 [9574400/10391263 (92%)]\tLoss: 1.574383\n",
            "Train Epoch: 2 [9625600/10391263 (93%)]\tLoss: 1.703073\n",
            "Train Epoch: 2 [9676800/10391263 (93%)]\tLoss: 1.841116\n",
            "Train Epoch: 2 [9728000/10391263 (94%)]\tLoss: 1.614269\n",
            "Train Epoch: 2 [9779200/10391263 (94%)]\tLoss: 1.476786\n",
            "Train Epoch: 2 [9830400/10391263 (95%)]\tLoss: 1.523181\n",
            "Train Epoch: 2 [9881600/10391263 (95%)]\tLoss: 1.599151\n",
            "Train Epoch: 2 [9932800/10391263 (96%)]\tLoss: 1.559852\n",
            "Train Epoch: 2 [9984000/10391263 (96%)]\tLoss: 1.823445\n",
            "Train Epoch: 2 [10035200/10391263 (97%)]\tLoss: 1.609175\n",
            "Train Epoch: 2 [10086400/10391263 (97%)]\tLoss: 1.705333\n",
            "Train Epoch: 2 [10137600/10391263 (98%)]\tLoss: 1.732322\n",
            "Train Epoch: 2 [10188800/10391263 (98%)]\tLoss: 1.654069\n",
            "Train Epoch: 2 [10240000/10391263 (99%)]\tLoss: 1.774049\n",
            "Train Epoch: 2 [10291200/10391263 (99%)]\tLoss: 1.692441\n",
            "Train Epoch: 2 [10342400/10391263 (100%)]\tLoss: 1.629753\n",
            "Dev accuracy  0.4887473316073943\n",
            "Train Epoch: 3 [0/10391263 (0%)]\tLoss: 1.637315\n",
            "Train Epoch: 3 [51200/10391263 (0%)]\tLoss: 1.588460\n",
            "Train Epoch: 3 [102400/10391263 (1%)]\tLoss: 1.793806\n",
            "Train Epoch: 3 [153600/10391263 (1%)]\tLoss: 1.618197\n",
            "Train Epoch: 3 [204800/10391263 (2%)]\tLoss: 1.876496\n",
            "Train Epoch: 3 [256000/10391263 (2%)]\tLoss: 1.660865\n",
            "Train Epoch: 3 [307200/10391263 (3%)]\tLoss: 1.648302\n",
            "Train Epoch: 3 [358400/10391263 (3%)]\tLoss: 1.627120\n",
            "Train Epoch: 3 [409600/10391263 (4%)]\tLoss: 1.649721\n",
            "Train Epoch: 3 [460800/10391263 (4%)]\tLoss: 1.792189\n",
            "Train Epoch: 3 [512000/10391263 (5%)]\tLoss: 1.572809\n",
            "Train Epoch: 3 [563200/10391263 (5%)]\tLoss: 1.584911\n",
            "Train Epoch: 3 [614400/10391263 (6%)]\tLoss: 1.591055\n",
            "Train Epoch: 3 [665600/10391263 (6%)]\tLoss: 1.461111\n",
            "Train Epoch: 3 [716800/10391263 (7%)]\tLoss: 1.675778\n",
            "Train Epoch: 3 [768000/10391263 (7%)]\tLoss: 1.742570\n",
            "Train Epoch: 3 [819200/10391263 (8%)]\tLoss: 1.722782\n",
            "Train Epoch: 3 [870400/10391263 (8%)]\tLoss: 1.616318\n",
            "Train Epoch: 3 [921600/10391263 (9%)]\tLoss: 1.608739\n",
            "Train Epoch: 3 [972800/10391263 (9%)]\tLoss: 1.681800\n",
            "Train Epoch: 3 [1024000/10391263 (10%)]\tLoss: 1.484831\n",
            "Train Epoch: 3 [1075200/10391263 (10%)]\tLoss: 1.680008\n",
            "Train Epoch: 3 [1126400/10391263 (11%)]\tLoss: 1.644250\n",
            "Train Epoch: 3 [1177600/10391263 (11%)]\tLoss: 1.580110\n",
            "Train Epoch: 3 [1228800/10391263 (12%)]\tLoss: 1.761103\n",
            "Train Epoch: 3 [1280000/10391263 (12%)]\tLoss: 1.813122\n",
            "Train Epoch: 3 [1331200/10391263 (13%)]\tLoss: 1.611858\n",
            "Train Epoch: 3 [1382400/10391263 (13%)]\tLoss: 1.688826\n",
            "Train Epoch: 3 [1433600/10391263 (14%)]\tLoss: 1.628649\n",
            "Train Epoch: 3 [1484800/10391263 (14%)]\tLoss: 1.832238\n",
            "Train Epoch: 3 [1536000/10391263 (15%)]\tLoss: 1.718003\n",
            "Train Epoch: 3 [1587200/10391263 (15%)]\tLoss: 1.816115\n",
            "Train Epoch: 3 [1638400/10391263 (16%)]\tLoss: 1.661040\n",
            "Train Epoch: 3 [1689600/10391263 (16%)]\tLoss: 1.656671\n",
            "Train Epoch: 3 [1740800/10391263 (17%)]\tLoss: 1.817448\n",
            "Train Epoch: 3 [1792000/10391263 (17%)]\tLoss: 1.805900\n",
            "Train Epoch: 3 [1843200/10391263 (18%)]\tLoss: 1.749906\n",
            "Train Epoch: 3 [1894400/10391263 (18%)]\tLoss: 1.539610\n",
            "Train Epoch: 3 [1945600/10391263 (19%)]\tLoss: 1.723960\n",
            "Train Epoch: 3 [1996800/10391263 (19%)]\tLoss: 1.735455\n",
            "Train Epoch: 3 [2048000/10391263 (20%)]\tLoss: 1.765528\n",
            "Train Epoch: 3 [2099200/10391263 (20%)]\tLoss: 1.675884\n",
            "Train Epoch: 3 [2150400/10391263 (21%)]\tLoss: 1.717985\n",
            "Train Epoch: 3 [2201600/10391263 (21%)]\tLoss: 1.592256\n",
            "Train Epoch: 3 [2252800/10391263 (22%)]\tLoss: 1.671695\n",
            "Train Epoch: 3 [2304000/10391263 (22%)]\tLoss: 1.659286\n",
            "Train Epoch: 3 [2355200/10391263 (23%)]\tLoss: 1.469947\n",
            "Train Epoch: 3 [2406400/10391263 (23%)]\tLoss: 1.727172\n",
            "Train Epoch: 3 [2457600/10391263 (24%)]\tLoss: 1.727733\n",
            "Train Epoch: 3 [2508800/10391263 (24%)]\tLoss: 1.571777\n",
            "Train Epoch: 3 [2560000/10391263 (25%)]\tLoss: 1.613997\n",
            "Train Epoch: 3 [2611200/10391263 (25%)]\tLoss: 1.583223\n",
            "Train Epoch: 3 [2662400/10391263 (26%)]\tLoss: 1.641234\n",
            "Train Epoch: 3 [2713600/10391263 (26%)]\tLoss: 1.840715\n",
            "Train Epoch: 3 [2764800/10391263 (27%)]\tLoss: 1.580349\n",
            "Train Epoch: 3 [2816000/10391263 (27%)]\tLoss: 1.677332\n",
            "Train Epoch: 3 [2867200/10391263 (28%)]\tLoss: 1.716014\n",
            "Train Epoch: 3 [2918400/10391263 (28%)]\tLoss: 1.807723\n",
            "Train Epoch: 3 [2969600/10391263 (29%)]\tLoss: 1.617788\n",
            "Train Epoch: 3 [3020800/10391263 (29%)]\tLoss: 1.704868\n",
            "Train Epoch: 3 [3072000/10391263 (30%)]\tLoss: 1.620891\n",
            "Train Epoch: 3 [3123200/10391263 (30%)]\tLoss: 1.632491\n",
            "Train Epoch: 3 [3174400/10391263 (31%)]\tLoss: 1.628148\n",
            "Train Epoch: 3 [3225600/10391263 (31%)]\tLoss: 1.771491\n",
            "Train Epoch: 3 [3276800/10391263 (32%)]\tLoss: 1.505372\n",
            "Train Epoch: 3 [3328000/10391263 (32%)]\tLoss: 1.852949\n",
            "Train Epoch: 3 [3379200/10391263 (33%)]\tLoss: 1.624360\n",
            "Train Epoch: 3 [3430400/10391263 (33%)]\tLoss: 1.609503\n",
            "Train Epoch: 3 [3481600/10391263 (34%)]\tLoss: 1.695977\n",
            "Train Epoch: 3 [3532800/10391263 (34%)]\tLoss: 1.688239\n",
            "Train Epoch: 3 [3584000/10391263 (34%)]\tLoss: 1.585759\n",
            "Train Epoch: 3 [3635200/10391263 (35%)]\tLoss: 1.785069\n",
            "Train Epoch: 3 [3686400/10391263 (35%)]\tLoss: 1.659289\n",
            "Train Epoch: 3 [3737600/10391263 (36%)]\tLoss: 1.639662\n",
            "Train Epoch: 3 [3788800/10391263 (36%)]\tLoss: 1.770789\n",
            "Train Epoch: 3 [3840000/10391263 (37%)]\tLoss: 1.680141\n",
            "Train Epoch: 3 [3891200/10391263 (37%)]\tLoss: 1.595040\n",
            "Train Epoch: 3 [3942400/10391263 (38%)]\tLoss: 1.566393\n",
            "Train Epoch: 3 [3993600/10391263 (38%)]\tLoss: 1.689256\n",
            "Train Epoch: 3 [4044800/10391263 (39%)]\tLoss: 1.736513\n",
            "Train Epoch: 3 [4096000/10391263 (39%)]\tLoss: 1.843395\n",
            "Train Epoch: 3 [4147200/10391263 (40%)]\tLoss: 1.603959\n",
            "Train Epoch: 3 [4198400/10391263 (40%)]\tLoss: 1.530688\n",
            "Train Epoch: 3 [4249600/10391263 (41%)]\tLoss: 1.716995\n",
            "Train Epoch: 3 [4300800/10391263 (41%)]\tLoss: 1.491503\n",
            "Train Epoch: 3 [4352000/10391263 (42%)]\tLoss: 1.651921\n",
            "Train Epoch: 3 [4403200/10391263 (42%)]\tLoss: 1.728383\n",
            "Train Epoch: 3 [4454400/10391263 (43%)]\tLoss: 1.622826\n",
            "Train Epoch: 3 [4505600/10391263 (43%)]\tLoss: 1.750815\n",
            "Train Epoch: 3 [4556800/10391263 (44%)]\tLoss: 1.702746\n",
            "Train Epoch: 3 [4608000/10391263 (44%)]\tLoss: 1.613818\n",
            "Train Epoch: 3 [4659200/10391263 (45%)]\tLoss: 1.672928\n",
            "Train Epoch: 3 [4710400/10391263 (45%)]\tLoss: 1.648650\n",
            "Train Epoch: 3 [4761600/10391263 (46%)]\tLoss: 1.654520\n",
            "Train Epoch: 3 [4812800/10391263 (46%)]\tLoss: 1.629265\n",
            "Train Epoch: 3 [4864000/10391263 (47%)]\tLoss: 1.742406\n",
            "Train Epoch: 3 [4915200/10391263 (47%)]\tLoss: 1.676113\n",
            "Train Epoch: 3 [4966400/10391263 (48%)]\tLoss: 1.569730\n",
            "Train Epoch: 3 [5017600/10391263 (48%)]\tLoss: 1.643738\n",
            "Train Epoch: 3 [5068800/10391263 (49%)]\tLoss: 1.848849\n",
            "Train Epoch: 3 [5120000/10391263 (49%)]\tLoss: 1.722327\n",
            "Train Epoch: 3 [5171200/10391263 (50%)]\tLoss: 1.527312\n",
            "Train Epoch: 3 [5222400/10391263 (50%)]\tLoss: 1.913430\n",
            "Train Epoch: 3 [5273600/10391263 (51%)]\tLoss: 1.763212\n",
            "Train Epoch: 3 [5324800/10391263 (51%)]\tLoss: 1.785935\n",
            "Train Epoch: 3 [5376000/10391263 (52%)]\tLoss: 1.724598\n",
            "Train Epoch: 3 [5427200/10391263 (52%)]\tLoss: 1.600313\n",
            "Train Epoch: 3 [5478400/10391263 (53%)]\tLoss: 1.581267\n",
            "Train Epoch: 3 [5529600/10391263 (53%)]\tLoss: 1.633082\n",
            "Train Epoch: 3 [5580800/10391263 (54%)]\tLoss: 1.675733\n",
            "Train Epoch: 3 [5632000/10391263 (54%)]\tLoss: 1.864913\n",
            "Train Epoch: 3 [5683200/10391263 (55%)]\tLoss: 1.719236\n",
            "Train Epoch: 3 [5734400/10391263 (55%)]\tLoss: 1.627854\n",
            "Train Epoch: 3 [5785600/10391263 (56%)]\tLoss: 1.580478\n",
            "Train Epoch: 3 [5836800/10391263 (56%)]\tLoss: 1.851513\n",
            "Train Epoch: 3 [5888000/10391263 (57%)]\tLoss: 1.569695\n",
            "Train Epoch: 3 [5939200/10391263 (57%)]\tLoss: 1.617059\n",
            "Train Epoch: 3 [5990400/10391263 (58%)]\tLoss: 1.680071\n",
            "Train Epoch: 3 [6041600/10391263 (58%)]\tLoss: 1.747515\n",
            "Train Epoch: 3 [6092800/10391263 (59%)]\tLoss: 1.778903\n",
            "Train Epoch: 3 [6144000/10391263 (59%)]\tLoss: 1.686365\n",
            "Train Epoch: 3 [6195200/10391263 (60%)]\tLoss: 1.737038\n",
            "Train Epoch: 3 [6246400/10391263 (60%)]\tLoss: 1.592818\n",
            "Train Epoch: 3 [6297600/10391263 (61%)]\tLoss: 1.556713\n",
            "Train Epoch: 3 [6348800/10391263 (61%)]\tLoss: 1.512622\n",
            "Train Epoch: 3 [6400000/10391263 (62%)]\tLoss: 1.701218\n",
            "Train Epoch: 3 [6451200/10391263 (62%)]\tLoss: 1.485150\n",
            "Train Epoch: 3 [6502400/10391263 (63%)]\tLoss: 1.689274\n",
            "Train Epoch: 3 [6553600/10391263 (63%)]\tLoss: 1.658969\n",
            "Train Epoch: 3 [6604800/10391263 (64%)]\tLoss: 1.672739\n",
            "Train Epoch: 3 [6656000/10391263 (64%)]\tLoss: 1.547763\n",
            "Train Epoch: 3 [6707200/10391263 (65%)]\tLoss: 1.786373\n",
            "Train Epoch: 3 [6758400/10391263 (65%)]\tLoss: 1.627940\n",
            "Train Epoch: 3 [6809600/10391263 (66%)]\tLoss: 1.672769\n",
            "Train Epoch: 3 [6860800/10391263 (66%)]\tLoss: 1.840804\n",
            "Train Epoch: 3 [6912000/10391263 (67%)]\tLoss: 1.823154\n",
            "Train Epoch: 3 [6963200/10391263 (67%)]\tLoss: 1.680941\n",
            "Train Epoch: 3 [7014400/10391263 (68%)]\tLoss: 1.689930\n",
            "Train Epoch: 3 [7065600/10391263 (68%)]\tLoss: 1.691025\n",
            "Train Epoch: 3 [7116800/10391263 (68%)]\tLoss: 1.664761\n",
            "Train Epoch: 3 [7168000/10391263 (69%)]\tLoss: 1.627396\n",
            "Train Epoch: 3 [7219200/10391263 (69%)]\tLoss: 1.710902\n",
            "Train Epoch: 3 [7270400/10391263 (70%)]\tLoss: 1.702284\n",
            "Train Epoch: 3 [7321600/10391263 (70%)]\tLoss: 1.516044\n",
            "Train Epoch: 3 [7372800/10391263 (71%)]\tLoss: 1.711022\n",
            "Train Epoch: 3 [7424000/10391263 (71%)]\tLoss: 1.495577\n",
            "Train Epoch: 3 [7475200/10391263 (72%)]\tLoss: 1.729297\n",
            "Train Epoch: 3 [7526400/10391263 (72%)]\tLoss: 1.720729\n",
            "Train Epoch: 3 [7577600/10391263 (73%)]\tLoss: 1.800972\n",
            "Train Epoch: 3 [7628800/10391263 (73%)]\tLoss: 1.823598\n",
            "Train Epoch: 3 [7680000/10391263 (74%)]\tLoss: 1.428883\n",
            "Train Epoch: 3 [7731200/10391263 (74%)]\tLoss: 1.795204\n",
            "Train Epoch: 3 [7782400/10391263 (75%)]\tLoss: 1.727015\n",
            "Train Epoch: 3 [7833600/10391263 (75%)]\tLoss: 1.720181\n",
            "Train Epoch: 3 [7884800/10391263 (76%)]\tLoss: 1.613031\n",
            "Train Epoch: 3 [7936000/10391263 (76%)]\tLoss: 1.682485\n",
            "Train Epoch: 3 [7987200/10391263 (77%)]\tLoss: 1.619569\n",
            "Train Epoch: 3 [8038400/10391263 (77%)]\tLoss: 1.499795\n",
            "Train Epoch: 3 [8089600/10391263 (78%)]\tLoss: 1.797795\n",
            "Train Epoch: 3 [8140800/10391263 (78%)]\tLoss: 1.702384\n",
            "Train Epoch: 3 [8192000/10391263 (79%)]\tLoss: 1.500085\n",
            "Train Epoch: 3 [8243200/10391263 (79%)]\tLoss: 1.661737\n",
            "Train Epoch: 3 [8294400/10391263 (80%)]\tLoss: 1.679497\n",
            "Train Epoch: 3 [8345600/10391263 (80%)]\tLoss: 1.604280\n",
            "Train Epoch: 3 [8396800/10391263 (81%)]\tLoss: 1.620945\n",
            "Train Epoch: 3 [8448000/10391263 (81%)]\tLoss: 1.694725\n",
            "Train Epoch: 3 [8499200/10391263 (82%)]\tLoss: 1.576252\n",
            "Train Epoch: 3 [8550400/10391263 (82%)]\tLoss: 1.627496\n",
            "Train Epoch: 3 [8601600/10391263 (83%)]\tLoss: 1.627516\n",
            "Train Epoch: 3 [8652800/10391263 (83%)]\tLoss: 1.503538\n",
            "Train Epoch: 3 [8704000/10391263 (84%)]\tLoss: 1.694995\n",
            "Train Epoch: 3 [8755200/10391263 (84%)]\tLoss: 1.594508\n",
            "Train Epoch: 3 [8806400/10391263 (85%)]\tLoss: 1.644389\n",
            "Train Epoch: 3 [8857600/10391263 (85%)]\tLoss: 1.643765\n",
            "Train Epoch: 3 [8908800/10391263 (86%)]\tLoss: 1.673400\n",
            "Train Epoch: 3 [8960000/10391263 (86%)]\tLoss: 1.669833\n",
            "Train Epoch: 3 [9011200/10391263 (87%)]\tLoss: 1.578901\n",
            "Train Epoch: 3 [9062400/10391263 (87%)]\tLoss: 1.799638\n",
            "Train Epoch: 3 [9113600/10391263 (88%)]\tLoss: 1.672094\n",
            "Train Epoch: 3 [9164800/10391263 (88%)]\tLoss: 1.684915\n",
            "Train Epoch: 3 [9216000/10391263 (89%)]\tLoss: 1.535359\n",
            "Train Epoch: 3 [9267200/10391263 (89%)]\tLoss: 1.578454\n",
            "Train Epoch: 3 [9318400/10391263 (90%)]\tLoss: 1.760632\n",
            "Train Epoch: 3 [9369600/10391263 (90%)]\tLoss: 1.762079\n",
            "Train Epoch: 3 [9420800/10391263 (91%)]\tLoss: 1.769919\n",
            "Train Epoch: 3 [9472000/10391263 (91%)]\tLoss: 1.629953\n",
            "Train Epoch: 3 [9523200/10391263 (92%)]\tLoss: 1.684347\n",
            "Train Epoch: 3 [9574400/10391263 (92%)]\tLoss: 1.931937\n",
            "Train Epoch: 3 [9625600/10391263 (93%)]\tLoss: 1.642339\n",
            "Train Epoch: 3 [9676800/10391263 (93%)]\tLoss: 1.686715\n",
            "Train Epoch: 3 [9728000/10391263 (94%)]\tLoss: 1.716640\n",
            "Train Epoch: 3 [9779200/10391263 (94%)]\tLoss: 1.547554\n",
            "Train Epoch: 3 [9830400/10391263 (95%)]\tLoss: 1.602823\n",
            "Train Epoch: 3 [9881600/10391263 (95%)]\tLoss: 1.763661\n",
            "Train Epoch: 3 [9932800/10391263 (96%)]\tLoss: 1.729339\n",
            "Train Epoch: 3 [9984000/10391263 (96%)]\tLoss: 1.638136\n",
            "Train Epoch: 3 [10035200/10391263 (97%)]\tLoss: 1.665408\n",
            "Train Epoch: 3 [10086400/10391263 (97%)]\tLoss: 1.633753\n",
            "Train Epoch: 3 [10137600/10391263 (98%)]\tLoss: 1.752394\n",
            "Train Epoch: 3 [10188800/10391263 (98%)]\tLoss: 1.458711\n",
            "Train Epoch: 3 [10240000/10391263 (99%)]\tLoss: 1.690356\n",
            "Train Epoch: 3 [10291200/10391263 (99%)]\tLoss: 1.730309\n",
            "Train Epoch: 3 [10342400/10391263 (100%)]\tLoss: 1.675980\n",
            "Dev accuracy  0.48932384892665587\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def train(args, model, device, train_samples, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    for i in range(len(train_samples)):\n",
        "        X, Y = train_samples[i]\n",
        "        train_items = LibriItems(X, Y, context=args['context'])\n",
        "        train_loader = torch.utils.data.DataLoader(train_items, batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.float().to(device)\n",
        "            target = target.long().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % args['log_interval'] == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(args, model, device, dev_samples):\n",
        "    model.eval()\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dev_samples)):\n",
        "            X, Y = dev_samples[i]\n",
        "\n",
        "            test_items = LibriItems(X, Y, context=args['context'])\n",
        "            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "            for data, true_y in test_loader:\n",
        "                data = data.float().to(device)\n",
        "                true_y = true_y.long().to(device)                \n",
        "                \n",
        "                output = model(data)\n",
        "                pred_y = torch.argmax(output, axis=1)\n",
        "\n",
        "                pred_y_list.extend(pred_y.tolist())\n",
        "                true_y_list.extend(true_y.tolist())\n",
        "\n",
        "    train_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "    return train_accuracy\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    model = Network().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    # If you want to use full Dataset, please pass None to csvpath\n",
        "    train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=\"/content/train_filenames_subset_8192_v2.csv\")\n",
        "    dev_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"dev-clean\")\n",
        "\n",
        "    for epoch in range(1, args['epoch'] + 1):\n",
        "        train(args, model, device, train_samples, optimizer, criterion, epoch)\n",
        "        test_acc = test(args, model, device, dev_samples)\n",
        "        print('Dev accuracy ', test_acc)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = {\n",
        "        'batch_size': 2048,\n",
        "        'context': 0,\n",
        "        'log_interval': 200,\n",
        "        'LIBRI_PATH': '/content',\n",
        "        'lr': 0.001,\n",
        "        'epoch': 3\n",
        "    }\n",
        "    main(args)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "239d5711fd35d2bb9bad2d5ca41e22b79107b4494fe73df9033b517b748af271"
    },
    "kernelspec": {
      "display_name": "PyTorch (3.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "hw1p2_s22_starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
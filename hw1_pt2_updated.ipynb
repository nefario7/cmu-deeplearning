{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nefario7/cmu-deeplearning/blob/working-hw1/hw1_pt2_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output \n",
        "! apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "! add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "! apt-get update -qq 2>&1 > /dev/null\n",
        "! apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "\n",
        "! google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "! echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "% cd /content\n",
        "! mkdir cmudrive\n",
        "% cd ..\n",
        "! google-drive-ocamlfuse /content/cmudrive\n",
        "! pip install kaggle wandb torch-summary\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/cmudrive/IDL/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! wandb login\n",
        "\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle \n",
        "! kaggle config set -n path -v /content\n",
        "! kaggle competitions download -c 11-785-s22-hw1p2\n",
        "! unzip -q /content/competitions/11-785-s22-hw1p2/11-785-s22-hw1p2.zip -d /content/hw1-data\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "NeSRw6SuWkx2"
      },
      "id": "NeSRw6SuWkx2",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "#!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "MIby3J0IWvkY"
      },
      "id": "MIby3J0IWvkY",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import yaml\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "from torchsummary import summary\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
      ],
      "metadata": {
        "id": "eYe24iHXXw5e"
      },
      "id": "eYe24iHXXw5e",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_saving(path, args, save_metadata=True, exp=\"Experiment\"):\n",
        "    save_name = ''\n",
        "    if not args['CSV_PATH']:\n",
        "        save_name += \"full_\"\n",
        "\n",
        "    for parameter, val in args.items():\n",
        "        abbr = parameter[0] if len(parameter) > 2 else parameter\n",
        "        if parameter == 'lr' :\n",
        "            data = abbr + str(val)\n",
        "            save_name += data\n",
        "            break\n",
        "        else:\n",
        "            data = abbr + str(val) + '_'\n",
        "            save_name += data\n",
        "\n",
        "    save_path = os.path.join(path, save_name)\n",
        "    try:\n",
        "        os.mkdir(save_path)\n",
        "    except FileExistsError:\n",
        "        d = input(\"Model name already exists. Delete existing model? (y/n)\")\n",
        "        if d == 'y':\n",
        "            import shutil\n",
        "            shutil.rmtree(save_path)\n",
        "            os.mkdir(save_path)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    with open(os.path.join(save_path, 'model_parameters.yaml'), 'w') as metadata:\n",
        "        yaml.dump({'Experiment': exp}, metadata, indent=8, default_flow_style=False)\n",
        "        yaml.dump(args, metadata, indent=4, default_flow_style=False)\n",
        "\n",
        "    return save_path\n",
        "\n",
        "def generate_submission(save_path, labels, timestamp): \n",
        "    sub_dir = save_path + timestamp\n",
        "    os.mkdir(sub_dir)\n",
        "    sub_path = os.path.join(sub_dir, 'submission.csv')\n",
        "\n",
        "    with open(sub_path, 'w') as f:\n",
        "        csvwrite = csv.writer(f)\n",
        "        csvwrite.writerow(['id', 'label'])\n",
        "        for i in range(len(labels)):\n",
        "            csvwrite.writerow([i, labels[i]])\n",
        "\n",
        "    print(f\"File saved at : {sub_path}\")\n",
        "    print(f\"Preview of submission.csv\")\n",
        "    df = pd.read_csv(sub_path)\n",
        "    df.head()\n",
        "\n",
        "    return sub_path"
      ],
      "metadata": {
        "id": "RREp-ifWY6Yy"
      },
      "id": "RREp-ifWY6Yy",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, context=0):\n",
        "        super(Network, self).__init__()\n",
        "        # TODO: Please try different architectures\n",
        "        c = (1 + 2 * context)\n",
        "        INPUT_SIZE = c * 13\n",
        "        NUM_CLASSES = 40\n",
        "\n",
        "        layers = [\n",
        "            nn.Linear(INPUT_SIZE, 512),\n",
        "            nn.BatchNorm1d(num_features = 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(num_features = 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(num_features = 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, NUM_CLASSES),\n",
        "        ]\n",
        "        \n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.classifier(A0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Qcny8yCsWxmq"
      },
      "id": "Qcny8yCsWxmq",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"dev-clean\", csvpath=None):\n",
        "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
        "        self.sample = sample \n",
        "        \n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        self.Y_dir = data_path + \"/\" + partition +\"/transcript/\"\n",
        "        \n",
        "        self.X_names = os.listdir(self.X_dir)\n",
        "        self.Y_names = os.listdir(self.Y_dir)\n",
        "\n",
        "        # using a small part of the dataset to debug\n",
        "        if csvpath:\n",
        "            subset = self.parse_csv(csvpath)\n",
        "            self.X_names = [i for i in self.X_names if i in subset]\n",
        "            self.Y_names = [i for i in self.Y_names if i in subset]\n",
        "        \n",
        "        if shuffle == True:\n",
        "            XY_names = list(zip(self.X_names, self.Y_names))\n",
        "            random.shuffle(XY_names)\n",
        "            self.X_names, self.Y_names = zip(*XY_names)\n",
        "        \n",
        "        assert(len(self.X_names) == len(self.Y_names))\n",
        "        self.length = len(self.X_names)\n",
        "        \n",
        "        self.PHONEMES = [\n",
        "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n",
        "      \n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[1])\n",
        "        return subset[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.length / self.sample))\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n",
        "        \n",
        "        X, Y = [], []\n",
        "        for j in sample_range:\n",
        "            X_path = self.X_dir + self.X_names[j]\n",
        "            Y_path = self.Y_dir + self.Y_names[j]\n",
        "            \n",
        "            label = [self.PHONEMES.index(yy) for yy in np.load(Y_path)][1:-1]\n",
        "\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "            Y.append(np.array(label))\n",
        "            \n",
        "        X, Y = np.concatenate(X), np.concatenate(Y)\n",
        "        return X, Y\n",
        "    \n",
        "class LibriItems(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y, context = 0):\n",
        "        assert(X.shape[0] == Y.shape[0])\n",
        "        \n",
        "        self.length  = X.shape[0]\n",
        "        self.context = context\n",
        "\n",
        "        if context == 0:\n",
        "            self.X, self.Y = X, Y\n",
        "        else:\n",
        "            X = np.pad(X, ((context,context), (0,0)), 'constant', constant_values=(0,0))\n",
        "            self.X, self.Y = X, Y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        if self.context == 0:\n",
        "            xx = self.X[i].flatten()\n",
        "            yy = self.Y[i]\n",
        "        else:\n",
        "            xx = self.X[i:(i + 2*self.context + 1)].flatten()\n",
        "            yy = self.Y[i]\n",
        "        return xx, yy"
      ],
      "metadata": {
        "id": "Jr9VJwbGWxrY"
      },
      "id": "Jr9VJwbGWxrY",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "2cbb1d57",
      "metadata": {
        "id": "2cbb1d57"
      },
      "outputs": [],
      "source": [
        "def train(args, model, device, train_samples, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "    for i in range(len(train_samples)):\n",
        "        X, Y = train_samples[i]\n",
        "        train_items = LibriItems(X, Y, context=args['context'])\n",
        "        train_loader = torch.utils.data.DataLoader(train_items, batch_size=args['batch_size'], num_workers=2, pin_memory=True, shuffle=True)\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.float().to(device)\n",
        "            target = target.long().to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            if batch_idx % args['log_interval'] == 0:\n",
        "                if args['log']:\n",
        "                    wandb.log({\"Training Loss\": loss.item()})\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(args, model, device, dev_samples):\n",
        "    model.eval()\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dev_samples)):\n",
        "            X, Y = dev_samples[i]\n",
        "\n",
        "            test_items = LibriItems(X, Y, context=args['context'])\n",
        "            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "            for data, true_y in test_loader:\n",
        "                data = data.float().to(device)\n",
        "                true_y = true_y.long().to(device)                \n",
        "                \n",
        "                output = model(data)\n",
        "                pred_y = torch.argmax(output, axis=1)\n",
        "\n",
        "                pred_y_list.extend(pred_y.tolist())\n",
        "                true_y_list.extend(true_y.tolist())\n",
        "\n",
        "    train_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "    return train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    model = Network(args['context']).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=args['CSV_PATH'])\n",
        "    dev_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"dev-clean\")\n",
        "\n",
        "    if args['log']:\n",
        "        wandb.init(project=\"phenome-hw1\", entity=\"nefario7\", config=args)\n",
        "\n",
        "    for epoch in range(1, args['epoch'] + 1):\n",
        "        train(args, model, device, train_samples, optimizer, criterion, epoch)\n",
        "        test_acc = test(args, model, device, dev_samples)\n",
        "\n",
        "        if args['log']:\n",
        "            wandb.log({\"Accuracy\": test_acc * 100})\n",
        "        print('Validation Accuracy ', test_acc)\n",
        "\n",
        "    if args['save']:\n",
        "        model_path = model_saving(path=r'/content/cmudrive/IDL/hw1-models-other', args=args, save_metadata=True, exp=\"Suggestion Med\")\n",
        "        torch.save(model, os.path.join(model_path, \"model.pt\"))\n",
        "        print(\"Model saved at : \", model_path)\n",
        "\n",
        "    if args['log']:\n",
        "        wandb.finish()"
      ],
      "metadata": {
        "id": "Qp8TiVzCGw3M"
      },
      "id": "Qp8TiVzCGw3M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    '': 'Sample',\n",
        "    'batch_size': 65536,\n",
        "    'epoch': 10,\n",
        "    'context': 16,\n",
        "    'lr': 0.001,\n",
        "    'LIBRI_PATH': '/content/hw1-data/hw1p2_student_data',\n",
        "    'CSV_PATH': '/content/hw1-data/train_filenames_subset_8192_v2.csv',\n",
        "    'log_interval': 500,\n",
        "    'save' : True,\n",
        "    'log' : True\n",
        "}\n",
        "\n",
        "main(args)"
      ],
      "metadata": {
        "id": "v5Dj1D1KYt6p"
      },
      "id": "v5Dj1D1KYt6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission"
      ],
      "metadata": {
        "id": "sAE0sHT3Z9UX"
      },
      "id": "sAE0sHT3Z9UX"
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from tqdm import tqdm\n",
        "import os, datetime\n",
        "\n",
        "class SubmissionSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, csv_path, sample=20000, shuffle=False, partition=\"test-clean\"):\n",
        "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
        "        self.sample = sample \n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        self.X_names = os.listdir(self.X_dir)\n",
        "\n",
        "        if csv_path:\n",
        "            self.X_names = list(pd.read_csv(csv_path).file)\n",
        "        \n",
        "        self.length = len(self.X_names)\n",
        "        self.PHONEMES = [\n",
        "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.length / self.sample))\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n",
        "        \n",
        "        X, Y = [], []\n",
        "        for j in sample_range:\n",
        "            X_path = self.X_dir + self.X_names[j]\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "\n",
        "        X = np.concatenate(X)\n",
        "        return X\n",
        "        \n",
        "class SubmissionItems(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, context = 0):   \n",
        "        self.length  = X.shape[0]\n",
        "        self.context = context\n",
        "\n",
        "        if context != 0:\n",
        "            X = np.pad(X, ((context,context), (0,0)), 'constant', constant_values=(0,0))\n",
        "        self.X = X\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        if self.context == 0:\n",
        "            xx = self.X[i].flatten()\n",
        "        else:\n",
        "            xx = self.X[i:(i + 2 * self.context + 1)].flatten()\n",
        "        return xx"
      ],
      "metadata": {
        "id": "2ealq3nfZ8y0"
      },
      "id": "2ealq3nfZ8y0",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement ensembling\n",
        "model_name = r'fullSample_b65536_e1_c16_lr0.001'\n",
        "model_type = r'hw1-models-other'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_dir = r'/content/cmudrive/IDL'\n",
        "model_path = os.path.join(model_dir, model_type, model_name, 'model.pt')\n",
        "meta_path = os.path.join(model_dir,  model_type, model_name, 'model_parameters.yaml')\n",
        "\n",
        "speech_model = torch.load(model_path).to(device)\n",
        "test_samples = SubmissionSamples(data_path = r'/content/hw1-data/hw1p2_student_data', csv_path=r'/content/hw1-data/test_order.csv')\n",
        "\n",
        "speech_model.eval()\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    for i in range(len(test_samples)):\n",
        "        X = test_samples[i]\n",
        "\n",
        "        test_items = SubmissionItems(X, context=args['context'])\n",
        "        test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "        for data in tqdm(test_loader):\n",
        "            data = data.float().to(device)              \n",
        "        \n",
        "            output = speech_model(data)\n",
        "            y = torch.argmax(output, axis=1)\n",
        "\n",
        "            labels.extend(y.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMh9pfZtd22K",
        "outputId": "04fbf6f2-b03a-4263-cd4d-f092c5a9b3c0"
      },
      "id": "oMh9pfZtd22K",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:14<00:00,  2.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_path = generate_submission(r'/content/cmudrive/IDL/hw1-submission/', labels, timestamp)\n",
        "! kaggle competitions submit -c 11-785-s22-hw1p2 -f $sub_path -m \"Sample Submission\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moZJfg2VakUk",
        "outputId": "ffeb0fdf-b148-4f1a-e226-8960d74aa432"
      },
      "id": "moZJfg2VakUk",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved at : /content/cmudrive/IDL/hw1-submission/2022-02-03_19-36-42/submission.csv\n",
            "Preview of submission.csv\n",
            "100% 20.5M/20.5M [00:02<00:00, 7.57MB/s]\n",
            "Successfully submitted to Frame-Level Speech Recognition"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "239d5711fd35d2bb9bad2d5ca41e22b79107b4494fe73df9033b517b748af271"
    },
    "kernelspec": {
      "display_name": "PyTorch (3.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "hw1-pt2-updated",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
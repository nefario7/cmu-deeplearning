{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nefario7/cmu-deeplearning/blob/working-hw2/Homework%202/Continue_ConvNext_HW2P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive and Download Data"
      ],
      "metadata": {
        "id": "e9WhS16VCva6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output \n",
        "! apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "! add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "! apt-get update -qq 2>&1 > /dev/null\n",
        "! apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "\n",
        "! google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "! echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "% cd /content\n",
        "! mkdir cmudrive\n",
        "% cd ..\n",
        "! google-drive-ocamlfuse /content/cmudrive\n",
        "! pip install kaggle wandb torch-summary\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/cmudrive/IDL/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggleÂ \n",
        "! kaggle config set -n path -v /content\n",
        "\n",
        "! wandb login 4bdbe9c204105e1264fe3f54df2732fd1fff8040\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "KSB0Pzy3Cu2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "! kaggle competitions download -c 11-785-s22-hw2p2-verification\n",
        "\n",
        "! unzip -q /content/competitions/11-785-s22-hw2p2-classification/11-785-s22-hw2p2-classification.zip -d /content\n",
        "! unzip -q /content/competitions/11-785-s22-hw2p2-verification/11-785-s22-hw2p2-verification.zip -d /content\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "ipZZI38GC-Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "zttLq9nw1aMP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwLEd0gdPbSc"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import wandb\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "import torchvision.models as models\n",
        "\n",
        "!pip install albumentations==0.4.6\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from functools import partial\n",
        "from IPython.display import clear_output \n",
        "\n",
        "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "torch.autograd.profiler.profile(False)\n",
        "torch.autograd.profiler.emit_nvtx(False)\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "# FaceNet Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MobileNetV2"
      ],
      "metadata": {
        "id": "lIHtrzBISdVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class InvertedResidualBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 stride,\n",
        "                 expand_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # Can only do identity residual connection if input & output are the same channel & spatial shape.\n",
        "        if stride == 1 and in_channels == out_channels:\n",
        "            self.do_identity = True\n",
        "        else:\n",
        "            self.do_identity = False\n",
        "        \n",
        "        # Expand Ratio is like 6, so hidden_dim >> in_channels\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "\n",
        "        self.feature_mixing = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=hidden_dim, kernel_size=(1, 1), stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU6()\n",
        "        )\n",
        "\n",
        "        self.spatial_mixing = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel=(3,3), stride=stride, padding=1, groups=hidden_dim, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU6()\n",
        "        )\n",
        "\n",
        "        self.bottleneck_channels = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=hidden_dim, out_channels=in_channels, kernel_size=(1, 1), stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(in_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.feature_mixing(x)\n",
        "        out = self.spatial_mixing(out)\n",
        "        out = self.bottleneck_channels(out)\n",
        "\n",
        "        if self.do_identity:\n",
        "            return x + out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    \"\"\"\n",
        "    The heavy lifting is already done in InvertedBottleneck.\n",
        "\n",
        "    Why MobileNetV2 and not V3? V2 is the foundation for V3, which uses \"neural\n",
        "    architecture search\" to find better configurations of V2. If you understand\n",
        "    V2 well, you can totally implement V3!\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes= 7000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        \"\"\"\n",
        "        First couple of layers are special, just do them here.\n",
        "        This is called the \"stem\". Usually, methods use it to downsample or twice.\n",
        "        \"\"\"\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel=(3, 3), )\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        Since we're just repeating InvertedResidualBlocks again and again, we\n",
        "        want to specify their parameters like this.\n",
        "        The four numbers in each row (a stage) are shown below.\n",
        "        - Expand ratio: We talked about this in InvertedResidualBlock\n",
        "        - Channels: This specifies the channel size before expansion\n",
        "        - # blocks: Each stage has many blocks, how many?\n",
        "        - Stride of first block: For some stages, we want to downsample. In a\n",
        "          downsampling stage, we set the first block in that stage to have\n",
        "          stride = 2, and the rest just have stride = 1.\n",
        "\n",
        "        Again, note that almost every stage here is downsampling! By the time\n",
        "        we get to the last stage, what is the image resolution? Can it still\n",
        "        be called an image for our dataset? Think about this, and make changes\n",
        "        as you want.\n",
        "        \"\"\"\n",
        "        self.stage_cfgs = [\n",
        "            # expand_ratio, channels, # blocks, stride of first block\n",
        "            [6,  24, 2, 2],\n",
        "            [6,  32, 3, 2],\n",
        "            [6,  64, 4, 2],\n",
        "            [6,  96, 3, 1],\n",
        "            [6, 160, 3, 2],\n",
        "            [6, 320, 1, 1],\n",
        "        ]\n",
        "\n",
        "        # Remember that our stem left us off at 16 channels. We're going to \n",
        "        # keep updating this in_channels variable as we go\n",
        "        in_channels = 16\n",
        "\n",
        "        # Let's make the layers\n",
        "        layers = []\n",
        "        for curr_stage in self.stage_cfgs:\n",
        "            expand_ratio, num_channels, num_blocks, stride = curr_stage\n",
        "            \n",
        "            for block_idx in range(num_blocks):\n",
        "                out_channels = num_channels\n",
        "                layers.append(InvertedResidualBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    # only have non-trivial stride if first block\n",
        "                    stride=stride if block_idx == 0 else 1, \n",
        "                    expand_ratio=expand_ratio\n",
        "                ))\n",
        "                # In channels of the next block is the out_channels of the current one\n",
        "                in_channels = out_channels \n",
        "            \n",
        "        self.layers = nn.Sequential(*layers) # Done, save them to the class\n",
        "\n",
        "        # Some final feature mixing\n",
        "        self.final_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 1280, kernel_size=1, padding=0, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(1280),\n",
        "            nn.ReLU6()\n",
        "        )\n",
        "\n",
        "        # Now, we need to build the final classification layer.\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            # Pool over & collapse the spatial dimensions to (1, 1)\n",
        "            # Collapse the trivial (1, 1) dimensions\n",
        "            # Project to our # of classes\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Usually, I like to use default pytorch initialization for stuff, but\n",
        "        MobileNetV2 made a point of putting in some custom ones, so let's just\n",
        "        use them.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stem(x)\n",
        "        out = self.layers(out)\n",
        "        out = self.final_block(out)\n",
        "        out = self.cls_layer(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "NQX8iZhfSdBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ConvNext"
      ],
      "metadata": {
        "id": "jqHIoX6Eytcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import math \n",
        "import torchvision\n",
        "\n",
        "class ConvNextBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
        "        super().__init__()\n",
        "        if stride == 1 and in_channels == out_channels:\n",
        "            self.do_identity = True\n",
        "        else:\n",
        "            self.do_identity = False\n",
        "\n",
        "        self.drop_prob =0.0\n",
        "\n",
        "        # expand ratio = 4 ## for convnext\n",
        "        hidden_dim = in_channels*expand_ratio\n",
        "\n",
        "        self.depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, groups =in_channels, kernel_size = 7, padding = 3, bias = False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "        )\n",
        "\n",
        "        self.pw_conv_inc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size = 1, stride =1 ,bias = False ), \n",
        "            nn.GELU(),\n",
        "        )\n",
        "        \n",
        "        self.pw_conv_dec = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size = 1, stride = 1, bias = False),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.depth_conv(x)\n",
        "        out = self.pw_conv_inc(out)\n",
        "        out = self.pw_conv_dec(out)\n",
        "\n",
        "        # if self.do_identity:\n",
        "        #     return x+ torchvision.ops.stochastic_depth(input = out, p=self.drop_prob, mode = \"batch\")\n",
        "        # else:\n",
        "        #     return out\n",
        "\n",
        "        # return x+ torchvision.ops.stochastic_depth(input = out, p=self.drop_prob, mode = \"batch\")\n",
        "        return x + out\n",
        "        \n",
        "class ConvNext(nn.Module):\n",
        "    def __init__(self, num_classes=7000, dims=[96, 192, 384, 768]):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, dims[0], kernel_size=4, stride = 4),\n",
        "            nn.BatchNorm2d(dims[0])\n",
        "        )\n",
        "        in_channels = dims[0]\n",
        "        self.stages = [\n",
        "            [4,dims[0], 3, 2], \n",
        "            [4,dims[1], 3, 2], \n",
        "            [4,dims[2], 9, 2], \n",
        "            [4,dims[3], 3, 2]    \n",
        "        ]\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        for i, current_stage in enumerate(self.stages):\n",
        "            expand_ratio, num_channels, num_blocks, stride = current_stage\n",
        "\n",
        "            out_channels = num_channels\n",
        "            if in_channels!=out_channels:\n",
        "                downsample_layer = nn.Sequential(\n",
        "                    nn.BatchNorm2d(in_channels), \n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=2, stride = 2)\n",
        "                )\n",
        "            else:\n",
        "                downsample_layer = nn.Identity()\n",
        "\n",
        "            in_channels = out_channels\n",
        "            layers.append(downsample_layer)\n",
        "            for block_idx in range(num_blocks):\n",
        "                layers.append(ConvNextBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels = out_channels, \n",
        "                    stride = stride ,\n",
        "                    expand_ratio=expand_ratio\n",
        "                ))\n",
        "\n",
        "        \n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.final_norm = nn.BatchNorm2d(dims[-1])\n",
        "\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            # Use dropout if you want post flatten \n",
        "            nn.Linear(dims[-1], num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, return_feats = False):\n",
        "        out = self.stem(x)\n",
        "        out = self.layers(out)\n",
        "        out = self.final_norm(out)\n",
        "        feats = self.cls_layer[:2](out)\n",
        "        out = self.cls_layer(out)\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out"
      ],
      "metadata": {
        "id": "pIwOg3HQjYWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myConvNextBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride, expansion, activation='gelu'):\n",
        "        super().__init__()\n",
        "        self.stride, self.expansion, self.activation = stride, expansion, activation\n",
        "\n",
        "        if self.stride == 1 and in_channels == out_channels:\n",
        "            self.do_identity = True\n",
        "        else:\n",
        "            self.do_identity = False\n",
        "\n",
        "        if self.activation == 'gelu':\n",
        "            self.activate = nn.GELU()\n",
        "        elif self.activation == 'relu6':\n",
        "            self.activate = nn.ReLU6()\n",
        "        \n",
        "        expansion_channels = in_channels * self.expansion\n",
        "\n",
        "        self.depthwise = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels,\n",
        "                      kernel_size=(7, 7), stride=1, padding=3, groups=in_channels, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "        )\n",
        "\n",
        "        self.increase_channels = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=expansion_channels,\n",
        "                      kernel_size=(1, 1), stride=1, padding=0, bias=False),\n",
        "            self.activate\n",
        "        )\n",
        "\n",
        "        self.decrease_channels = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=expansion_channels, out_channels=out_channels,\n",
        "                      kernel_size=(1, 1), stride=1, padding=0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.increase_channels(out)\n",
        "        out = self.decrease_channels(out)\n",
        "\n",
        "        # if self.do_identity:\n",
        "        #     return out + x\n",
        "        # else:\n",
        "        #     return out\n",
        "        return x + out\n",
        "\n",
        "class myConvNext(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes, channel_set, block_set):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert len(channel_set) == len(block_set)\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, channel_set[0], kernel_size=(4, 4), stride=4, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(channel_set[0]),\n",
        "        )\n",
        "\n",
        "        main_layers = []\n",
        "        for i, block in enumerate(block_set):\n",
        "            for _ in range(block):\n",
        "                main_layers.append(\n",
        "                    myConvNextBlock(\n",
        "                        in_channels=channel_set[i], \n",
        "                        out_channels=channel_set[i], \n",
        "                        stride=2, \n",
        "                        expansion=4\n",
        "                        )\n",
        "                    )\n",
        "                         \n",
        "            if i != len(block_set) - 1:\n",
        "                downsample_layer = nn.Sequential(\n",
        "                    nn.BatchNorm2d(channel_set[i]), \n",
        "                    nn.Conv2d(channel_set[i], channel_set[i+1], kernel_size=(2, 2), stride = 2)\n",
        "                )\n",
        "                main_layers.append(downsample_layer)\n",
        "\n",
        "        main_layers.append(nn.BatchNorm2d(channel_set[-1]))\n",
        "        self.main_blocks = nn.Sequential(*main_layers)\n",
        "        flattening = [            \n",
        "                        nn.AdaptiveAvgPool2d((1,1)),\n",
        "                        nn.BatchNorm2d(channel_set[-1]),\n",
        "                        nn.Flatten()]\n",
        "        self.flatten_layers = nn.Sequential(*flattening)\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            nn.Linear(channel_set[-1], num_classes)\n",
        "        )\n",
        "\n",
        "        self.initialize_weights()\n",
        "        \n",
        "    def forward(self, x, return_feats=False):\n",
        "        out = self.stem(x)\n",
        "        out = self.main_blocks(out)\n",
        "        feats = self.flatten_layers(out)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.2)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.2)\n",
        "                nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "49R3LSVuE8Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet"
      ],
      "metadata": {
        "id": "sI3KJtVvypyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2dAuto(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
        "\n",
        "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False) \n",
        "# conv = conv3x3(in_channels=32, out_channels=64)\n",
        "\n",
        "def get_activation(option):\n",
        "    return nn.ModuleDict([\n",
        "        ['relu', nn.ReLU(inplace=True)],\n",
        "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
        "        ['selu', nn.SELU(inplace=True)],\n",
        "        ['none', nn.Identity()]\n",
        "    ])[option]\n",
        "\n",
        "def resnet_block(in_channels, out_channels, conv, *args, **kwargs):\n",
        "    resnet_block_layers = [\n",
        "              conv(in_channels, out_channels, *args, **kwargs),\n",
        "              nn.BatchNorm2d(out_channels)\n",
        "              ]\n",
        "    return nn.Sequential(*resnet_block_layers)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.activation = activation\n",
        "        self.blocks = nn.Identity()\n",
        "        self.activate = get_activation(activation)\n",
        "        self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.apply_shortcut: residual = self.shortcut(x)\n",
        "        x = self.blocks(x)\n",
        "        x = x + residual\n",
        "        x = self.activate(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def apply_shortcut(self):\n",
        "        return self.in_channels != self.out_channels\n",
        "\n",
        "class ResNetResidualBlock(ResidualBlock):\n",
        "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
        "        self.expansion = expansion\n",
        "        self.downsampling = downsampling\n",
        "        self.conv = conv\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,stride=self.downsampling, bias=False),\n",
        "            nn.BatchNorm2d(self.expanded_channels)\n",
        "            ) if self.should_apply_shortcut else None\n",
        "        \n",
        "    @property\n",
        "    def expanded_channels(self):\n",
        "        return self.out_channels * self.expansion\n",
        "    \n",
        "    @property\n",
        "    def should_apply_shortcut(self):\n",
        "        return self.in_channels != self.expanded_channels\n",
        "\n",
        "class ResNetBasicBlock(ResNetResidualBlock):\n",
        "    expansion=1\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "            resnet_block(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
        "            get_activation(self.activation),\n",
        "            resnet_block(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
        "        )\n",
        "\n",
        "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "            resnet_block(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
        "            get_activation(self.activation),\n",
        "            resnet_block(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
        "            get_activation(self.activation),\n",
        "            resnet_block(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
        "        )\n",
        "    \n",
        "\n",
        "class ResNetLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        downsampling = 2 if in_channels != out_channels else 1\n",
        "        self.blocks = nn.Sequential(\n",
        "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
        "            *[block(out_channels * block.expansion, \n",
        "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        return x\n",
        "\n",
        "class ResNetEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], activation='relu', block=ResNetBasicBlock, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.blocks_sizes = blocks_sizes\n",
        "        \n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
        "            get_activation(activation),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
        "        self.blocks = nn.ModuleList([ \n",
        "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
        "                        block=block,*args, **kwargs),\n",
        "            *[ResNetLayer(in_channels * block.expansion, \n",
        "                          out_channels, n=n, activation=activation, \n",
        "                          block=block, *args, **kwargs) \n",
        "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
        "        ])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.gate(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "class ResNetDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.decoder = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "class myResNet(nn.Module):\n",
        "    def __init__(self, inc, outc, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.encoder = ResNetEncoder(inc, *args, **kwargs)\n",
        "        self.decoder = ResNetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, outc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "S0-TNZOih-a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = resnet34(in_channels=3, n_classes=7000)\n",
        "# layers = list(model.children())[:-1]\n",
        "# layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "# layers.append(nn.Flatten())\n",
        "# layers.append(nn.Linear(512, 7000))\n",
        "# backbone = nn.Sequential(*layers)\n",
        "# # model = nn.Flatten(model)\n",
        "# # model = models.resnet34(pretrained=False)\n",
        "# summary(backbone, (3, 224, 224))"
      ],
      "metadata": {
        "id": "oZGzHJgfG2m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FaceNet"
      ],
      "metadata": {
        "id": "yvXHNZRKOlDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convnext(in_channels, n_classes, variant):\n",
        "    params = {\n",
        "        'tiny': {'channel_set': [96, 192, 384, 768], 'block_set': [3, 3, 9, 3]},\n",
        "        'small': {'channel_set': [96, 192, 384, 768], 'block_set': [3, 3, 27, 3]},\n",
        "        'big': {'channel_set': [128, 256, 512, 1024], 'block_set': [3, 3, 27, 3]},\n",
        "        'large': {'channel_set': [192, 384, 768, 1536], 'block_set': [3, 3, 27, 3]},\n",
        "    }\n",
        "\n",
        "    convnext = myConvNext(in_channels, n_classes, **params[variant])\n",
        "    layers = list(convnext.children())[:-1]\n",
        "    return layers, 768\n",
        "\n",
        "\n",
        "def resnet34(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    resnet = myResNet(in_channels, n_classes, block=block, deepths=[3, 4, 6, 3], *args, **kwargs)\n",
        "    layers = list(resnet.children())[:-1]\n",
        "    return layers, 512"
      ],
      "metadata": {
        "id": "KqlnguCifYe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cm, size = convnext(3, 7000, 'tiny')\n",
        "# backbone = nn.Sequential(*cm)\n",
        "# cls_layer = nn.Sequential(\n",
        "#     nn.AdaptiveAvgPool2d((1,1)),\n",
        "#     nn.Flatten(),\n",
        "#     nn.Linear(size, 7000))\n",
        "\n",
        "# summary(backbone, (3, 224, 224))"
      ],
      "metadata": {
        "id": "KvHd8yJiv2_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-mh_ocWIJR"
      },
      "outputs": [],
      "source": [
        "class FaceNet(nn.Module):\n",
        "    def list_to_kwarg(self, inc, outc, kernel, s, p):\n",
        "        params = dict()\n",
        "        params[\"in_channels\"] = inc\n",
        "        params[\"out_channels\"] = outc\n",
        "        params[\"kernel_size\"] = kernel\n",
        "        params[\"stride\"] = s\n",
        "        params[\"padding\"] = p\n",
        "        return params\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        num_classes = 7000\n",
        "        num_channels = 3\n",
        "        backbone_layers = []\n",
        "        if config['backbone'] == 'simple':\n",
        "            for l_idx, l_params in config['arch'].items():\n",
        "                conv_params = self.list_to_kwarg(*l_params[\"conv\"])\n",
        "                backbone_layers.append(nn.Conv2d(**conv_params))\n",
        "                backbone_layers.append(nn.BatchNorm2d(conv_params[\"out_channels\"]))\n",
        "                backbone_layers.append(nn.ReLU())\n",
        "                if l_params[\"pool\"] is not None:\n",
        "                    if l_params[\"pool\"][\"max\"]:\n",
        "                        backbone_layers.append(nn.AdaptiveMaxPool2d(l_params[\"pool\"][\"output\"]))\n",
        "                    else:\n",
        "                        backbone_layers.append(nn.AdaptiveAvgPool2d(l_params[\"pool\"][\"output\"]))\n",
        "            backbone_layers.append(nn.Flatten())\n",
        "            self.backbone = nn.Sequential(*backbone_layers)\n",
        "            self.cls_layer = nn.Linear(512, num_classes)\n",
        "        else:\n",
        "            if config['backbone'] == 'resnet_34':\n",
        "                backbone_layers, size = resnet34(num_channels, num_classes)\n",
        "                self.backbone = nn.Sequential(*backbone_layers)\n",
        "                flattening = [            \n",
        "                            nn.AdaptiveAvgPool2d((1,1)),\n",
        "                            nn.BatchNorm2d(size),\n",
        "                            nn.Flatten()]\n",
        "                self.flatten_layers = nn.Sequential(*flattening)\n",
        "                self.cls_layer = nn.Sequential(nn.Linear(size, num_classes))\n",
        "\n",
        "            elif config['backbone'] == 'convnext':\n",
        "                backbone_layers, size = convnext(num_channels, num_classes, 'tiny')\n",
        "\n",
        "                self.backbone = nn.Sequential(*backbone_layers)\n",
        "                self.flatten_layers = nn.Identity()\n",
        "                self.cls_layer = nn.Sequential(nn.Linear(size, num_classes))\n",
        "\n",
        "        self.initialize_weights()\n",
        "    \n",
        "    def forward(self, x, return_feats=False):\n",
        "        feats = self.backbone(x)\n",
        "        feats = self.flatten_layers(feats)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.2)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.2)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            # elif isinstance(m, nn.BatchNorm2d):\n",
        "            #     nn.init.constant_(m.weight, 1)\n",
        "            #     nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FaceNet Training"
      ],
      "metadata": {
        "id": "gtxmWqOch4-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArcFace(torch.nn.Module):\n",
        "    \"\"\" ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n",
        "    \"\"\"\n",
        "    def __init__(self, s=64.0, margin=0.5):\n",
        "        super(ArcFace, self).__init__()\n",
        "        self.scale = s\n",
        "        self.cos_m = math.cos(margin)\n",
        "        self.sin_m = math.sin(margin)\n",
        "        self.theta = math.cos(math.pi - margin)\n",
        "        self.sinmm = math.sin(math.pi - margin) * margin\n",
        "        self.easy_margin = False\n",
        "\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n",
        "        index = torch.where(labels != -1)[0]\n",
        "        target_logit = logits[index, labels[index].view(-1)]\n",
        "\n",
        "        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n",
        "        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m  # cos(target+margin)\n",
        "        if self.easy_margin:\n",
        "            final_target_logit = torch.where(\n",
        "                target_logit > 0, cos_theta_m, target_logit)\n",
        "        else:\n",
        "            final_target_logit = torch.where(\n",
        "                target_logit > self.theta, cos_theta_m, target_logit - self.sinmm)\n",
        "\n",
        "        logits[index, labels[index].view(-1)] = final_target_logit\n",
        "        logits = logits * self.scale\n",
        "        return logits\n",
        "\n",
        "class CosFace(torch.nn.Module):\n",
        "    def __init__(self, s=64.0, m=0.40):\n",
        "        super(CosFace, self).__init__()\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n",
        "        index = torch.where(labels != -1)[0]\n",
        "        target_logit = logits[index, labels[index].view(-1)]\n",
        "        final_target_logit = target_logit - self.m\n",
        "        logits[index, labels[index].view(-1)] = final_target_logit\n",
        "        logits = logits * self.s\n",
        "        return logits"
      ],
      "metadata": {
        "id": "d8-uBtKRzZch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UowI9OcUYPjP"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import ToTensor\n",
        "class FaceNetSetup:\n",
        "    def __init__(self, config, save_path):\n",
        "        self.config = config\n",
        "        self.log = config['log']\n",
        "\n",
        "        if config['subset']:\n",
        "            train_path = r\"train_subset/train_subset\"\n",
        "        else:\n",
        "            train_path = r\"classification/classification/train\"\n",
        "        self.SAVE_DIR = save_path\n",
        "        self.DATA_DIR = r\"/content\" \n",
        "        self.TRAIN_DIR = osp.join(self.DATA_DIR, train_path) \n",
        "        self.VAL_DIR = osp.join(self.DATA_DIR, r\"classification/classification/dev\")\n",
        "\n",
        "    def __check_model_params(self):\n",
        "        num_trainable_parameters = 0\n",
        "        for p in self.model.parameters():\n",
        "            num_trainable_parameters += p.numel()\n",
        "        print(\"Number of Params: {}\".format(num_trainable_parameters))\n",
        "        assert num_trainable_parameters <= 35000000\n",
        "\n",
        "    def __gen_model_name(self):\n",
        "        save_name = ''\n",
        "        if not self.config['subset']:\n",
        "            save_name += \"Full_\"\n",
        "        for key, val in self.config.items():\n",
        "            abbr = key[0] if len(key) > 2 else key\n",
        "            if isinstance(val, dict):\n",
        "                data = 'lr' + str(val[\"lr\"])\n",
        "                save_name += data\n",
        "                break\n",
        "            else:\n",
        "                data = abbr + str(val) + '_'\n",
        "                save_name += data\n",
        "        if self.config['randomize']:\n",
        "            save_name = save_name + \"-v\" + str(np.random.randint(10, 1000))\n",
        "        print(\"\\nModel Name: \", save_name)\n",
        "\n",
        "        return save_name\n",
        "\n",
        "    def __save_model_params(self, continue_train):\n",
        "        # Create Model Directory\n",
        "        save_path = os.path.join(self.SAVE_DIR, self.model_name)\n",
        "        if not continue_train:\n",
        "            try:\n",
        "                os.mkdir(save_path)\n",
        "            except FileExistsError:\n",
        "                d = input(\"Model name already exists. Delete existing model? (y/n)\")\n",
        "                if d == 'y':\n",
        "                    import shutil\n",
        "                    shutil.rmtree(save_path)\n",
        "                    os.mkdir(save_path)\n",
        "                else:\n",
        "                    print(\"Exiting!\")\n",
        "                    exit(0)\n",
        "                    return None\n",
        "\n",
        "            os.mkdir(os.path.join(save_path, 'Checkpoints'))\n",
        "            # Saving Model Configuration\n",
        "            with open(os.path.join(save_path, 'model_config.yaml'), 'w') as metadata:\n",
        "                yaml.dump({'Experiment': self.config['']}, metadata, indent=4, default_flow_style=False)\n",
        "                yaml.dump(self.config, metadata, indent=4, default_flow_style=False)\n",
        "            print(\"Model to be saved at: \", save_path)\n",
        "        return save_path\n",
        "\n",
        "    def __dataloaders(self): \n",
        "        \"\"\"\n",
        "        Transforms (data augmentation) is quite important for this task.\n",
        "        Go explore https://pytorch.org/vision/stable/transforms.html for more details\n",
        "        \"\"\"\n",
        "        if self.config[\"transforms\"]:\n",
        "            self.train_transforms = [\n",
        "                                    # ttf.RandomRotation(10, expand=False),\n",
        "                                    ttf.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2)),\n",
        "                                    ttf.RandomHorizontalFlip(p=0.4),\n",
        "                                    ttf.RandomResizedCrop((224, 224), scale=(0.3, 1)),\n",
        "                                    ttf.RandAugment(),\n",
        "                                    ttf.ToTensor(),\n",
        "                                    # ttf.PILToTensor(),\n",
        "                                    # ttf.ConvertImageDtype(torch.float),\n",
        "                                    # ttf.RandomErasing(p=0.4),\n",
        "                                    ]\n",
        "            self.val_transforms = [ttf.ToTensor()]\n",
        "        else:\n",
        "            self.train_transforms = [ttf.ToTensor()]\n",
        "            self.val_transforms = [ttf.ToTensor()]\n",
        "\n",
        "        self.train_dataset = torchvision.datasets.ImageFolder(self.TRAIN_DIR, transform=ttf.Compose(self.train_transforms))\n",
        "        self.val_dataset = torchvision.datasets.ImageFolder(self.VAL_DIR, transform=ttf.Compose(self.val_transforms))\n",
        "\n",
        "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.config['batch_size'], shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
        "        self.val_loader = DataLoader(self.val_dataset, batch_size=self.config['batch_size'], shuffle=False, drop_last=True, num_workers=2)\n",
        "\n",
        "    def setup(self, continue_train=False, chkpt=None):\n",
        "        self.__dataloaders()\n",
        "        # Model\n",
        "        # self.model = FaceNet(self.config)\n",
        "        # conv_params = {'channel_set': [96, 192, 384, 768], 'block_set': [3, 3, 9, 3]}\n",
        "        # self.model = myConvNext(3, 7000, **conv_params)\n",
        "        self.model = ConvNext()\n",
        "        self.model.cuda()\n",
        "        summary(self.model, (3, 224, 224))\n",
        "\n",
        "        self.__check_model_params()\n",
        "        self.model_name = self.__gen_model_name()\n",
        "        self.model_path = self.__save_model_params(continue_train)\n",
        "\n",
        "        # Loss\n",
        "        if self.config[\"loss\"] == 'CrossEL':\n",
        "            self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "        elif self.config[\"loss\"] == 'ArcFace':\n",
        "            self.criterion == ArcFace()\n",
        "        \n",
        "        # Optimizer\n",
        "        if self.config[\"optimizer\"] == 'SGD':\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), **self.config['optim'])\n",
        "        elif self.config[\"optimizer\"] == \"Adam\":\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), **self.config['optim'])\n",
        "        elif self.config[\"optimizer\"] == \"AdamW\":\n",
        "            self.optimizer = optim.AdamW(self.model.parameters(), **self.config['optim'])\n",
        "\n",
        "        if continue_train:\n",
        "            self.chkpt = chkpt\n",
        "            assert chkpt is not None\n",
        "            chkpt_path = os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(chkpt) + '.pth')\n",
        "            checkpoint = torch.load(chkpt_path)\n",
        "            print(checkpoint.keys())\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            print(\"Continuing training from epoch \", checkpoint[\"epoch\"] )\n",
        "\n",
        "        # Scheduler\n",
        "        if self.config[\"scheduler\"] == 'CosineAnnealingLR':\n",
        "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=(len(self.train_loader) * self.config['epochs']))\n",
        "        elif self.config[\"scheduler\"] == 'ReduceLRonPlateau':\n",
        "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max',factor=0.5, patience=2)\n",
        "\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    def train(self, validate=False, continue_train=False):\n",
        "        if continue_train:\n",
        "            chkpt = self.chkpt\n",
        "        else: \n",
        "            chkpt = 0   \n",
        "        # epochs = self.config['epochs'] - chkpt\n",
        "        epochs = self.config['epochs']\n",
        "        batch_size = self.config['batch_size']\n",
        "\n",
        "        if self.log:\n",
        "            wandb.init(project=\"hw2-letsgo\", entity=\"nefario7\", config=self.config)\n",
        "            wandb.watch(self.model, criterion=self.criterion, log=\"all\", log_freq=batch_size, idx=None,log_graph=True)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            print(\"-\"*25 + \"Epoch \" + str(epoch) + \"-\"*25)\n",
        "            # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "            batch_bar = tqdm(total=len(self.train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "            num_correct = 0\n",
        "            total_loss = 0\n",
        "\n",
        "            for i, (x, y) in enumerate(self.train_loader):\n",
        "                self.model.train()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                x = x.cuda()\n",
        "                y = y.cuda()\n",
        "\n",
        "                # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "                with torch.cuda.amp.autocast():     \n",
        "                    outputs = self.model(x)\n",
        "                    loss = self.criterion(outputs, y)\n",
        "\n",
        "                # Update # correct & loss as we go\n",
        "                num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "                total_loss += loss\n",
        "\n",
        "                # tqdm lets you add some details so you can monitor training as you train.\n",
        "                batch_bar.set_postfix(\n",
        "                    acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "                    loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "                    num_correct=num_correct,\n",
        "                    lr=\"{:.04f}\".format(float(self.optimizer.param_groups[0]['lr'])))\n",
        "                \n",
        "                # Another couple things you need for FP16. \n",
        "                self.scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "                self.scaler.step(self.optimizer) # This is a replacement for optimizer.step()\n",
        "                self.scaler.update() # This is something added just for FP16\n",
        "                if self.config[\"scheduler\"] == 'CosineAnnealingLR':\n",
        "                    self.scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "\n",
        "                batch_bar.update() # Update tqdm bar                \n",
        "\n",
        "            batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "            trainacc = 100 * num_correct / (len(self.train_loader) * batch_size)\n",
        "            trainlos = float(total_loss / len(self.train_loader))\n",
        "            trainlra = float(self.optimizer.param_groups[0]['lr'])\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}: Train Acc {trainacc:.04f}%, Train Loss {trainlos:.04f}, Learning Rate {trainlra:.04f}\")\n",
        "            print(f\"Total Loss = {total_loss}, Train Loader = {len(self.train_loader)}\")\n",
        "\n",
        "            if self.log:\n",
        "                    wandb.log({\n",
        "                        \"Training Accuracy\": trainacc,\n",
        "                        \"Training Loss\": trainlos,\n",
        "                        \"Num Correct\": num_correct,\n",
        "                        \"Learning Rate\": trainlra\n",
        "                            })\n",
        "            if self.config[\"scheduler\"] == 'ReduceLRonPlateau':\n",
        "                val_acc = self.validate(self.model)\n",
        "                self.model.train()\n",
        "                self.scheduler.step(val_acc) # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "            elif validate:\n",
        "                val_acc = self.validate(self.model)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                self.save_model(epoch)\n",
        "            \n",
        "            # Save Checkpoint\n",
        "            self.save_checkpoint(epoch + chkpt + 1, self.model, self.optimizer, total_loss / len(self.train_loader))\n",
        "    \n",
        "        return self.model\n",
        "        \n",
        "    def validate(self, val_model):\n",
        "        val_model.eval()\n",
        "        batch_bar = tqdm(total=len(self.val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "        num_correct = 0\n",
        "        for i, (x, y) in enumerate(self.val_loader):\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = val_model(x)\n",
        "\n",
        "            num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "            batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * self.config['batch_size'])))\n",
        "\n",
        "            batch_bar.update()\n",
        "            \n",
        "        batch_bar.close()\n",
        "        val_acc = 100 * num_correct / len(self.val_dataset)\n",
        "        print(\"\\nValidation: {:.04f}%\".format(val_acc))\n",
        "        if self.log:\n",
        "            wandb.log({\"Validation Accuracy\": val_acc})\n",
        "\n",
        "        return val_acc\n",
        "\n",
        "    def save_checkpoint(self, epoch, model, optimizer, loss):\n",
        "        print(\"\\nSaving Checkpoint!\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(epoch) + '.pth'))\n",
        "\n",
        "    def save_model(self, epoch, onnx=False):\n",
        "        # if save_best:\n",
        "        #     torch.save(self.model.state_dict(), os.path.join(self.model_path, \"best_model.pth\"))\n",
        "        # else:\n",
        "        print(\"\\nSaving Model!\")\n",
        "        name = os.path.join(self.model_path, \"model_\" + str(epoch) + \".pth\")\n",
        "        torch.save(self.model.state_dict(), name)\n",
        "        if onnx:\n",
        "            torch.onnx.export(self.model, name.split('.')[0] + '.onnx')\n",
        "            wandb.save(name.split('.')[0] + '.onnx')\n",
        "\n",
        "        print(\"Model saved at : \", self.model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run(config, val, folder = 'working'):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # FaceNet\n",
        "    folder_path = r'/content/cmudrive/IDL/hw2-' + folder\n",
        "    face = FaceNetSetup(config, save_path =folder_path)\n",
        "    face.setup()\n",
        "    # Model Training\n",
        "    facenet_model = face.train(validate=val)\n",
        "    # Save Trained Model\n",
        "    face.save_model()\n",
        "    # Validation\n",
        "    face.validate(facenet_model)\n",
        "    if face.log:\n",
        "        wandb.finish()\n",
        "\n",
        "    return face\n",
        "\n",
        "def loop_run(config, tests, folder):\n",
        "    for param in tests:\n",
        "        p, v = list(param.items())[0]\n",
        "        if isinstance(v, list):\n",
        "            config[p] = v[0]\n",
        "            config['optim'] = v[1]\n",
        "        else:\n",
        "            config[p] = v\n",
        "        print(config)\n",
        "\n",
        "        face = run(config, True, folder=folder)\n",
        "\n",
        "        del face.model\n"
      ],
      "metadata": {
        "id": "gBvT9fX7YXlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters and Run"
      ],
      "metadata": {
        "id": "LydVUNlzgPfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config = {\n",
        "#     '': 'convnext',\n",
        "#     'batch_size': 256,\n",
        "#     'transforms': True,\n",
        "#     'epochs': 100,\n",
        "#     'backbone': 'convnext',\n",
        "#     'dropout': None,\n",
        "#     'optimizer': 'SGD',         # SGD, Adam, AdamW\n",
        "#     'loss': 'CrossEL',          # CrossEL, \n",
        "#     'optim': {'lr': 0.1, 'momentum':0.9, 'weight_decay':1e-4m},\n",
        "#     'scheduler': 'CosineAnnealingLR', \n",
        "#     'subset': False,\n",
        "#     'save': True,\n",
        "#     'log': True,\n",
        "#     'randomize': False,\n",
        "# }\n",
        "\n",
        "# folder_path = r'/content/cmudrive/IDL/hw2-' + 'letsgo'\n",
        "# face = FaceNetSetup(config, save_path=folder_path)\n",
        "# face.setup(continue_train=True, chkpt=67)\n",
        "# face.train(continue_train=True, validate=False)"
      ],
      "metadata": {
        "id": "TA1ll5igWyXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    '': 'ConvNext',\n",
        "    'batch_size': 256,\n",
        "    'transforms': True,\n",
        "    'epochs': 50,\n",
        "    'backbone': 'convnext',\n",
        "    'dropout': None,\n",
        "    'optimizer': 'SGD',                 # SGD, Adam, AdamW\n",
        "    'loss': 'CrossEL',                  # CrossEL\n",
        "    'scheduler': 'ReduceLRonPlateau',   # CosineAnnealingLR, ReduceLRonPlateau\n",
        "    'optim': {'lr': 0.1, 'momentum':0.9, 'weight_decay':1e-4, 'nesterov':True},\n",
        "    'subset': False,\n",
        "    'save': True,\n",
        "    'log': True,\n",
        "    'randomize': False,\n",
        "}"
      ],
      "metadata": {
        "id": "zX9G_Xe_PNkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop Test\n",
        "# tests = [\n",
        "    # {'transforms': False}, \n",
        "    # {'transforms': True}, \n",
        "    # {'optimizer': ['Adam', {'lr':0.01}]}, \n",
        "    # {'optimizer': ['AdamW', {'lr':0.01, 'weight_decay':1e-4}]}\n",
        "# ]\n",
        "\n",
        "# Standalone Test\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# # FaceNet\n",
        "# folder_path = r'/content/cmudrive/IDL/hw2-' + 'please'\n",
        "# face = FaceNetSetup(config, save_path = folder_path)\n",
        "# face.setup()\n",
        "\n",
        "# # Model Training\n",
        "# facenet_model = face.train(validate=True)\n",
        "\n",
        "# # Save Trained Model\n",
        "# face.save_model()\n",
        "\n",
        "# # Validation\n",
        "# face.validate(facenet_model)\n",
        "\n",
        "# if face.log:\n",
        "#     wandb.finish()\n",
        "\n",
        "\n",
        "folder_path = r'/content/cmudrive/IDL/hw2-' + 'please'\n",
        "face = FaceNetSetup(config, save_path=folder_path)\n",
        "face.setup(continue_train=True, chkpt=50)\n",
        "face.train(continue_train=True, validate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gAI6SFMtXIzO",
        "outputId": "34bc1c0d-be5b-4bd4-abe4-24e145105fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "ââSequential: 1-1                        [-1, 96, 56, 56]          --\n",
            "|    ââConv2d: 2-1                       [-1, 96, 56, 56]          4,704\n",
            "|    ââBatchNorm2d: 2-2                  [-1, 96, 56, 56]          192\n",
            "ââSequential: 1-2                        [-1, 768, 7, 7]           --\n",
            "|    ââIdentity: 2-3                     [-1, 96, 56, 56]          --\n",
            "|    ââConvNextBlock: 2-4                [-1, 96, 56, 56]          --\n",
            "|    |    ââSequential: 3-1              [-1, 96, 56, 56]          4,896\n",
            "|    |    ââSequential: 3-2              [-1, 384, 56, 56]         36,864\n",
            "|    |    ââSequential: 3-3              [-1, 96, 56, 56]          36,864\n",
            "|    ââConvNextBlock: 2-5                [-1, 96, 56, 56]          --\n",
            "|    |    ââSequential: 3-4              [-1, 96, 56, 56]          4,896\n",
            "|    |    ââSequential: 3-5              [-1, 384, 56, 56]         36,864\n",
            "|    |    ââSequential: 3-6              [-1, 96, 56, 56]          36,864\n",
            "|    ââConvNextBlock: 2-6                [-1, 96, 56, 56]          --\n",
            "|    |    ââSequential: 3-7              [-1, 96, 56, 56]          4,896\n",
            "|    |    ââSequential: 3-8              [-1, 384, 56, 56]         36,864\n",
            "|    |    ââSequential: 3-9              [-1, 96, 56, 56]          36,864\n",
            "|    ââSequential: 2-7                   [-1, 192, 28, 28]         --\n",
            "|    |    ââBatchNorm2d: 3-10            [-1, 96, 56, 56]          192\n",
            "|    |    ââConv2d: 3-11                 [-1, 192, 28, 28]         73,920\n",
            "|    ââConvNextBlock: 2-8                [-1, 192, 28, 28]         --\n",
            "|    |    ââSequential: 3-12             [-1, 192, 28, 28]         9,792\n",
            "|    |    ââSequential: 3-13             [-1, 768, 28, 28]         147,456\n",
            "|    |    ââSequential: 3-14             [-1, 192, 28, 28]         147,456\n",
            "|    ââConvNextBlock: 2-9                [-1, 192, 28, 28]         --\n",
            "|    |    ââSequential: 3-15             [-1, 192, 28, 28]         9,792\n",
            "|    |    ââSequential: 3-16             [-1, 768, 28, 28]         147,456\n",
            "|    |    ââSequential: 3-17             [-1, 192, 28, 28]         147,456\n",
            "|    ââConvNextBlock: 2-10               [-1, 192, 28, 28]         --\n",
            "|    |    ââSequential: 3-18             [-1, 192, 28, 28]         9,792\n",
            "|    |    ââSequential: 3-19             [-1, 768, 28, 28]         147,456\n",
            "|    |    ââSequential: 3-20             [-1, 192, 28, 28]         147,456\n",
            "|    ââSequential: 2-11                  [-1, 384, 14, 14]         --\n",
            "|    |    ââBatchNorm2d: 3-21            [-1, 192, 28, 28]         384\n",
            "|    |    ââConv2d: 3-22                 [-1, 384, 14, 14]         295,296\n",
            "|    ââConvNextBlock: 2-12               [-1, 384, 14, 14]         --\n",
            "|    |    ââSequential: 3-23             [-1, 384, 14, 14]         19,584\n",
            "|    |    ââSequential: 3-24             [-1, 1536, 14, 14]        589,824\n",
            "|    |    ââSequential: 3-25             [-1, 384, 14, 14]         589,824\n",
            "|    ââConvNextBlock: 2-13               [-1, 384, 14, 14]         --\n",
            "|    |    ââSequential: 3-26             [-1, 384, 14, 14]         19,584\n",
            "|    |    ââSequential: 3-27             [-1, 1536, 14, 14]        589,824\n",
            "|    |    ââSequential: 3-28             [-1, 384, 14, 14]         589,824\n",
            "|    ââConvNextBlock: 2-14               [-1, 384, 14, 14]         --\n",
            "|    |    ââSequential: 3-29             [-1, 384, 14, 14]         19,584\n",
            "|    |    ââSequential: 3-30             [-1, 1536, 14, 14]        589,824\n",
            "|    |    ââSequential: 3-31             [-1, 384, 14, 14]         589,824\n",
            "|    ââConvNextBlock: 2-15               [-1, 384, 14, 14]         --\n",
            "|    |    ââSequential: 3-32             [-1, 384, 14, 14]         19,584\n",
            "|    |    ââSequential: 3-33             [-1, 1536, 14, 14]        589,824\n",
            "|    |    ââSequential: 3-34             [-1, 384, 14, 14]         589,824\n",
            "|    ââConvNextBlock: 2-16               [-1, 384, 14, 14]         --\n",
            "|    |    ââSequential: 3-35             [-1, 384, 14, 14]         19,584\n",
            "|    |    ââSequential: 3-36             [-1, 1536, 14, 14]        589,824\n",
            "|    |    ââSequential: 3-37             [-1, 384, 14, 14]         589,824\n",
            "|    ââConvNextBlock: 2-17               [-1, 384, 14, 14]         --\n",
            "|    |    ââSequential: 3-38             [-1, 384, 14, 14]         19,584\n",
            "|    |    ââSequential: 3-39             [-1, 1536, 14, 14]        589,824\n",
            "|    |    ââSequential: 3-40             [-1, 384, 14, 14]         589,824\n",
            "|    ââConvNextBlock: 2-18               [-1, 384, 14, 14]         --\n",
            "|    |    ââSequential: 3-41             [-1, 384, 14, 14]         19,584\n",
            "|    |    ââSequential: 3-42             [-1, 1536, 14, 14]        589,824\n",
            "|    |    ââSequential: 3-43             [-1, 384, 14, 14]         589,824\n",
            "|    ââConvNextBlock: 2-19               [-1, 384, 14, 14]         --\n",
            "|    |    ââSequential: 3-44             [-1, 384, 14, 14]         19,584\n",
            "|    |    ââSequential: 3-45             [-1, 1536, 14, 14]        589,824\n",
            "|    |    ââSequential: 3-46             [-1, 384, 14, 14]         589,824\n",
            "|    ââConvNextBlock: 2-20               [-1, 384, 14, 14]         --\n",
            "|    |    ââSequential: 3-47             [-1, 384, 14, 14]         19,584\n",
            "|    |    ââSequential: 3-48             [-1, 1536, 14, 14]        589,824\n",
            "|    |    ââSequential: 3-49             [-1, 384, 14, 14]         589,824\n",
            "|    ââSequential: 2-21                  [-1, 768, 7, 7]           --\n",
            "|    |    ââBatchNorm2d: 3-50            [-1, 384, 14, 14]         768\n",
            "|    |    ââConv2d: 3-51                 [-1, 768, 7, 7]           1,180,416\n",
            "|    ââConvNextBlock: 2-22               [-1, 768, 7, 7]           --\n",
            "|    |    ââSequential: 3-52             [-1, 768, 7, 7]           39,168\n",
            "|    |    ââSequential: 3-53             [-1, 3072, 7, 7]          2,359,296\n",
            "|    |    ââSequential: 3-54             [-1, 768, 7, 7]           2,359,296\n",
            "|    ââConvNextBlock: 2-23               [-1, 768, 7, 7]           --\n",
            "|    |    ââSequential: 3-55             [-1, 768, 7, 7]           39,168\n",
            "|    |    ââSequential: 3-56             [-1, 3072, 7, 7]          2,359,296\n",
            "|    |    ââSequential: 3-57             [-1, 768, 7, 7]           2,359,296\n",
            "|    ââConvNextBlock: 2-24               [-1, 768, 7, 7]           --\n",
            "|    |    ââSequential: 3-58             [-1, 768, 7, 7]           39,168\n",
            "|    |    ââSequential: 3-59             [-1, 3072, 7, 7]          2,359,296\n",
            "|    |    ââSequential: 3-60             [-1, 768, 7, 7]           2,359,296\n",
            "ââBatchNorm2d: 1-3                       [-1, 768, 7, 7]           1,536\n",
            "ââSequential: 1                          []                        --\n",
            "|    ââAdaptiveAvgPool2d: 2-25           [-1, 768, 1, 1]           --\n",
            "|    ââFlatten: 2-26                     [-1, 768]                 --\n",
            "ââSequential: 1-4                        [-1, 7000]                --\n",
            "|    ââAdaptiveAvgPool2d: 2-27           [-1, 768, 1, 1]           --\n",
            "|    ââFlatten: 2-28                     [-1, 768]                 --\n",
            "|    ââLinear: 2-29                      [-1, 7000]                5,383,000\n",
            "==========================================================================================\n",
            "Total params: 33,156,760\n",
            "Trainable params: 33,156,760\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 4.55\n",
            "==========================================================================================\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 125.52\n",
            "Params size (MB): 126.48\n",
            "Estimated Total Size (MB): 252.58\n",
            "==========================================================================================\n",
            "Number of Params: 33156760\n",
            "\n",
            "Model Name:  Full_ConvNext_b256_tTrue_e50_bconvnext_dNone_oSGD_lCrossEL_sReduceLRonPlateau_lr0.1\n",
            "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n",
            "Continuing training from epoch  50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnefario7\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/wandb/run-20220315_181328-edlzxx9i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/nefario7/hw2-letsgo/runs/edlzxx9i\" target=\"_blank\">fluent-gorge-23</a></strong> to <a href=\"https://wandb.ai/nefario7/hw2-letsgo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------Epoch 0-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50: Train Acc 99.0113%, Train Loss 1.4731, Learning Rate 0.0063\n",
            "Total Loss = 804.28759765625, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 85.9543%\n",
            "\n",
            "Saving Model!\n",
            "Model saved at :  /content/cmudrive/IDL/hw2-please/Full_ConvNext_b256_tTrue_e50_bconvnext_dNone_oSGD_lCrossEL_sReduceLRonPlateau_lr0.1\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 1-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50: Train Acc 99.0685%, Train Loss 1.4697, Learning Rate 0.0063\n",
            "Total Loss = 802.434326171875, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 85.8514%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 2-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50: Train Acc 99.0256%, Train Loss 1.4719, Learning Rate 0.0063\n",
            "Total Loss = 803.6770629882812, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.1771%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 3-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50: Train Acc 99.0220%, Train Loss 1.4693, Learning Rate 0.0063\n",
            "Total Loss = 802.2288208007812, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.1229%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 4-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50: Train Acc 99.0399%, Train Loss 1.4682, Learning Rate 0.0063\n",
            "Total Loss = 801.661376953125, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 85.9886%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 5-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50: Train Acc 99.0063%, Train Loss 1.4679, Learning Rate 0.0063\n",
            "Total Loss = 801.4757690429688, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.1457%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 6-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50: Train Acc 99.1393%, Train Loss 1.4578, Learning Rate 0.0031\n",
            "Total Loss = 795.959716796875, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.6057%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 7-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50: Train Acc 99.1758%, Train Loss 1.4555, Learning Rate 0.0031\n",
            "Total Loss = 794.7139282226562, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.5657%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 8-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50: Train Acc 99.1293%, Train Loss 1.4564, Learning Rate 0.0031\n",
            "Total Loss = 795.1715698242188, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.4943%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 9-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50: Train Acc 99.1272%, Train Loss 1.4558, Learning Rate 0.0031\n",
            "Total Loss = 794.8919677734375, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.5286%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 10-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50: Train Acc 99.1851%, Train Loss 1.4500, Learning Rate 0.0016\n",
            "Total Loss = 791.7093505859375, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.6571%\n",
            "\n",
            "Saving Model!\n",
            "Model saved at :  /content/cmudrive/IDL/hw2-please/Full_ConvNext_b256_tTrue_e50_bconvnext_dNone_oSGD_lCrossEL_sReduceLRonPlateau_lr0.1\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 11-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50: Train Acc 99.1873%, Train Loss 1.4506, Learning Rate 0.0016\n",
            "Total Loss = 792.03173828125, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8371%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 12-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50: Train Acc 99.2109%, Train Loss 1.4484, Learning Rate 0.0016\n",
            "Total Loss = 790.8237915039062, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.6886%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 13-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50: Train Acc 99.2230%, Train Loss 1.4476, Learning Rate 0.0016\n",
            "Total Loss = 790.3978271484375, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.7457%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 14-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50: Train Acc 99.1994%, Train Loss 1.4485, Learning Rate 0.0016\n",
            "Total Loss = 790.870849609375, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.7000%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 15-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50: Train Acc 99.2202%, Train Loss 1.4466, Learning Rate 0.0008\n",
            "Total Loss = 789.86669921875, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8429%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 16-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50: Train Acc 99.1994%, Train Loss 1.4473, Learning Rate 0.0008\n",
            "Total Loss = 790.2464599609375, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8229%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 17-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50: Train Acc 99.2517%, Train Loss 1.4454, Learning Rate 0.0008\n",
            "Total Loss = 789.1870727539062, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8429%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 18-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50: Train Acc 99.2152%, Train Loss 1.4451, Learning Rate 0.0004\n",
            "Total Loss = 789.0362548828125, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8457%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 19-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50: Train Acc 99.2352%, Train Loss 1.4444, Learning Rate 0.0004\n",
            "Total Loss = 788.6648559570312, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8800%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 20-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50: Train Acc 99.2137%, Train Loss 1.4449, Learning Rate 0.0004\n",
            "Total Loss = 788.9308471679688, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.9143%\n",
            "\n",
            "Saving Model!\n",
            "Model saved at :  /content/cmudrive/IDL/hw2-please/Full_ConvNext_b256_tTrue_e50_bconvnext_dNone_oSGD_lCrossEL_sReduceLRonPlateau_lr0.1\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 21-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50: Train Acc 99.2595%, Train Loss 1.4441, Learning Rate 0.0004\n",
            "Total Loss = 788.4917602539062, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8457%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 22-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50: Train Acc 99.1916%, Train Loss 1.4444, Learning Rate 0.0004\n",
            "Total Loss = 788.6183471679688, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8571%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 23-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50: Train Acc 99.2288%, Train Loss 1.4436, Learning Rate 0.0004\n",
            "Total Loss = 788.2271118164062, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8314%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 24-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50: Train Acc 99.2552%, Train Loss 1.4435, Learning Rate 0.0002\n",
            "Total Loss = 788.138671875, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8514%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 25-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50: Train Acc 99.2624%, Train Loss 1.4424, Learning Rate 0.0002\n",
            "Total Loss = 787.55712890625, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8714%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 26-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50: Train Acc 99.2731%, Train Loss 1.4422, Learning Rate 0.0002\n",
            "Total Loss = 787.4480590820312, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8914%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 27-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50: Train Acc 99.2338%, Train Loss 1.4427, Learning Rate 0.0001\n",
            "Total Loss = 787.6932373046875, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.9057%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 28-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50: Train Acc 99.2438%, Train Loss 1.4426, Learning Rate 0.0001\n",
            "Total Loss = 787.6332397460938, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8857%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 29-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50: Train Acc 99.2531%, Train Loss 1.4423, Learning Rate 0.0001\n",
            "Total Loss = 787.5050659179688, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8771%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 30-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50: Train Acc 99.2445%, Train Loss 1.4432, Learning Rate 0.0000\n",
            "Total Loss = 787.9873657226562, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.9229%\n",
            "\n",
            "Saving Model!\n",
            "Model saved at :  /content/cmudrive/IDL/hw2-please/Full_ConvNext_b256_tTrue_e50_bconvnext_dNone_oSGD_lCrossEL_sReduceLRonPlateau_lr0.1\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 31-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50: Train Acc 99.2710%, Train Loss 1.4421, Learning Rate 0.0000\n",
            "Total Loss = 787.37548828125, Train Loader = 546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation: 86.8914%\n",
            "\n",
            "Saving Checkpoint!\n",
            "-------------------------Epoch 32-------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  55%|ââââââ    | 301/546 [08:56<07:05,  1.74s/it, acc=99.2395%, loss=1.4424, lr=0.0000, num_correct=76470]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-cb56174ff063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceNetSetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinue_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchkpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinue_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-8ba9fbc675f1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, validate, continue_train)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# Update # correct & loss as we go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Classification Task: Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08Zv2AWFrfVP"
      },
      "outputs": [],
      "source": [
        "class ClassificationTestSet(Dataset):\n",
        "    # It's possible to load test set data using ImageFolder without making a custom class.\n",
        "    # See if you can think it through!\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationSubmission():\n",
        "    def __init__(self, data_path, csv_path):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.drive_dir = r'/content/cmudrive/IDL'\n",
        "        self.DATA_DIR = r\"/content\" \n",
        "        self.TEST_DIR = osp.join(self.DATA_DIR, r\"classification/classification/test\")\n",
        "\n",
        "    def __get_labels(self, imodel, iargs):\n",
        "        imodel.eval()\n",
        "        labels = []\n",
        "        print(f\"Context = {iargs['context']} | Batch Size = {iargs['batch_size']} | Arch = {iargs['arch']}\")\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(self.test_samples)):\n",
        "                X = self.test_samples[i]\n",
        "                test_items = SubmissionItems(X, context=iargs['context'])\n",
        "                test_loader = torch.utils.data.DataLoader(test_items, batch_size=iargs['batch_size'], num_workers=2, pin_memory=True, shuffle=False)\n",
        "\n",
        "                for data in tqdm(test_loader):\n",
        "                    data = data.float().to(self.device)              \n",
        "                    output = imodel(data)\n",
        "                    y = torch.argmax(output, axis=1)\n",
        "                    labels.extend(y.tolist())\n",
        "        return labels\n",
        "\n",
        "    def __load_model(self, model_name, model_type): \n",
        "        meta_path = os.path.join(self.drive_dir,  model_type, model_name, 'model_parameters.yaml')\n",
        "        with open(meta_path, 'r') as meta:\n",
        "            args = yaml.safe_load(meta)\n",
        "\n",
        "        model_path = os.path.join(self.drive_dir, model_type, model_name, 'model.pth')\n",
        "        model = Network(args[\"arch\"], args['context'], args['drop']).to(self.device)\n",
        "        # summary(model)\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        return model, args\n",
        "\n",
        "    def simple_inference(self, model_name, model_type):\n",
        "        print(\"Running inference...\")\n",
        "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        model, args = self.__load_model(model_name, model_type)\n",
        "        labels = self.__get_labels(model, args)\n",
        "        \n",
        "        return labels\n",
        "\n",
        "    def ensemble_inference(self, model_names, model_type):\n",
        "        print(\"Running ensembled inference...\")\n",
        "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "        prelim_labels = []\n",
        "        for name in model_names:\n",
        "            print(\"\\n\\n\\tModel : \", name)\n",
        "            model, args = self.__load_model(name, model_type)\n",
        "            prelim_labels.append(self.__get_labels(model, args))\n",
        "\n",
        "        accs = [86.146, 85.79, 84.95]\n",
        "        w = accs / np.sum(accs)\n",
        "\n",
        "        print(\"Combining predictions...\")\n",
        "        labels_df = pd.DataFrame(prelim_labels)\n",
        "        labels_df = labels_df.transpose()\n",
        "        ensembled_labels = labels_df.mode(axis=1, dropna=False).iloc[:, 0].tolist()\n",
        "        # ensembled_labels = np.where((df.iloc[:,1] == df.iloc[:, 2]), df.iloc[:, 1], df.iloc[:, 0]).tolist()\n",
        "\n",
        "        return labels_df, ensembled_labels\n",
        "\n",
        "    def generate_submission(self, save_path, labels): \n",
        "        sub_dir = os.path.join(self.drive_dir, save_path + self.timestamp)\n",
        "        sub_path = os.path.join(sub_dir, 'submission.csv')\n",
        "\n",
        "        with open(r\"/content/classification_early_submission.csv\", \"w+\") as f:\n",
        "            f.write(\"id,label\\n\")\n",
        "            for i in tqdm(range(len(test_dataset))):\n",
        "                f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))\n",
        "\n",
        "        print(f\"File saved at : {sub_path}\")\n",
        "        return sub_path"
      ],
      "metadata": {
        "id": "BmT-HNdOfyOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "LffOR234mUht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td_qvGwr16z0"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = r\"/content\"\n",
        "TEST_DIR = osp.join(DATA_DIR, r\"classification/classification/test\")\n",
        "val_transforms = [ttf.ToTensor()]\n",
        "\n",
        "test_dataset = ClassificationTestSet(TEST_DIR, ttf.Compose(val_transforms))\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, drop_last=False, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2WQEUjXkWvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd6afd9-8faf-432e-bb80-855bd32e45ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNext(\n",
              "  (stem): Sequential(\n",
              "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
              "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layers): Sequential(\n",
              "    (0): Identity()\n",
              "    (1): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (2): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (3): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (5): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
              "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (6): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
              "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (7): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
              "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (8): Sequential(\n",
              "      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (9): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (10): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (11): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (12): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (13): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (14): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (15): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (16): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (17): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (18): Sequential(\n",
              "      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (19): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
              "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (20): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
              "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (21): ConvNextBlock(\n",
              "      (depth_conv): Sequential(\n",
              "        (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
              "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (pw_conv_inc): Sequential(\n",
              "        (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): GELU()\n",
              "      )\n",
              "      (pw_conv_dec): Sequential(\n",
              "        (0): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (cls_layer): Sequential(\n",
              "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Linear(in_features=768, out_features=7000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "folder_path = r'/content/cmudrive/IDL/hw2-please/Full_ConvNext_b256_tTrue_e50_bconvnext_dNone_oSGD_lCrossEL_sReduceLRonPlateau_lr0.1'\n",
        "\n",
        "# Checkpoint Loading\n",
        "# valmodel = ConvNext()\n",
        "# valmodel.cuda()\n",
        "# no = 50\n",
        "# val_path = os.path.join(folder_path, 'Checkpoints', 'chkpt_' + str(no) + '.pth')\n",
        "\n",
        "# Model Loading\n",
        "valmodel = ConvNext()\n",
        "valmodel.cuda()\n",
        "val_path = os.path.join(folder_path, 'model_50.pth')\n",
        "\n",
        "\n",
        "valmodel.load_state_dict(torch.load(val_path))\n",
        "valmodel.eval()\n",
        "# batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "\n",
        "# res = []\n",
        "# for i, (x) in enumerate(test_loader):\n",
        "#     with torch.no_grad():\n",
        "#         x = x.cuda()\n",
        "  \n",
        "#         outputs = valmodel(x)\n",
        "\n",
        "#         y = torch.argmax(outputs, axis=1)\n",
        "#         res.extend(y.tolist())\n",
        "\n",
        "#         batch_bar.update()\n",
        "    \n",
        "# batch_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vob9a2-HkW_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d068bb7c-3831-429b-e378-8925565e6f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 35000/35000 [00:00<00:00, 537011.33it/s]\n"
          ]
        }
      ],
      "source": [
        "with open(r\"/content/classification_submission_convnext.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in tqdm(range(len(test_dataset))):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpxatBfT4jSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b91a89c-57ae-4dbf-bdf9-19ae81bd6091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 541k/541k [00:00<00:00, 948kB/s]\n",
            "Successfully submitted to Face Recognition"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-classification -f /content/classification_submission_convnext.csv -m \"MyConvNext\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsJx1l1T4twC"
      },
      "source": [
        "# Verification Task"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 6K verification dev images, but 166K \"pairs\" for you to compare. So, it's much more efficient to compute the features for the 6K verification images, and just compare afterwards.\n",
        "\n",
        "This will be done by creating a dictionary mapping the image file names to the features. Then, you'll use this dictionary to compute the similarities for each pair."
      ],
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceVerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.anchor_net = ConvNext()\n",
        "        self.positive_net = ConvNext()\n",
        "        self.negative_net = ConvNext()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "tlMo2oAuvIM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # We return the image, as well as the path to that image (relative path)\n",
        "        return self.transforms(Image.open(self.img_paths[idx])), osp.relpath(self.img_paths[idx], self.data_dir)"
      ],
      "metadata": {
        "id": "m1YtIwxuL7H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TripletDataset(torchvision.datasets.VisionDataset):\n",
        "  def __init__(self, root, transform):  \n",
        "    # For \"root\", note that you're making this dataset on top of the regular classification dataset.\n",
        "    self.dataset = torchvision.datasets.ImageFolder(root=root, transform=transform)\n",
        "    \n",
        "    # map class indices to dataset image indices\n",
        "    self.classes_to_img_indices = [[] for _ in range(len(self.dataset.classes))]\n",
        "    for img_idx, (_, class_id) in enumerate(self.dataset.samples):\n",
        "      self.classes_to_img_indices[class_id].append(img_idx)\n",
        "    \n",
        "    # VisionDataset attributes for display\n",
        "    self.root = root\n",
        "    self.length = len(self.dataset.classes) # pseudo length! Length of this dataset is 7000, *not* the actual # of images in the dataset. You can just increase the # of epochs you train for.\n",
        "    self.transforms = self.dataset.transforms\n",
        "          \n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "    \n",
        "  def __getitem__(self, anchor_class_idx):\n",
        "    \"\"\"Treat the given index as the anchor class and pick a triplet randomly\"\"\"\n",
        "    anchor_class = self.classes_to_img_indices[anchor_class_idx]\n",
        "    # choose positive pair (assuming each class has at least 2 images)\n",
        "    anchor, positive = np.random.choice(a=anchor_class, size=2, replace=False)\n",
        "    # choose negative image\n",
        "    # hint for further exploration: you can choose 2 negative images to make it a Quadruplet Loss\n",
        "\n",
        "    classes_to_choose_negative_class_from = list(range(self.length))\n",
        "    classes_to_choose_negative_class_from.pop(???) # TODO: What are we removing?\n",
        "    negative_class = # TODO: How do we randomly choose a negative class?\n",
        "    negative = # TODO: How do we get a sample from that negative class?\n",
        "    \n",
        "    # self.dataset[idx] will return a tuple (image tensor, class label). You can use its outputs to train for classification alongside verification\n",
        "    # If you do not want to train for classification, you can use self.dataset[idx][0] to get the image tensor\n",
        "    return self.dataset[anchor], self.dataset[positive], self.dataset[negative]\n"
      ],
      "metadata": {
        "id": "UqPt26qO06rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triplets = TripletDataset(root='classification/classification/dev')"
      ],
      "metadata": {
        "id": "DUFlNl851GaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98lmjm0S4tHR"
      },
      "outputs": [],
      "source": [
        "val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"), ttf.Compose([ttf.ToTensor()]))\n",
        "val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(val_ver_loader), total=len(val_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try the final outputs too!\n",
        "        features = valmodel(imgs, return_feats=False)\n",
        "        gelu = nn.GELU()\n",
        "        features = gelu(features) \n",
        "        for i, feature in enumerate(features):\n",
        "            feats_dict[path_names[i]] = feature\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qw45H-eMyyn",
        "outputId": "4710ed18-cb6b-4bab-c519-a3b3a90ab63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What does this dict look like?\n",
        "print(list(feats_dict.items())[0][1].shape)"
      ],
      "metadata": {
        "id": "k6TG6RD6NTtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df93ac30-1ff2-4682-bad4-9746ca5ac615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "cosine_sim = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_dev.csv\")\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "gt_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2, gt = line.split(\",\")\n",
        "    img_path1 = img_path1.split('/')[-1]\n",
        "    img_path2 = img_path2.split('/')[-1]\n",
        "    feat1 = feats_dict[img_path1]\n",
        "    feat2 = feats_dict[img_path2]\n",
        "\n",
        "    # TODO: Use the similarity metric\n",
        "    sim_score = cosine_sim(feat1, feat2)\n",
        "    pred_similarities.append(sim_score.item())\n",
        "    gt_similarities.append(int(gt))\n",
        "\n",
        "pred_similarities = np.array(pred_similarities)\n",
        "gt_similarities = np.array(gt_similarities)\n",
        "\n",
        "print(\"AUC:\", roc_auc_score(gt_similarities, pred_similarities))"
      ],
      "metadata": {
        "id": "_zuqds2qNO6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf7dba6-8d3a-4ade-c950-f1eb59696041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9629680014176387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verification Task: Submit to Kaggle"
      ],
      "metadata": {
        "id": "sakRa8oZOlKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/test\"), ttf.Compose([ttf.ToTensor()]))\n",
        "test_ver_loader = torch.utils.data.DataLoader(test_veri_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "oDK3knDcOrOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valmodel.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(test_ver_loader), total=len(test_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try to final outputs too!\n",
        "        feats = valmodel(imgs, return_feats=False)\n",
        "        gelu = nn.GELU()\n",
        "        feats = gelu(feats) \n",
        "        for i, feat in enumerate(feats):\n",
        "            feats_dict[path_names[i]] = feat\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igeRT3WxOrB_",
        "outputId": "98c86b80-308b-42b7-bf64-fe3df6f31ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "cosine_sim = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_test.csv\")\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2 = line.split(\",\")\n",
        "    img_path1 = img_path1.split('/')[-1]\n",
        "    img_path2 = img_path2.split('/')[-1]\n",
        "    feat1 = feats_dict[img_path1]\n",
        "    feat2 = feats_dict[img_path2]\n",
        "    sim_score = cosine_sim(feat1, feat2)\n",
        "    pred_similarities.append(sim_score.item())\n",
        "\n",
        "    # TODO: Finish up verification testing.\n",
        "    # How to use these img_paths? What to do with the features?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4OZL_FNOq1r",
        "outputId": "218c917e-ec4b-4a3b-8b54-778401bc2d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r\"/content/verification_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,match\\n\")\n",
        "    for i in range(len(pred_similarities)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_similarities[i]))"
      ],
      "metadata": {
        "id": "fYXiglWkPBDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5zB7P8O687N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a663e3a0-3fd0-4456-f827-8ed63d69e5aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 16.7M/16.7M [00:00<00:00, 43.3MB/s]\n",
            "Successfully submitted to Face Verification"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-verification -f /content/verification_submission.csv -m 'MyConvNext'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zttLq9nw1aMP",
        "mIqmojPaWD0H",
        "lIHtrzBISdVx",
        "sI3KJtVvypyZ",
        "yvXHNZRKOlDS",
        "sakRa8oZOlKr"
      ],
      "name": "Continue ConvNext HW2P2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "- Work on Data Augmentation\n",
        "- Clean up Submission CSV code"
      ],
      "metadata": {
        "id": "wKOjRbuQi94R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive and Download Data"
      ],
      "metadata": {
        "id": "e9WhS16VCva6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output \n",
        "! apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "! add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "! apt-get update -qq 2>&1 > /dev/null\n",
        "! apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "\n",
        "! google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "! echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "% cd /content\n",
        "! mkdir cmudrive\n",
        "% cd ..\n",
        "! google-drive-ocamlfuse /content/cmudrive\n",
        "! pip install kaggle wandb torch-summary\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/cmudrive/IDL/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggleÂ \n",
        "! kaggle config set -n path -v /content\n",
        "\n",
        "! wandb login\n",
        "! pip install --upgrade --force-reinstall --no-deps albumentations\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "KSB0Pzy3Cu2o"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "! kaggle competitions download -c 11-785-s22-hw2p2-verification\n",
        "\n",
        "! unzip -q /content/competitions/11-785-s22-hw2p2-classification/11-785-s22-hw2p2-classification.zip -d /content\n",
        "! unzip -q /content/competitions/11-785-s22-hw2p2-verification/11-785-s22-hw2p2-verification.zip -d /content\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "ipZZI38GC-Sj"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "zttLq9nw1aMP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "jwLEd0gdPbSc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "import torchvision.models as models\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "# torch.autograd.set_detect_anomaly(False)\n",
        "# torch.autograd.profiler.profile(False)\n",
        "# torch.autograd.profiler.emit_nvtx(False)"
      ],
      "metadata": {
        "id": "uDoTcOeI1bB9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "f5e2045b-c228-44b3-b905-30c697376aeb"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-191-2c9b5fc69a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExponentialLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToTensorV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ToTensorV2' from 'albumentations.pytorch' (/usr/local/lib/python3.7/dist-packages/albumentations/pytorch/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "oIiHf52qfh9D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "## FaceNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "Ny-mh_ocWIJR"
      },
      "outputs": [],
      "source": [
        "class FaceNet(nn.Module):\n",
        "    \"\"\"\n",
        "    The Very Low early deadline architecture is a 4-layer CNN.\n",
        "    The first Conv layer has 64 channels, kernel size 7, and stride 4.\n",
        "    The next three have 128, 256, and 512 channels. Each have kernel size 3 and stride 2.\n",
        "    Think about what the padding should be for each layer to not change spatial resolution.\n",
        "    Each Conv layer is accompanied by a Batchnorm and ReLU layer.\n",
        "    Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1.\n",
        "    Then, remove (Flatten?) these trivial 1x1 dimensions away.\n",
        "    Look through https://pytorch.org/docs/stable/nn.html \n",
        "    TODO: Fill out the model definition below! \n",
        "\n",
        "    Why does a very simple network have 4 convolutions?\n",
        "    Input images are 224x224. Note that each of these convolutions downsample.\n",
        "    Downsampling 2x effectively doubles the receptive field, increasing the spatial\n",
        "    region each pixel extracts features from. Downsampling 32x is standard\n",
        "    for most image models.\n",
        "\n",
        "    Why does a very simple network have high channel sizes?\n",
        "    Every time you downsample 2x, you do 4x less computation (at same channel size).\n",
        "    To maintain the same level of computation, you 2x increase # of channels, which \n",
        "    increases computation by 4x. So, balances out to same computation.\n",
        "    Another intuition is - as you downsample, you lose spatial information. Want\n",
        "    to preserve some of it in the channel dimension.\n",
        "    \"\"\"\n",
        "    def list_to_kwarg(self, inc, outc, kernel, s, p):\n",
        "        params = dict()\n",
        "        params[\"in_channels\"] = inc\n",
        "        params[\"out_channels\"] = outc\n",
        "        params[\"kernel_size\"] = kernel\n",
        "        params[\"stride\"] = s\n",
        "        params[\"padding\"] = p\n",
        "        return params\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Note that first conv is stride 4. It is (was?) standard to downsample.\n",
        "        # 4x early on, as with 224x224 images, 4x4 patches are just low-level details.\n",
        "\n",
        "        # Food for thoughts: \n",
        "        # ? Why is the first conv kernel size 7, not kernel size 3?\n",
        "        # ? Use activation before or after Pooling?\n",
        "        num_classes = 7000\n",
        "        num_channels = 3\n",
        "        layers = []\n",
        "        if config['backbone'] == 'simple':\n",
        "            for l_idx, l_params in config['arch'].items():\n",
        "                conv_params = self.list_to_kwarg(*l_params[\"conv\"])\n",
        "                layers.append(nn.Conv2d(**conv_params))\n",
        "                layers.append(nn.BatchNorm2d(conv_params[\"out_channels\"]))\n",
        "                layers.append(nn.ReLU())\n",
        "                if l_params[\"pool\"] is not None:\n",
        "                    if l_params[\"pool\"][\"max\"]:\n",
        "                        layers.append(nn.AdaptiveMaxPool2d(l_params[\"pool\"][\"output\"]))\n",
        "                    else:\n",
        "                        layers.append(nn.AdaptiveAvgPool2d(l_params[\"pool\"][\"output\"]))\n",
        "            layers.append(nn.Flatten())\n",
        "            self.backbone = nn.Sequential(*layers)\n",
        "            self.cls_layer = nn.Linear(512, num_classes)\n",
        "        else:\n",
        "            if config['backbone'] == 'resnet_18':\n",
        "                resnet_net = models.resnet18(pretrained=True)\n",
        "                oc = 512\n",
        "            elif config['backbone'] == 'resnet_34':\n",
        "                resnet_net = models.resnet34(pretrained=True)\n",
        "                oc = 512\n",
        "\n",
        "            layers = list(resnet_net.children())[:-2]\n",
        "            layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "            layers.append(nn.Flatten())\n",
        "\n",
        "            self.backbone = nn.Sequential(*layers)\n",
        "            self.backbone.out_channels = oc\n",
        "            self.cls_layer = nn.Linear(self.backbone.out_channels, num_classes)\n",
        "    \n",
        "    def forward(self, x, return_feats=False):\n",
        "        \"\"\"\n",
        "        What is return_feats? It essentially returns the second-to-last-layer\n",
        "        features of a given image. It's a \"feature encoding\" of the input image,\n",
        "        and you can use it for the verification task. You would use the outputs\n",
        "        of the final classification layer for the classification task.\n",
        "\n",
        "        You might also find that the classification outputs are sometimes better\n",
        "        for verification too - try both.\n",
        "        \"\"\"\n",
        "        feats = self.backbone(x)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FaceNet Training"
      ],
      "metadata": {
        "id": "gtxmWqOch4-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "UowI9OcUYPjP"
      },
      "outputs": [],
      "source": [
        "from albumentations.augmentations.transforms import HorizontalFlip\n",
        "class FaceNetSetup:\n",
        "    def __init__(self, config, save_path):\n",
        "        self.config = config\n",
        "        self.log = config['log']\n",
        "\n",
        "        if config['subset']:\n",
        "            train_path = r\"train_subset/train_subset\"\n",
        "        else:\n",
        "            train_path = r\"classification/classification/train\"\n",
        "\n",
        "        self.SAVE_DIR = save_path\n",
        "        self.DATA_DIR = r\"/content\" \n",
        "        self.TRAIN_DIR = osp.join(self.DATA_DIR, train_path) \n",
        "        self.VAL_DIR = osp.join(self.DATA_DIR, r\"classification/classification/dev\")\n",
        "\n",
        "    def __check_model_params(self):\n",
        "        num_trainable_parameters = 0\n",
        "        for p in self.model.parameters():\n",
        "            num_trainable_parameters += p.numel()\n",
        "        print(\"Number of Params: {}\".format(num_trainable_parameters))\n",
        "        assert num_trainable_parameters <= 35000000\n",
        "\n",
        "    def __gen_model_name(self):\n",
        "        save_name = ''\n",
        "        if not self.config['subset']:\n",
        "            save_name += \"Full_\"\n",
        "        for key, val in self.config.items():\n",
        "            abbr = key[0] if len(key) > 2 else key\n",
        "            if isinstance(val, dict):\n",
        "                data = 'lr' + str(val[\"lr\"])\n",
        "                save_name += data\n",
        "                break\n",
        "            else:\n",
        "                data = abbr + str(val) + '_'\n",
        "                save_name += data\n",
        "        if self.config['randomize']:\n",
        "            save_name = save_name + \"-v\" + str(np.random.randint(10, 1000))\n",
        "        print(\"\\nModel Name: \", save_name)\n",
        "\n",
        "        return save_name\n",
        "\n",
        "    def __save_model_params(self):\n",
        "        # Create Model Directory\n",
        "        save_path = os.path.join(self.SAVE_DIR, self.model_name)\n",
        "        try:\n",
        "            os.mkdir(save_path)\n",
        "        except FileExistsError:\n",
        "                d = input(\"Model name already exists. Delete existing model? (y/n)\")\n",
        "                if d == 'y':\n",
        "                    import shutil\n",
        "                    shutil.rmtree(save_path)\n",
        "                    os.mkdir(save_path)\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "        os.mkdir(os.path.join(save_path, 'Checkpoints'))\n",
        "        # Saving Model Configuration\n",
        "        with open(os.path.join(save_path, 'model_config.yaml'), 'w') as metadata:\n",
        "            yaml.dump({'Experiment': self.config['']}, metadata, indent=4, default_flow_style=False)\n",
        "            yaml.dump(self.config, metadata, indent=4, default_flow_style=False)\n",
        "        print(\"Model saved at: \", save_path)\n",
        "        return save_path\n",
        "\n",
        "    def __dataloaders(self): \n",
        "        \"\"\"\n",
        "        Transforms (data augmentation) is quite important for this task.\n",
        "        Go explore https://pytorch.org/vision/stable/transforms.html for more details\n",
        "        \"\"\"\n",
        "        if self.config[\"transforms\"] is not None:\n",
        "            # transforms_train = A.Compose([\n",
        "            #     A.Pad(25, padding_mode='symmetric'),\n",
        "            #     A.HorizontalFlip(), \n",
        "            #     A.RandomRotation(10),\n",
        "            #     A.OpticalDistortion(mean, std),\n",
        "            #     A.RandomCrop(width=256, height=256),\n",
        "            #     A.HorizontalFlip(p=0.5),\n",
        "            #     A.RandomBrightnessContrast(p=0.2),\n",
        "            # ])\n",
        "            self.train_transform = A.Compose([\n",
        "                    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.4),\n",
        "                    A.RandomCrop(height=128, width=128),\n",
        "                    A.HorizontalFlip(p=0.5),\n",
        "                    A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.4),\n",
        "                    A.RandomBrightnessContrast(p=0.5),\n",
        "                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "                    ToTensorV2(),])\n",
        "            self.val_transform = A.Compose([\n",
        "                    A.HorizontalFlip(p=0.5),\n",
        "                    A.CenterCrop(height=128, width=128),\n",
        "                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "                    ToTensorV2(),])\n",
        "        else:\n",
        "            self.train_transforms = A.Compose([ToTensorV2()])\n",
        "            self.val_transforms = A.Compose([ToTensorV2()])\n",
        "\n",
        "        self.train_dataset = torchvision.datasets.ImageFolder(self.TRAIN_DIR, transform=self.train_transform)\n",
        "        self.val_dataset = torchvision.datasets.ImageFolder(self.VAL_DIR, transform=self.val_transform)\n",
        "\n",
        "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.config['batch_size'], shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
        "        self.val_loader = DataLoader(self.val_dataset, batch_size=self.config['batch_size'], shuffle=False, drop_last=True, num_workers=1)\n",
        "\n",
        "    def setup(self):\n",
        "        self.__dataloaders()\n",
        "    \n",
        "        # Model\n",
        "        self.model = FaceNet(self.config)\n",
        "        self.model.cuda()\n",
        "        # summary(self.model, (3, 224, 224))\n",
        "        self.__check_model_params()\n",
        "        self.model_name = self.__gen_model_name()\n",
        "        self.model_path = self.__save_model_params()\n",
        "\n",
        "        if self.log:\n",
        "            wandb.init(project=\"hw2-preliminary\", entity=\"nefario7\", config=self.config)\n",
        "\n",
        "        # Loss\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), **self.config['optim'])\n",
        "\n",
        "        # Scheduler\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=(len(self.train_loader) * self.config['epochs']))\n",
        "        # T_max is \"how many times will i call scheduler.step() until it reaches 0 lr?\"\n",
        "\n",
        "        # For this homework, we strongly strongly recommend using FP16 to speed up training.\n",
        "        # It helps more for larger models.\n",
        "        # Go to https://effectivemachinelearning.com/PyTorch/8._Faster_training_with_mixed_precision\n",
        "        # and compare \"Single precision training\" section with \"Mixed precision training\" section\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    def train(self):\n",
        "        epochs = self.config['epochs']\n",
        "        batch_size = self.config['epochs']\n",
        "        if self.log:\n",
        "            wandb.watch(self.model, criterion=self.criterion, log=\"all\", log_freq=batch_size, idx=None,log_graph=True)\n",
        "        for epoch in range(epochs):\n",
        "            print(\"\\n-------------------------------------------------------------\")\n",
        "            # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "            batch_bar = tqdm(total=len(self.train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "            num_correct = 0\n",
        "            total_loss = 0\n",
        "\n",
        "            for i, (x, y) in enumerate(self.train_loader):\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                x = x.cuda()\n",
        "                y = y.cuda()\n",
        "\n",
        "                # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "                with torch.cuda.amp.autocast():     \n",
        "                    outputs = self.model(x)\n",
        "                    loss = self.criterion(outputs, y)\n",
        "\n",
        "                # Update # correct & loss as we go\n",
        "                num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "                total_loss += float(loss)\n",
        "                train_loss = float(total_loss / (i + 1))\n",
        "\n",
        "                if self.log and i % 10 == 0:\n",
        "                    wandb.log({\n",
        "                        \"Training Accuracy\": 100 * num_correct / ((i + 1) * batch_size),\n",
        "                        \"Training Loss\": float(train_loss),\n",
        "                        \"Num Correct\": num_correct,\n",
        "                        \"Learning Rate\": float(self.optimizer.param_groups[0]['lr'])\n",
        "                               })\n",
        "\n",
        "                batch_bar.set_postfix(\n",
        "                    acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "                    loss=\"{:.04f}\".format(train_loss),\n",
        "                    num_correct=num_correct,\n",
        "                    lr=\"{:.04f}\".format(float(self.optimizer.param_groups[0]['lr'])))\n",
        "                \n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer) \n",
        "                self.scaler.update() # This is something added just for FP16\n",
        "\n",
        "                self.scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "                batch_bar.update()\n",
        "\n",
        "            batch_bar.close() # You need this to close the tqdm bar\n",
        "            print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "                epoch + 1,\n",
        "                epochs,\n",
        "                100 * num_correct / (len(self.train_loader) * batch_size),\n",
        "                float(total_loss / len(self.train_loader)),\n",
        "                float(self.optimizer.param_groups[0]['lr'])))\n",
        "            \n",
        "            #Epoch Validation\n",
        "            self.validate(self.model)\n",
        "            \n",
        "            # Save Checkpoint after epoch\n",
        "            self.save_checkpoint(epoch, self.model, self.optimizer, total_loss / len(self.train_loader))\n",
        "            \n",
        "        if self.log:\n",
        "            wandb.finish()\n",
        "            \n",
        "        return self.model\n",
        "\n",
        "    def validate(self, val_model):\n",
        "        val_model.eval()\n",
        "        batch_bar = tqdm(total=len(self.val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "        num_correct = 0\n",
        "        for i, (x, y) in enumerate(self.val_loader):\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = val_model(x)\n",
        "\n",
        "            num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "            batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * self.config['batch_size'])))\n",
        "\n",
        "            batch_bar.update()\n",
        "            \n",
        "        batch_bar.close()\n",
        "        print(\"\\nValidation: {:.04f}%\".format(100 * num_correct / len(self.val_dataset)))\n",
        "\n",
        "    def save_checkpoint(self, epoch, model, optimizer, loss):\n",
        "        print(\"Saving Checkpoint!\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(epoch) + '.pth'))\n",
        "\n",
        "    def save_model(self, onnx=False):\n",
        "        # if save_best:\n",
        "        #     torch.save(self.model.state_dict(), os.path.join(self.model_path, \"best_model.pth\"))\n",
        "        # else:\n",
        "\n",
        "        name = os.path.join(self.model_path, \"model.pth\")\n",
        "        torch.save(self.model.state_dict(), name)\n",
        "        if onnx:\n",
        "            torch.onnx.export(self.model, name.split('.')[0] + '.onnx')\n",
        "            wandb.save(name.split('.')[0] + '.onnx')\n",
        "\n",
        "        print(\"Model saved at : \", self.model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "LydVUNlzgPfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The well-accepted SGD batch_size & lr combination for CNN classification is 256 batch size for 0.1 learning rate.\n",
        "When changing batch size for SGD, follow the linear scaling rule - halving batch size -> halve learning rate, etc.\n",
        "This is less theoretically supported for Adam, but in my experience, it's a decent ballpark estimate.\n",
        "Just for the early submission. We'd want you to train like 50 epochs for your main submissions.\n",
        "\"\"\"\n",
        "config = {\n",
        "    '': 'Test',\n",
        "    'batch_size': 1024,\n",
        "    'transforms': None,\n",
        "    'epochs': 2,\n",
        "    'backbone': 'simple',\n",
        "    'dropout': None,\n",
        "    'optimizer': 'SGD',\n",
        "    'optim':{'lr': 0.1, 'momentum':0.9, 'weight_decay':1e-4},\n",
        "    'arch': {   #Order (In, Out, Kernel, Stride, Padding)\n",
        "        1: {\"conv\": [3, 64, (7, 7), 4, 2], \"pool\": {'max': True, \"output\": (56, 56)}},\n",
        "        2: {\"conv\": [64, 128, (3, 3), 2, 1], \"pool\": {'max': True, \"output\": (28, 28)}},\n",
        "        3: {\"conv\": [128, 256, (3, 3), 2, 1], \"pool\": {'max': True, \"output\": (14, 14)}},\n",
        "        4: {\"conv\": [256, 512, (3, 3), 2, 1], \"pool\": {'max': False, \"output\": (1, 1)}}\n",
        "    },\n",
        "    'scheduler': 'CosineAnnealingLR',\n",
        "    'subset': True,\n",
        "    'save': True,\n",
        "    'log': False,\n",
        "    'randomize': True,\n",
        "}\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# FaceNet\n",
        "face = FaceNetSetup(config, save_path = r'/content/cmudrive/IDL/hw2-early')\n",
        "face.setup()\n",
        "\n",
        "# Model Training\n",
        "facenet_model = face.train()\n",
        "\n",
        "# Save Trained Model\n",
        "face.save_model()\n",
        "\n",
        "# Validation\n",
        "face.validate(facenet_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "J0Ar0syQljp1",
        "outputId": "745bc532-1644-40b6-f215-60bca987b204"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-195-7e5a3b1da583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# FaceNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceNetSetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'/content/cmudrive/IDL/hw2-early'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Model Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-f0b67d089479>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-f0b67d089479>\u001b[0m in \u001b[0;36m__dataloaders\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     ToTensorV2(),])\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_transforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mToTensorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_transforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mToTensorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ToTensorV2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "## Classification Task: Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "08Zv2AWFrfVP"
      },
      "outputs": [],
      "source": [
        "class ClassificationTestSet(Dataset):\n",
        "    # It's possible to load test set data using ImageFolder without making a custom class.\n",
        "    # See if you can think it through!\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationSubmission():\n",
        "    def __init__(self, data_path, csv_path):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.drive_dir = r'/content/cmudrive/IDL'\n",
        "        self.DATA_DIR = r\"/content\" \n",
        "        self.TEST_DIR = osp.join(self.DATA_DIR, r\"classification/classification/test\")\n",
        "\n",
        "    def __get_labels(self, imodel, iargs):\n",
        "        imodel.eval()\n",
        "        labels = []\n",
        "        print(f\"Context = {iargs['context']} | Batch Size = {iargs['batch_size']} | Arch = {iargs['arch']}\")\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(self.test_samples)):\n",
        "                X = self.test_samples[i]\n",
        "                test_items = SubmissionItems(X, context=iargs['context'])\n",
        "                test_loader = torch.utils.data.DataLoader(test_items, batch_size=iargs['batch_size'], num_workers=2, pin_memory=True, shuffle=False)\n",
        "\n",
        "                for data in tqdm(test_loader):\n",
        "                    data = data.float().to(self.device)              \n",
        "                    output = imodel(data)\n",
        "                    y = torch.argmax(output, axis=1)\n",
        "                    labels.extend(y.tolist())\n",
        "        return labels\n",
        "\n",
        "    def __load_model(self, model_name, model_type): \n",
        "        meta_path = os.path.join(self.drive_dir,  model_type, model_name, 'model_parameters.yaml')\n",
        "        with open(meta_path, 'r') as meta:\n",
        "            args = yaml.safe_load(meta)\n",
        "\n",
        "        model_path = os.path.join(self.drive_dir, model_type, model_name, 'model.pth')\n",
        "        model = Network(args[\"arch\"], args['context'], args['drop']).to(self.device)\n",
        "        # summary(model)\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        return model, args\n",
        "\n",
        "    def simple_inference(self, model_name, model_type):\n",
        "        print(\"Running inference...\")\n",
        "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        model, args = self.__load_model(model_name, model_type)\n",
        "        labels = self.__get_labels(model, args)\n",
        "        \n",
        "        return labels\n",
        "\n",
        "    def ensemble_inference(self, model_names, model_type):\n",
        "        print(\"Running ensembled inference...\")\n",
        "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "        prelim_labels = []\n",
        "        for name in model_names:\n",
        "            print(\"\\n\\n\\tModel : \", name)\n",
        "            model, args = self.__load_model(name, model_type)\n",
        "            prelim_labels.append(self.__get_labels(model, args))\n",
        "\n",
        "        accs = [86.146, 85.79, 84.95]\n",
        "        w = accs / np.sum(accs)\n",
        "\n",
        "        print(\"Combining predictions...\")\n",
        "        labels_df = pd.DataFrame(prelim_labels)\n",
        "        labels_df = labels_df.transpose()\n",
        "        ensembled_labels = labels_df.mode(axis=1, dropna=False).iloc[:, 0].tolist()\n",
        "        # ensembled_labels = np.where((df.iloc[:,1] == df.iloc[:, 2]), df.iloc[:, 1], df.iloc[:, 0]).tolist()\n",
        "\n",
        "        return labels_df, ensembled_labels\n",
        "\n",
        "    def generate_submission(self, save_path, labels): \n",
        "        sub_dir = os.path.join(self.drive_dir, save_path + self.timestamp)\n",
        "        sub_path = os.path.join(sub_dir, 'submission.csv')\n",
        "\n",
        "        with open(r\"/content/classification_early_submission.csv\", \"w+\") as f:\n",
        "            f.write(\"id,label\\n\")\n",
        "            for i in tqdm(range(len(test_dataset))):\n",
        "                f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))\n",
        "\n",
        "        print(f\"File saved at : {sub_path}\")\n",
        "        return sub_path"
      ],
      "metadata": {
        "id": "BmT-HNdOfyOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "td_qvGwr16z0"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = r\"/content\"\n",
        "TEST_DIR = osp.join(DATA_DIR, r\"classification/classification/test\")\n",
        "val_transforms = [ttf.ToTensor()]\n",
        "\n",
        "test_dataset = ClassificationTestSet(TEST_DIR, ttf.Compose(val_transforms))\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False,\n",
        "                         drop_last=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "U2WQEUjXkWvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93c78f1-4ae9-4683-8bd7-c90f9a890381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ],
      "source": [
        "face.model.eval()\n",
        "batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "\n",
        "res = []\n",
        "for i, (x) in enumerate(test_loader):\n",
        "    with torch.no_grad():\n",
        "        x = x.cuda()\n",
        "  \n",
        "        outputs = face.model(x)\n",
        "\n",
        "        y = torch.argmax(outputs, axis=1)\n",
        "        res.extend(y.tolist())\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "batch_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "Vob9a2-HkW_V"
      },
      "outputs": [],
      "source": [
        "with open(r\"/content/classification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in tqdm(range(len(test_dataset))):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "zpxatBfT4jSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe597e47-9f3d-4da7-d9eb-42d323c75924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 541k/541k [00:00<00:00, 2.65MB/s]\n",
            "Successfully submitted to Face Recognition"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-classification -f /content/classification_early_submission.csv -m \"Early2\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verification"
      ],
      "metadata": {
        "id": "K9svme0hfXji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsJx1l1T4twC"
      },
      "source": [
        "## Verification Task: Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 6K verification dev images, but 166K \"pairs\" for you to compare. So, it's much more efficient to compute the features for the 6K verification images, and just compare afterwards.\n",
        "\n",
        "This will be done by creating a dictionary mapping the image file names to the features. Then, you'll use this dictionary to compute the similarities for each pair."
      ],
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/verification/verification/dev | wc -l\n",
        "!cat /content/verification/verification/verification_dev.csv | wc -l"
      ],
      "metadata": {
        "id": "ZV-WsTi9LrVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d179e8f7-4f39-45a7-fb52-81121c977316"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000\n",
            "166801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # We return the image, as well as the path to that image (relative path)\n",
        "        return self.transforms(Image.open(self.img_paths[idx])), osp.relpath(self.img_paths[idx], self.data_dir)"
      ],
      "metadata": {
        "id": "m1YtIwxuL7H0"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "98lmjm0S4tHR"
      },
      "outputs": [],
      "source": [
        "val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"),\n",
        "                                       ttf.Compose(val_transforms))\n",
        "val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=config['batch_size'], \n",
        "                                             shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "face.model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(val_ver_loader), total=len(val_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try the final outputs too!\n",
        "        features = face.model(imgs, return_feats=True) \n",
        "        for i, feature in enumerate(features):\n",
        "            feats_dict[path_names[i]] = feature\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow.\n",
        "\n",
        "# print(list(feats_dict.items())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qw45H-eMyyn",
        "outputId": "08da605d-6570-4832-f3bc-80080a66de43"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "cosine_sim = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_dev.csv\")\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "gt_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2, gt = line.split(\",\")\n",
        "    img_path1 = img_path1.split('/')[-1]\n",
        "    img_path2 = img_path2.split('/')[-1]\n",
        "    feat1 = feats_dict[img_path1]\n",
        "    feat2 = feats_dict[img_path2]\n",
        "\n",
        "    # TODO: Use the similarity metric\n",
        "    sim_score = cosine_sim(feat1, feat2)\n",
        "    pred_similarities.append(sim_score.item())\n",
        "    gt_similarities.append(int(gt))\n",
        "\n",
        "pred_similarities = np.array(pred_similarities)\n",
        "gt_similarities = np.array(gt_similarities)\n",
        "\n",
        "print(\"AUC:\", roc_auc_score(gt_similarities, pred_similarities))"
      ],
      "metadata": {
        "id": "_zuqds2qNO6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c74ef24-7878-4acd-ba6d-b0fa70df462c"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9558763519432023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verification Task: Submit to Kaggle"
      ],
      "metadata": {
        "id": "sakRa8oZOlKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/test\"),\n",
        "                                        ttf.Compose(val_transforms))\n",
        "test_ver_loader = torch.utils.data.DataLoader(test_veri_dataset, batch_size=config['batch_size'], \n",
        "                                              shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "oDK3knDcOrOE"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face.model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(test_ver_loader), total=len(test_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try to final outputs too!\n",
        "        feats = face.model(imgs, return_feats=True) \n",
        "        for i, feat in enumerate(feats):\n",
        "            feats_dict[path_names[i]] = feat\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow.\n",
        "\n",
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "cosine_sim = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_test.csv\")\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2 = line.split(\",\")\n",
        "    img_path1 = img_path1.split('/')[-1]\n",
        "    img_path2 = img_path2.split('/')[-1]\n",
        "    feat1 = feats_dict[img_path1]\n",
        "    feat2 = feats_dict[img_path2]\n",
        "    sim_score = cosine_sim(feat1, feat2)\n",
        "    pred_similarities.append(sim_score.item())\n",
        "\n",
        "    # TODO: Finish up verification testing.\n",
        "    # How to use these img_paths? What to do with the features?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igeRT3WxOrB_",
        "outputId": "f9d8b42f-cf3b-42d1-ed53-378a5d4fd9f4"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r\"/content/verification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,match\\n\")\n",
        "    for i in range(len(pred_similarities)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_similarities[i]))"
      ],
      "metadata": {
        "id": "fYXiglWkPBDv"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "P5zB7P8O687N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41072302-cd49-45a9-bbf0-a37c825acaf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 16.6M/16.6M [00:00<00:00, 38.9MB/s]\n",
            "Successfully submitted to Face Verification"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-verification -f /content/verification_early_submission.csv -m 'Early'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "e9WhS16VCva6",
        "zttLq9nw1aMP",
        "mIqmojPaWD0H",
        "gtxmWqOch4-B",
        "UpgCHImRkYQW",
        "K9svme0hfXji",
        "PsJx1l1T4twC"
      ],
      "name": "FaceNet Working",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nefario7/cmu-deeplearning/blob/working-hw2/Homework%202/facenet_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive and Download Data"
      ],
      "metadata": {
        "id": "e9WhS16VCva6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output \n",
        "! apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "! add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "! apt-get update -qq 2>&1 > /dev/null\n",
        "! apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "\n",
        "! google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "! echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "% cd /content\n",
        "! mkdir cmudrive\n",
        "% cd ..\n",
        "! google-drive-ocamlfuse /content/cmudrive\n",
        "! pip install kaggle wandb torch-summary\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/cmudrive/IDL/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle \n",
        "! kaggle config set -n path -v /content\n",
        "\n",
        "! wandb login\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "KSB0Pzy3Cu2o"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "! kaggle competitions download -c 11-785-s22-hw2p2-verification\n",
        "\n",
        "! unzip -q /content/competitions/11-785-s22-hw2p2-classification/11-785-s22-hw2p2-classification.zip -d /content\n",
        "! unzip -q /content/competitions/11-785-s22-hw2p2-verification/11-785-s22-hw2p2-verification.zip -d /content\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "ipZZI38GC-Sj"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "zttLq9nw1aMP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "jwLEd0gdPbSc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "import torchvision.models as models\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
        "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "# torch.autograd.set_detect_anomaly(False)\n",
        "# torch.autograd.profiler.profile(False)\n",
        "# torch.autograd.profiler.emit_nvtx(False)"
      ],
      "metadata": {
        "id": "uDoTcOeI1bB9"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "# FaceNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "Ny-mh_ocWIJR"
      },
      "outputs": [],
      "source": [
        "class FaceNet(nn.Module):\n",
        "    \"\"\"\n",
        "    The Very Low early deadline architecture is a 4-layer CNN.\n",
        "    The first Conv layer has 64 channels, kernel size 7, and stride 4.\n",
        "    The next three have 128, 256, and 512 channels. Each have kernel size 3 and stride 2.\n",
        "    Think about what the padding should be for each layer to not change spatial resolution.\n",
        "    Each Conv layer is accompanied by a Batchnorm and ReLU layer.\n",
        "    Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1.\n",
        "    Then, remove (Flatten?) these trivial 1x1 dimensions away.\n",
        "    Look through https://pytorch.org/docs/stable/nn.html \n",
        "    TODO: Fill out the model definition below! \n",
        "\n",
        "    Why does a very simple network have 4 convolutions?\n",
        "    Input images are 224x224. Note that each of these convolutions downsample.\n",
        "    Downsampling 2x effectively doubles the receptive field, increasing the spatial\n",
        "    region each pixel extracts features from. Downsampling 32x is standard\n",
        "    for most image models.\n",
        "\n",
        "    Why does a very simple network have high channel sizes?\n",
        "    Every time you downsample 2x, you do 4x less computation (at same channel size).\n",
        "    To maintain the same level of computation, you 2x increase # of channels, which \n",
        "    increases computation by 4x. So, balances out to same computation.\n",
        "    Another intuition is - as you downsample, you lose spatial information. Want\n",
        "    to preserve some of it in the channel dimension.\n",
        "    \"\"\"\n",
        "    def list_to_kwarg(self, inc, outc, kernel, s, p):\n",
        "        params = dict()\n",
        "        params[\"in_channels\"] = inc\n",
        "        params[\"out_channels\"] = outc\n",
        "        params[\"kernel_size\"] = kernel\n",
        "        params[\"stride\"] = s\n",
        "        params[\"padding\"] = p\n",
        "        return params\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Note that first conv is stride 4. It is (was?) standard to downsample.\n",
        "        # 4x early on, as with 224x224 images, 4x4 patches are just low-level details.\n",
        "\n",
        "        # Food for thoughts: \n",
        "        # ? Why is the first conv kernel size 7, not kernel size 3?\n",
        "        # ? Use activation before or after Pooling?\n",
        "        num_classes = 7000\n",
        "        num_channels = 3\n",
        "        layers = []\n",
        "        if config['backbone'] == 'simple':\n",
        "            for l_idx, l_params in config['arch'].items():\n",
        "                conv_params = self.list_to_kwarg(*l_params[\"conv\"])\n",
        "                layers.append(nn.Conv2d(**conv_params))\n",
        "                layers.append(nn.BatchNorm2d(conv_params[\"out_channels\"]))\n",
        "                layers.append(nn.ReLU())\n",
        "                if l_params[\"pool\"] is not None:\n",
        "                    if l_params[\"pool\"][\"max\"]:\n",
        "                        layers.append(nn.AdaptiveMaxPool2d(l_params[\"pool\"][\"output\"]))\n",
        "                    else:\n",
        "                        layers.append(nn.AdaptiveAvgPool2d(l_params[\"pool\"][\"output\"]))\n",
        "            layers.append(nn.Flatten())\n",
        "            self.backbone = nn.Sequential(*layers)\n",
        "            self.cls_layer = nn.Linear(512, num_classes)\n",
        "        else:\n",
        "            if config['backbone'] == 'resnet_18':\n",
        "                resnet_net = models.resnet18(pretrained=True)\n",
        "                oc = 512\n",
        "            elif config['backbone'] == 'resnet_34':\n",
        "                resnet_net = models.resnet34(pretrained=True)\n",
        "                oc = 512\n",
        "\n",
        "            layers = list(resnet_net.children())[:-2]\n",
        "            layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "            layers.append(nn.Flatten())\n",
        "\n",
        "            self.backbone = nn.Sequential(*layers)\n",
        "            self.backbone.out_channels = oc\n",
        "            self.cls_layer = nn.Linear(self.backbone.out_channels, num_classes)\n",
        "    \n",
        "    def forward(self, x, return_feats=False):\n",
        "        \"\"\"\n",
        "        What is return_feats? It essentially returns the second-to-last-layer\n",
        "        features of a given image. It's a \"feature encoding\" of the input image,\n",
        "        and you can use it for the verification task. You would use the outputs\n",
        "        of the final classification layer for the classification task.\n",
        "\n",
        "        You might also find that the classification outputs are sometimes better\n",
        "        for verification too - try both.\n",
        "        \"\"\"\n",
        "        feats = self.backbone(x)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "# self.backbone = nn.Sequential(\n",
        "\n",
        "\n",
        "#     nn.Conv2d(num_channels, 64, (7, 7), stride=4, padding=2),\n",
        "#     nn.BatchNorm2d(64),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool2d(kernel_size=(3,3), stride=1, padding=1),\n",
        "\n",
        "#     nn.Conv2d(64, 128, (3, 3), stride=2, padding=1),\n",
        "#     nn.BatchNorm2d(128),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool2d(kernel_size=(3,3), stride=1,padding=1),\n",
        "\n",
        "#     nn.Conv2d(128, 256, (3, 3), stride=2, padding=1),\n",
        "#     nn.BatchNorm2d(256),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool2d(kernel_size=(3,3), stride=1,padding=1),\n",
        "\n",
        "#     nn.Conv2d(256, 512, (3, 3), stride=2, padding=1),\n",
        "#     nn.BatchNorm2d(512),\n",
        "#     nn.ReLU(),\n",
        "#     nn.AdaptiveAvgPool2d((1,1)),\n",
        "\n",
        "#     nn.Flatten(),\n",
        "#     ) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FaceNet Training"
      ],
      "metadata": {
        "id": "gtxmWqOch4-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "UowI9OcUYPjP"
      },
      "outputs": [],
      "source": [
        "class FaceNetSetup:\n",
        "    def __init__(self, config, save_path):\n",
        "        self.config = config\n",
        "        self.log = config['log']\n",
        "\n",
        "        if config['subset']:\n",
        "            train_path = r\"train_subset/train_subset\"\n",
        "        else:\n",
        "            train_path = r\"classification/classification/train\"\n",
        "        self.SAVE_DIR = save_path\n",
        "        self.DATA_DIR = r\"/content\" \n",
        "        self.TRAIN_DIR = osp.join(self.DATA_DIR, train_path) \n",
        "        self.VAL_DIR = osp.join(self.DATA_DIR, r\"classification/classification/dev\")\n",
        "\n",
        "    def __check_model_params(self):\n",
        "        num_trainable_parameters = 0\n",
        "        for p in self.model.parameters():\n",
        "            num_trainable_parameters += p.numel()\n",
        "        print(\"Number of Params: {}\".format(num_trainable_parameters))\n",
        "        assert num_trainable_parameters <= 35000000\n",
        "\n",
        "    def __gen_model_name(self):\n",
        "        save_name = ''\n",
        "        if not self.config['subset']:\n",
        "            save_name += \"Full_\"\n",
        "        for key, val in self.config.items():\n",
        "            abbr = key[0] if len(key) > 2 else key\n",
        "            if type(key) == 'dict':\n",
        "                data = 'lr' + str(val[\"lr\"])\n",
        "                save_name += data\n",
        "                break\n",
        "            else:\n",
        "                data = abbr + str(val) + '_'\n",
        "                save_name += data\n",
        "        if self.config['randomize']:\n",
        "            save_name = save_name + \"-v\" + str(np.random.randint(10, 1000))\n",
        "        print(\"\\nModel will be saved as : \", save_name)\n",
        "\n",
        "        return save_name\n",
        "\n",
        "    def __save_model_params(self, save_name):\n",
        "        # Create Model Directory\n",
        "        save_path = os.path.join(self.SAVE_DIR, save_name)\n",
        "        try:\n",
        "            os.mkdir(save_path)\n",
        "        except FileExistsError:\n",
        "                d = input(\"Model name already exists. Delete existing model? (y/n)\")\n",
        "                if d == 'y':\n",
        "                    import shutil\n",
        "                    shutil.rmtree(save_path)\n",
        "                    os.mkdir(save_path)\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "        # Saving Model Configuration\n",
        "        with open(os.path.join(save_path, 'model_config.yaml'), 'w') as metadata:\n",
        "            yaml.dump({'Experiment': self.config['']}, metadata, indent=8, default_flow_style=False)\n",
        "            yaml.dump(self.config, metadata, indent=4, default_flow_style=False)\n",
        "        print(\"\\nModel will be saved at : \", save_path)\n",
        "        return save_path\n",
        "\n",
        "    def __dataloaders(self): \n",
        "        \"\"\"\n",
        "        Transforms (data augmentation) is quite important for this task.\n",
        "        Go explore https://pytorch.org/vision/stable/transforms.html for more details\n",
        "        \"\"\"\n",
        "        if self.config[\"transforms\"] is not None:\n",
        "            pass\n",
        "        else:\n",
        "            self.train_transforms = [ttf.ToTensor()]\n",
        "            self.val_transforms = [ttf.ToTensor()]\n",
        "\n",
        "        self.train_dataset = torchvision.datasets.ImageFolder(self.TRAIN_DIR, transform=ttf.Compose(self.train_transforms))\n",
        "        self.val_dataset = torchvision.datasets.ImageFolder(self.VAL_DIR, transform=ttf.Compose(self.val_transforms))\n",
        "\n",
        "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.config['batch_size'], shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
        "        self.val_loader = DataLoader(self.val_dataset, batch_size=self.config['batch_size'], shuffle=False, drop_last=True, num_workers=1)\n",
        "\n",
        "    def setup(self):\n",
        "        self.__dataloaders()\n",
        "    \n",
        "        # Model\n",
        "        self.model = FaceNet(self.config)\n",
        "        self.model.cuda()\n",
        "        # summary(self.model, (3, 224, 224))\n",
        "        self.__check_model_params()\n",
        "        self.model_name = self.__gen_model_name()\n",
        "        self.model_path = self.__save_model_params()\n",
        "\n",
        "        if self.log:\n",
        "            wandb.init(project=\"hw2-preliminary\", entity=\"nefario7\", config=self.config)\n",
        "\n",
        "        # Loss\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), **self.config['optim'])\n",
        "\n",
        "        # Scheduler\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=(len(self.train_loader) * self.config['epochs']))\n",
        "        # T_max is \"how many times will i call scheduler.step() until it reaches 0 lr?\"\n",
        "\n",
        "        # For this homework, we strongly strongly recommend using FP16 to speed up training.\n",
        "        # It helps more for larger models.\n",
        "        # Go to https://effectivemachinelearning.com/PyTorch/8._Faster_training_with_mixed_precision\n",
        "        # and compare \"Single precision training\" section with \"Mixed precision training\" section\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    def train(self):\n",
        "        epochs = self.config['epochs']\n",
        "        batch_size = self.config['epochs']\n",
        "        if self.log:\n",
        "            wandb.watch(self.model, criterion=self.criterion, log=\"all\", log_freq=batch_size, idx=None,log_graph=True)\n",
        "        for epoch in range(epochs):\n",
        "            # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "            batch_bar = tqdm(total=len(self.train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "            num_correct = 0\n",
        "            total_loss = 0\n",
        "\n",
        "            for i, (x, y) in enumerate(self.train_loader):\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                x = x.cuda()\n",
        "                y = y.cuda()\n",
        "\n",
        "                # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "                with torch.cuda.amp.autocast():     \n",
        "                    outputs = self.model(x)\n",
        "                    loss = self.criterion(outputs, y)\n",
        "\n",
        "                # Update # correct & loss as we go\n",
        "                num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "                total_loss += float(loss)\n",
        "                train_loss = float(total_loss / (i + 1))\n",
        "\n",
        "                if self.log and i % 10 == 0:\n",
        "                    wandb.log({\n",
        "                        \"Training Accuracy\": 100 * num_correct / ((i + 1) * batch_size),\n",
        "                        \"Training Loss\": float(train_loss),\n",
        "                        \"Num Correct\": num_correct,\n",
        "                        \"Learning Rate\": float(self.optimizer.param_groups[0]['lr'])\n",
        "                               })\n",
        "\n",
        "                batch_bar.set_postfix(\n",
        "                    acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "                    loss=\"{:.04f}\".format(train_loss),\n",
        "                    num_correct=num_correct,\n",
        "                    lr=\"{:.04f}\".format(float(self.optimizer.param_groups[0]['lr'])))\n",
        "                \n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer) \n",
        "                self.scaler.update() # This is something added just for FP16\n",
        "\n",
        "                self.scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "                batch_bar.update()\n",
        "\n",
        "            batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "            #Epoch Validation\n",
        "            self.validate(self.model)\n",
        "\n",
        "            print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "                epoch + 1,\n",
        "                epochs,\n",
        "                100 * num_correct / (len(self.train_loader) * batch_size),\n",
        "                float(total_loss / len(self.train_loader)),\n",
        "                float(self.optimizer.param_groups[0]['lr'])))\n",
        "            \n",
        "            # Save Checkpoint after epoch\n",
        "            self.save_checkpoint(epoch, self.model, self.optimizer, total_loss / len(self.train_loader))\n",
        "            \n",
        "        if self.log:\n",
        "            wandb.finish()\n",
        "            \n",
        "        return self.model\n",
        "\n",
        "    def validate(self, val_model):\n",
        "        val_model.eval()\n",
        "        batch_bar = tqdm(total=len(self.val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "        num_correct = 0\n",
        "        for i, (x, y) in enumerate(self.val_loader):\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = val_model(x)\n",
        "\n",
        "            num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "            batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * self.config['batch_size'])))\n",
        "\n",
        "            batch_bar.update()\n",
        "            \n",
        "        batch_bar.close()\n",
        "        print(\"Validation: {:.04f}%\".format(100 * num_correct / len(self.val_dataset)))\n",
        "\n",
        "    def save_checkpoint(self, epoch, model, optimizer, loss):\n",
        "        print(\"Saving Checkpoint!\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(epoch) + '.pth'))\n",
        "\n",
        "    def save_model(self, onnx=False):\n",
        "        # if save_best:\n",
        "        #     torch.save(self.model.state_dict(), os.path.join(self.model_path, \"best_model.pth\"))\n",
        "        # else:\n",
        "\n",
        "        name = os.path.join(self.model_path, \"model.pth\")\n",
        "        torch.save(self.model.state_dict(), name)\n",
        "        if onnx:\n",
        "            torch.onnx.export(self.model, name.split('.')[0] + '.onnx')\n",
        "            wandb.save(name.split('.')[0] + '.onnx')\n",
        "\n",
        "        print(\"Model saved at : \", self.model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters and Run"
      ],
      "metadata": {
        "id": "LydVUNlzgPfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The well-accepted SGD batch_size & lr combination for CNN classification is 256 batch size for 0.1 learning rate.\n",
        "When changing batch size for SGD, follow the linear scaling rule - halving batch size -> halve learning rate, etc.\n",
        "This is less theoretically supported for Adam, but in my experience, it's a decent ballpark estimate.\n",
        "Just for the early submission. We'd want you to train like 50 epochs for your main submissions.\n",
        "\"\"\"\n",
        "config = {\n",
        "    '': 'Early',\n",
        "    'batch_size': 256,\n",
        "    'transforms': None,\n",
        "    'epochs': 25,\n",
        "    'backbone': 'resnet_34',\n",
        "    'dropout': None,\n",
        "    'optimizer': 'SGD',\n",
        "    'optim':{'lr': 0.1, 'momentum':0.9, 'weight_decay':1e-4},\n",
        "        #Order (In, Out, Kernel, Stride, Padding)\n",
        "    'arch': {   \n",
        "        1: {\"conv\": [3, 64, (7, 7), 4, 2], \"pool\": {'max': True, \"output\": (56, 56)}},\n",
        "        2: {\"conv\": [64, 128, (3, 3), 2, 1], \"pool\": {'max': True, \"output\": (28, 28)}},\n",
        "        3: {\"conv\": [128, 256, (3, 3), 2, 1], \"pool\": {'max': True, \"output\": (14, 14)}},\n",
        "        4: {\"conv\": [256, 512, (3, 3), 2, 1], \"pool\": {'max': False, \"output\": (1, 1)}}\n",
        "    },\n",
        "    'scheduler': 'CosineAnnealingLR',\n",
        "    'subset': False,\n",
        "    'save': True,\n",
        "    'log': True,\n",
        "    'randomize': False,\n",
        "}\n",
        "\n",
        "# FaceNet\n",
        "face = FaceNetSetup(config, save_path = r'/content/cmudrive/IDL/hw2-early')\n",
        "face.setup()\n",
        "\n",
        "# Model Training\n",
        "facenet_model = face.train()\n",
        "\n",
        "# Save Trained Model\n",
        "face.save_model()\n",
        "\n",
        "# Validation\n",
        "face.validate(facenet_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773,
          "referenced_widgets": [
            "7936a07ea510402596745d249a772720",
            "5e5506f438c447138d9c01f0145254c8",
            "415ba4e2a19547eca85e920b02a30d61",
            "ad771a4a55054c23895bf9687356cefc",
            "089baae3ed5043aba3c148fc7517c952",
            "41befe409ee84f56a09847b384dcde01",
            "719acc26c7424659b20430e01e819962",
            "b14dffaf239e406094195d6399c81e8e"
          ]
        },
        "id": "J0Ar0syQljp1",
        "outputId": "ce9f6f7e-05a3-4885-cb52-976a7a7d3426"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Params: 24875672\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/nefario7/hw2-preliminary/runs/j47cs0ek\" target=\"_blank\">zesty-silence-5</a></strong> to <a href=\"https://wandb.ai/nefario7/hw2-preliminary\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25: Train Acc 0.0513%, Train Loss 8.8824, Learning Rate 0.0996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/25: Train Acc 0.1245%, Train Loss 8.8545, Learning Rate 0.0984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/25: Train Acc 0.5275%, Train Loss 8.7438, Learning Rate 0.0965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/25: Train Acc 5.9780%, Train Loss 7.6241, Learning Rate 0.0938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/25: Train Acc 53.9341%, Train Loss 5.8712, Learning Rate 0.0905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/25: Train Acc 181.9121%, Train Loss 4.3969, Learning Rate 0.0864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/25: Train Acc 368.9817%, Train Loss 3.1624, Learning Rate 0.0819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/25: Train Acc 548.2051%, Train Loss 2.2062, Learning Rate 0.0768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/25: Train Acc 681.0256%, Train Loss 1.5612, Learning Rate 0.0713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/25: Train Acc 786.5421%, Train Loss 1.0814, Learning Rate 0.0655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/25: Train Acc 873.0623%, Train Loss 0.7169, Learning Rate 0.0594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/25: Train Acc 941.8681%, Train Loss 0.4345, Learning Rate 0.0531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/25: Train Acc 986.0879%, Train Loss 0.2420, Learning Rate 0.0469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/25: Train Acc 1006.3077%, Train Loss 0.1326, Learning Rate 0.0406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/25: Train Acc 1014.3883%, Train Loss 0.0757, Learning Rate 0.0345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/25: Train Acc 1019.3260%, Train Loss 0.0440, Learning Rate 0.0287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/25: Train Acc 1022.0952%, Train Loss 0.0263, Learning Rate 0.0232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/25: Train Acc 1023.4505%, Train Loss 0.0177, Learning Rate 0.0181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/25: Train Acc 1023.8608%, Train Loss 0.0146, Learning Rate 0.0136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/25: Train Acc 1023.9780%, Train Loss 0.0136, Learning Rate 0.0095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/25: Train Acc 1023.9927%, Train Loss 0.0132, Learning Rate 0.0062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/25: Train Acc 1023.9927%, Train Loss 0.0130, Learning Rate 0.0035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/25: Train Acc 1024.0000%, Train Loss 0.0129, Learning Rate 0.0016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/25: Train Acc 1024.0000%, Train Loss 0.0128, Learning Rate 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/25: Train Acc 1024.0000%, Train Loss 0.0127, Learning Rate 0.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 6929... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7936a07ea510402596745d249a772720",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.38MB of 0.38MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>████▇▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>Num Correct</td><td>▁▁▁▁▁▂▄▅▆▆▇▇█████████████</td></tr><tr><td>Training Accuracy</td><td>▁▁▁▁▁▂▄▅▆▆▇▇█████████████</td></tr><tr><td>Training Loss</td><td>███▇▆▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>0.0</td></tr><tr><td>Num Correct</td><td>139776</td></tr><tr><td>Training Accuracy</td><td>1024.0</td></tr><tr><td>Training Loss</td><td>0.01273</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">zesty-silence-5</strong>: <a href=\"https://wandb.ai/nefario7/hw2-preliminary/runs/j47cs0ek\" target=\"_blank\">https://wandb.ai/nefario7/hw2-preliminary/runs/j47cs0ek</a><br/>\n",
              "Find logs at: <code></code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Classification Task: Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "08Zv2AWFrfVP"
      },
      "outputs": [],
      "source": [
        "class ClassificationTestSet(Dataset):\n",
        "    # It's possible to load test set data using ImageFolder without making a custom class.\n",
        "    # See if you can think it through!\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationSubmission():\n",
        "    def __init__(self, data_path, csv_path):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.drive_dir = r'/content/cmudrive/IDL'\n",
        "        self.DATA_DIR = r\"/content\" \n",
        "        self.TEST_DIR = osp.join(self.DATA_DIR, r\"classification/classification/test\")\n",
        "\n",
        "    def __get_labels(self, imodel, iargs):\n",
        "        imodel.eval()\n",
        "        labels = []\n",
        "        print(f\"Context = {iargs['context']} | Batch Size = {iargs['batch_size']} | Arch = {iargs['arch']}\")\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(self.test_samples)):\n",
        "                X = self.test_samples[i]\n",
        "                test_items = SubmissionItems(X, context=iargs['context'])\n",
        "                test_loader = torch.utils.data.DataLoader(test_items, batch_size=iargs['batch_size'], num_workers=2, pin_memory=True, shuffle=False)\n",
        "\n",
        "                for data in tqdm(test_loader):\n",
        "                    data = data.float().to(self.device)              \n",
        "                    output = imodel(data)\n",
        "                    y = torch.argmax(output, axis=1)\n",
        "                    labels.extend(y.tolist())\n",
        "        return labels\n",
        "\n",
        "    def __load_model(self, model_name, model_type): \n",
        "        meta_path = os.path.join(self.drive_dir,  model_type, model_name, 'model_parameters.yaml')\n",
        "        with open(meta_path, 'r') as meta:\n",
        "            args = yaml.safe_load(meta)\n",
        "\n",
        "        model_path = os.path.join(self.drive_dir, model_type, model_name, 'model.pth')\n",
        "        model = Network(args[\"arch\"], args['context'], args['drop']).to(self.device)\n",
        "        # summary(model)\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        return model, args\n",
        "\n",
        "    def simple_inference(self, model_name, model_type):\n",
        "        print(\"Running inference...\")\n",
        "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        model, args = self.__load_model(model_name, model_type)\n",
        "        labels = self.__get_labels(model, args)\n",
        "        \n",
        "        return labels\n",
        "\n",
        "    def ensemble_inference(self, model_names, model_type):\n",
        "        print(\"Running ensembled inference...\")\n",
        "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "        prelim_labels = []\n",
        "        for name in model_names:\n",
        "            print(\"\\n\\n\\tModel : \", name)\n",
        "            model, args = self.__load_model(name, model_type)\n",
        "            prelim_labels.append(self.__get_labels(model, args))\n",
        "\n",
        "        accs = [86.146, 85.79, 84.95]\n",
        "        w = accs / np.sum(accs)\n",
        "\n",
        "        print(\"Combining predictions...\")\n",
        "        labels_df = pd.DataFrame(prelim_labels)\n",
        "        labels_df = labels_df.transpose()\n",
        "        ensembled_labels = labels_df.mode(axis=1, dropna=False).iloc[:, 0].tolist()\n",
        "        # ensembled_labels = np.where((df.iloc[:,1] == df.iloc[:, 2]), df.iloc[:, 1], df.iloc[:, 0]).tolist()\n",
        "\n",
        "        return labels_df, ensembled_labels\n",
        "\n",
        "    def generate_submission(self, save_path, labels): \n",
        "        sub_dir = os.path.join(self.drive_dir, save_path + self.timestamp)\n",
        "        sub_path = os.path.join(sub_dir, 'submission.csv')\n",
        "\n",
        "        with open(r\"/content/classification_early_submission.csv\", \"w+\") as f:\n",
        "            f.write(\"id,label\\n\")\n",
        "            for i in tqdm(range(len(test_dataset))):\n",
        "                f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))\n",
        "\n",
        "        print(f\"File saved at : {sub_path}\")\n",
        "        return sub_path"
      ],
      "metadata": {
        "id": "BmT-HNdOfyOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "td_qvGwr16z0"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = r\"/content\"\n",
        "TEST_DIR = osp.join(DATA_DIR, r\"classification/classification/test\")\n",
        "val_transforms = [ttf.ToTensor()]\n",
        "\n",
        "test_dataset = ClassificationTestSet(TEST_DIR, ttf.Compose(val_transforms))\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False,\n",
        "                         drop_last=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "U2WQEUjXkWvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93c78f1-4ae9-4683-8bd7-c90f9a890381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ],
      "source": [
        "face.model.eval()\n",
        "batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "\n",
        "res = []\n",
        "for i, (x) in enumerate(test_loader):\n",
        "    with torch.no_grad():\n",
        "        x = x.cuda()\n",
        "  \n",
        "        outputs = face.model(x)\n",
        "\n",
        "        y = torch.argmax(outputs, axis=1)\n",
        "        res.extend(y.tolist())\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "batch_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "Vob9a2-HkW_V"
      },
      "outputs": [],
      "source": [
        "with open(r\"/content/classification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in tqdm(range(len(test_dataset))):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "zpxatBfT4jSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe597e47-9f3d-4da7-d9eb-42d323c75924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 541k/541k [00:00<00:00, 2.65MB/s]\n",
            "Successfully submitted to Face Recognition"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-classification -f /content/classification_early_submission.csv -m \"Early2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsJx1l1T4twC"
      },
      "source": [
        "# Verification Task: Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 6K verification dev images, but 166K \"pairs\" for you to compare. So, it's much more efficient to compute the features for the 6K verification images, and just compare afterwards.\n",
        "\n",
        "This will be done by creating a dictionary mapping the image file names to the features. Then, you'll use this dictionary to compute the similarities for each pair."
      ],
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/verification/verification/dev | wc -l\n",
        "!cat /content/verification/verification/verification_dev.csv | wc -l"
      ],
      "metadata": {
        "id": "ZV-WsTi9LrVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d179e8f7-4f39-45a7-fb52-81121c977316"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000\n",
            "166801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # We return the image, as well as the path to that image (relative path)\n",
        "        return self.transforms(Image.open(self.img_paths[idx])), osp.relpath(self.img_paths[idx], self.data_dir)"
      ],
      "metadata": {
        "id": "m1YtIwxuL7H0"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "98lmjm0S4tHR"
      },
      "outputs": [],
      "source": [
        "val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"),\n",
        "                                       ttf.Compose(val_transforms))\n",
        "val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=config['batch_size'], \n",
        "                                             shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "face.model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(val_ver_loader), total=len(val_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try the final outputs too!\n",
        "        features = face.model(imgs, return_feats=True) \n",
        "        for i, feature in enumerate(features):\n",
        "            feats_dict[path_names[i]] = feature\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qw45H-eMyyn",
        "outputId": "08da605d-6570-4832-f3bc-80080a66de43"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What does this dict look like?\n",
        "print(list(feats_dict.items())[0])"
      ],
      "metadata": {
        "id": "k6TG6RD6NTtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "cosine_sim = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_dev.csv\")\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "gt_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2, gt = line.split(\",\")\n",
        "    img_path1 = img_path1.split('/')[-1]\n",
        "    img_path2 = img_path2.split('/')[-1]\n",
        "    feat1 = feats_dict[img_path1]\n",
        "    feat2 = feats_dict[img_path2]\n",
        "\n",
        "    # TODO: Use the similarity metric\n",
        "    sim_score = cosine_sim(feat1, feat2)\n",
        "    pred_similarities.append(sim_score.item())\n",
        "    gt_similarities.append(int(gt))\n",
        "\n",
        "pred_similarities = np.array(pred_similarities)\n",
        "gt_similarities = np.array(gt_similarities)\n",
        "\n",
        "print(\"AUC:\", roc_auc_score(gt_similarities, pred_similarities))"
      ],
      "metadata": {
        "id": "_zuqds2qNO6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c74ef24-7878-4acd-ba6d-b0fa70df462c"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9558763519432023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verification Task: Submit to Kaggle"
      ],
      "metadata": {
        "id": "sakRa8oZOlKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/test\"),\n",
        "                                        ttf.Compose(val_transforms))\n",
        "test_ver_loader = torch.utils.data.DataLoader(test_veri_dataset, batch_size=config['batch_size'], \n",
        "                                              shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "oDK3knDcOrOE"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face.model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(test_ver_loader), total=len(test_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try to final outputs too!\n",
        "        feats = face.model(imgs, return_feats=True) \n",
        "        for i, feat in enumerate(feats):\n",
        "            feats_dict[path_names[i]] = feat\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igeRT3WxOrB_",
        "outputId": "f9d8b42f-cf3b-42d1-ed53-378a5d4fd9f4"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "cosine_sim = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_test.csv\")\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2 = line.split(\",\")\n",
        "    img_path1 = img_path1.split('/')[-1]\n",
        "    img_path2 = img_path2.split('/')[-1]\n",
        "    feat1 = feats_dict[img_path1]\n",
        "    feat2 = feats_dict[img_path2]\n",
        "    sim_score = cosine_sim(feat1, feat2)\n",
        "    pred_similarities.append(sim_score.item())\n",
        "\n",
        "    # TODO: Finish up verification testing.\n",
        "    # How to use these img_paths? What to do with the features?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4OZL_FNOq1r",
        "outputId": "b1dc3848-8dc2-4ee1-ee19-8a8af1c7d663"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r\"/content/verification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,match\\n\")\n",
        "    for i in range(len(pred_similarities)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_similarities[i]))"
      ],
      "metadata": {
        "id": "fYXiglWkPBDv"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "P5zB7P8O687N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41072302-cd49-45a9-bbf0-a37c825acaf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 16.6M/16.6M [00:00<00:00, 38.9MB/s]\n",
            "Successfully submitted to Face Verification"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-verification -f /content/verification_early_submission.csv -m 'Early'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALiq9PTl7KwY"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuAsK_tKhzH9",
        "outputId": "3153045c-3f9c-4015-c1b0-29f3b0323d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 22 00:51:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    33W /  70W |   3324MiB / 15109MiB |    100%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# If you keep re-initializing your model in Colab, can run out of GPU memory, need to restart.\n",
        "# These three lines can help that - run this before you re-initialize your model\n",
        "\n",
        "# del face.model\n",
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4yvdL6G_3CJV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "e9WhS16VCva6",
        "zttLq9nw1aMP",
        "jJ9b7vV9K5j2",
        "mIqmojPaWD0H",
        "gtxmWqOch4-B",
        "sakRa8oZOlKr",
        "ALiq9PTl7KwY"
      ],
      "name": "HW2P2_Starter.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7936a07ea510402596745d249a772720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5e5506f438c447138d9c01f0145254c8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_415ba4e2a19547eca85e920b02a30d61",
              "IPY_MODEL_ad771a4a55054c23895bf9687356cefc"
            ]
          }
        },
        "5e5506f438c447138d9c01f0145254c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "415ba4e2a19547eca85e920b02a30d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_089baae3ed5043aba3c148fc7517c952",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.08MB of 1.08MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41befe409ee84f56a09847b384dcde01"
          }
        },
        "ad771a4a55054c23895bf9687356cefc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_719acc26c7424659b20430e01e819962",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b14dffaf239e406094195d6399c81e8e"
          }
        },
        "089baae3ed5043aba3c148fc7517c952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41befe409ee84f56a09847b384dcde01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "719acc26c7424659b20430e01e819962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b14dffaf239e406094195d6399c81e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
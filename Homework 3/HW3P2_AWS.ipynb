{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_fwJWcpqJDR"
   },
   "source": [
    "# Prelimilaries and Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TbAnqEppm6MK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Hit:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic-updates InRelease  \n",
      "Hit:3 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Hit:4 http://security.ubuntu.com/ubuntu bionic-security InRelease              \n",
      "Get:5 https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  InRelease [1484 B]\n",
      "Get:6 https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/amd64  InRelease [1481 B]\n",
      "Get:7 https://nvidia.github.io/nvidia-docker/ubuntu18.04/amd64  InRelease [1474 B]\n",
      "Hit:8 https://apt.repos.neuron.amazonaws.com bionic InRelease                  \n",
      "Get:9 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic InRelease [15.4 kB]\n",
      "Hit:10 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease         \n",
      "Get:11 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic/main amd64 Packages [1368 B]\n",
      "Get:12 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic/main Translation-en [1004 B]\n",
      "Fetched 22.2 kB in 1s (19.8 kB/s)                   \n",
      "Reading package lists... Done\n",
      "Hit:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Hit:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic-updates InRelease  \u001b[0m\n",
      "Hit:3 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Get:4 https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  InRelease [1484 B]\n",
      "Get:5 https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/amd64  InRelease [1481 B]\n",
      "Get:6 https://nvidia.github.io/nvidia-docker/ubuntu18.04/amd64  InRelease [1474 B]m\u001b[33m\n",
      "Hit:7 https://apt.repos.neuron.amazonaws.com bionic InRelease                  \u001b[0m\u001b[33m\n",
      "Hit:8 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic InReleasem  \u001b[33m\n",
      "Hit:9 http://security.ubuntu.com/ubuntu bionic-security InRelease        \n",
      "Hit:10 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease         \u001b[0m\u001b[33m\u001b[33m\n",
      "Fetched 4439 B in 1s (5228 B/s)[0m                \u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "8 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  libaio1 librados2 librbd1\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "The following NEW packages will be installed:\n",
      "  google-drive-ocamlfuse\n",
      "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
      "Need to get 1330 kB of archives.\n",
      "After this operation, 7023 kB of additional disk space will be used.\n",
      "Get:1 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic/main amd64 google-drive-ocamlfuse amd64 0.7.27-0ubuntu1~ubuntu18.04.1 [1330 kB]\n",
      "Fetched 1330 kB in 1s (1549 kB/s)                \u001b[0m33m\u001b[33m\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package google-drive-ocamlfuse.\n",
      "(Reading database ... 142945 files and directories currently installed.)\n",
      "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J/bin/sh: 1: xdg-open: not found\n",
      "/bin/sh: 1: firefox: not found\n",
      "/bin/sh: 1: google-chrome: not found\n",
      "/bin/sh: 1: chromium-browser: not found\n",
      "Couldn't get a file descriptor referring to the console\n",
      "Cannot retrieve auth tokens.\n",
      "Failure(\"Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=KkMZG%2Fwd%2Fi-RnYatJjCoPilw6ZttCEbp0VBKSb2eSwo\")\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output \n",
    "!sudo add-apt-repository -y ppa:alessandro-strada/ppa\n",
    "!sudo apt update && sudo apt install google-drive-ocamlfuse\n",
    "!google-drive-ocamlfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LfcjHRWK0ncO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  libaio1 librados2 librbd1\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "The following additional packages will be installed:\n",
      "  libgc1c2\n",
      "Suggested packages:\n",
      "  cmigemo dict dict-wn dictd libsixel-bin mpv w3m-el w3m-img xdg-utils xsel\n",
      "The following NEW packages will be installed:\n",
      "  libgc1c2 w3m\n",
      "0 upgraded, 2 newly installed, 0 to remove and 8 not upgraded.\n",
      "Need to get 1006 kB of archives.\n",
      "After this operation, 2939 kB of additional disk space will be used.\n",
      "Get:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libgc1c2 amd64 1:7.4.2-8ubuntu1 [81.8 kB]\n",
      "Get:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 w3m amd64 0.5.3-36build1 [924 kB]\n",
      "Fetched 1006 kB in 0s (44.3 MB/s)\n",
      "Selecting previously unselected package libgc1c2:amd64.\n",
      "(Reading database ... 142950 files and directories currently installed.)\n",
      "Preparing to unpack .../libgc1c2_1%3a7.4.2-8ubuntu1_amd64.deb ...\n",
      "Unpacking libgc1c2:amd64 (1:7.4.2-8ubuntu1) ...\n",
      "Selecting previously unselected package w3m.\n",
      "Preparing to unpack .../w3m_0.5.3-36build1_amd64.deb ...\n",
      "Unpacking w3m (0.5.3-36build1) ...\n",
      "Setting up libgc1c2:amd64 (1:7.4.2-8ubuntu1) ...\n",
      "Setting up w3m (0.5.3-36build1) ...\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  libaio1 librados2 librbd1\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "The following additional packages will be installed:\n",
      "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
      "  libfile-basedir-perl libfile-desktopentry-perl libfile-listing-perl\n",
      "  libfile-mimeinfo-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
      "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
      "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
      "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
      "  libio-socket-ssl-perl libipc-system-simple-perl liblwp-mediatypes-perl\n",
      "  liblwp-protocol-https-perl libmailtools-perl libnet-dbus-perl\n",
      "  libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl libtie-ixhash-perl\n",
      "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
      "  libwww-robotrules-perl libx11-protocol-perl libxml-parser-perl\n",
      "  libxml-twig-perl libxml-xpathengine-perl perl-openssl-defaults\n",
      "  x11-xserver-utils\n",
      "Suggested packages:\n",
      "  libdigest-hmac-perl libgssapi-perl libcrypt-ssleay-perl libauthen-ntlm-perl\n",
      "  libunicode-map8-perl libunicode-string-perl xml-twig-tools nickle cairo-5c\n",
      "  xorg-docs-core\n",
      "The following NEW packages will be installed:\n",
      "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
      "  libfile-basedir-perl libfile-desktopentry-perl libfile-listing-perl\n",
      "  libfile-mimeinfo-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
      "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
      "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
      "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
      "  libio-socket-ssl-perl libipc-system-simple-perl liblwp-mediatypes-perl\n",
      "  liblwp-protocol-https-perl libmailtools-perl libnet-dbus-perl\n",
      "  libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl libtie-ixhash-perl\n",
      "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
      "  libwww-robotrules-perl libx11-protocol-perl libxml-parser-perl\n",
      "  libxml-twig-perl libxml-xpathengine-perl perl-openssl-defaults\n",
      "  x11-xserver-utils xdg-utils\n",
      "0 upgraded, 41 newly installed, 0 to remove and 8 not upgraded.\n",
      "Need to get 2542 kB of archives.\n",
      "After this operation, 8421 kB of additional disk space will be used.\n",
      "Get:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libdata-dump-perl all 1.23-1 [27.0 kB]\n",
      "Get:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
      "Get:3 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libipc-system-simple-perl all 1.25-4 [22.5 kB]\n",
      "Get:4 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libfile-basedir-perl all 0.07-1 [16.9 kB]\n",
      "Get:5 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
      "Get:6 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libfile-desktopentry-perl all 0.22-1 [18.2 kB]\n",
      "Get:7 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
      "Get:8 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
      "Get:9 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libfile-listing-perl all 6.04-1 [9774 B]\n",
      "Get:10 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libfile-mimeinfo-perl all 0.28-1 [41.4 kB]\n",
      "Get:11 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libfont-afm-perl all 1.20-2 [13.2 kB]\n",
      "Get:12 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
      "Get:13 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
      "Get:14 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
      "Get:15 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
      "Get:16 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
      "Get:17 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-form-perl all 6.03-1 [23.5 kB]\n",
      "Get:18 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tree-perl all 5.07-1 [200 kB]\n",
      "Get:19 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-format-perl all 2.12-1 [41.3 kB]\n",
      "Get:20 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-cookies-perl all 6.04-1 [17.2 kB]\n",
      "Get:21 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-daemon-perl all 6.01-1 [17.0 kB]\n",
      "Get:22 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-negotiate-perl all 6.00-2 [13.4 kB]\n",
      "Get:23 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 perl-openssl-defaults amd64 3build1 [7012 B]\n",
      "Get:24 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libnet-ssleay-perl amd64 1.84-1ubuntu0.2 [283 kB]\n",
      "Get:25 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libio-socket-ssl-perl all 2.060-3~ubuntu18.04.1 [173 kB]\n",
      "Get:26 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libnet-http-perl all 6.17-1 [22.7 kB]\n",
      "Get:27 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libtry-tiny-perl all 0.30-1 [20.5 kB]\n",
      "Get:28 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libwww-robotrules-perl all 6.01-1 [14.1 kB]\n",
      "Get:29 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libwww-perl all 6.31-1ubuntu0.1 [137 kB]\n",
      "Get:30 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-protocol-https-perl all 6.07-2 [8284 B]\n",
      "Get:31 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libnet-smtp-ssl-perl all 1.04-1 [5948 B]\n",
      "Get:32 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libmailtools-perl all 2.18-1 [74.0 kB]\n",
      "Get:33 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libxml-parser-perl amd64 2.44-2build3 [199 kB]\n",
      "Get:34 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libxml-twig-perl all 1:3.50-1 [156 kB]\n",
      "Get:35 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libnet-dbus-perl amd64 1.1.0-4build2 [176 kB]\n",
      "Get:36 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libtie-ixhash-perl all 1.23-2 [11.2 kB]\n",
      "Get:37 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libx11-protocol-perl all 0.56-7 [149 kB]\n",
      "Get:38 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libxml-xpathengine-perl all 0.14-1 [31.8 kB]\n",
      "Get:39 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 x11-xserver-utils amd64 7.7+7build1 [159 kB]\n",
      "Get:40 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic-updates/main amd64 xdg-utils all 1.1.2-1ubuntu2.5 [60.4 kB]\n",
      "Get:41 http://us-east-2.ec2.archive.ubuntu.com/ubuntu bionic/main amd64 libauthen-sasl-perl all 2.1600-1 [48.7 kB]\n",
      "Fetched 2542 kB in 0s (15.1 MB/s)               \n",
      "Extracting templates from packages: 100%\n",
      "Selecting previously unselected package libdata-dump-perl.\n",
      "(Reading database ... 143062 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libdata-dump-perl_1.23-1_all.deb ...\n",
      "Unpacking libdata-dump-perl (1.23-1) ...\n",
      "Selecting previously unselected package libencode-locale-perl.\n",
      "Preparing to unpack .../01-libencode-locale-perl_1.05-1_all.deb ...\n",
      "Unpacking libencode-locale-perl (1.05-1) ...\n",
      "Selecting previously unselected package libipc-system-simple-perl.\n",
      "Preparing to unpack .../02-libipc-system-simple-perl_1.25-4_all.deb ...\n",
      "Unpacking libipc-system-simple-perl (1.25-4) ...\n",
      "Selecting previously unselected package libfile-basedir-perl.\n",
      "Preparing to unpack .../03-libfile-basedir-perl_0.07-1_all.deb ...\n",
      "Unpacking libfile-basedir-perl (0.07-1) ...\n",
      "Selecting previously unselected package liburi-perl.\n",
      "Preparing to unpack .../04-liburi-perl_1.73-1_all.deb ...\n",
      "Unpacking liburi-perl (1.73-1) ...\n",
      "Selecting previously unselected package libfile-desktopentry-perl.\n",
      "Preparing to unpack .../05-libfile-desktopentry-perl_0.22-1_all.deb ...\n",
      "Unpacking libfile-desktopentry-perl (0.22-1) ...\n",
      "Selecting previously unselected package libtimedate-perl.\n",
      "Preparing to unpack .../06-libtimedate-perl_2.3000-2_all.deb ...\n",
      "Unpacking libtimedate-perl (2.3000-2) ...\n",
      "Selecting previously unselected package libhttp-date-perl.\n",
      "Preparing to unpack .../07-libhttp-date-perl_6.02-1_all.deb ...\n",
      "Unpacking libhttp-date-perl (6.02-1) ...\n",
      "Selecting previously unselected package libfile-listing-perl.\n",
      "Preparing to unpack .../08-libfile-listing-perl_6.04-1_all.deb ...\n",
      "Unpacking libfile-listing-perl (6.04-1) ...\n",
      "Selecting previously unselected package libfile-mimeinfo-perl.\n",
      "Preparing to unpack .../09-libfile-mimeinfo-perl_0.28-1_all.deb ...\n",
      "Unpacking libfile-mimeinfo-perl (0.28-1) ...\n",
      "Selecting previously unselected package libfont-afm-perl.\n",
      "Preparing to unpack .../10-libfont-afm-perl_1.20-2_all.deb ...\n",
      "Unpacking libfont-afm-perl (1.20-2) ...\n",
      "Selecting previously unselected package libhtml-tagset-perl.\n",
      "Preparing to unpack .../11-libhtml-tagset-perl_3.20-3_all.deb ...\n",
      "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
      "Selecting previously unselected package libhtml-parser-perl.\n",
      "Preparing to unpack .../12-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
      "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
      "Selecting previously unselected package libio-html-perl.\n",
      "Preparing to unpack .../13-libio-html-perl_1.001-1_all.deb ...\n",
      "Unpacking libio-html-perl (1.001-1) ...\n",
      "Selecting previously unselected package liblwp-mediatypes-perl.\n",
      "Preparing to unpack .../14-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
      "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
      "Selecting previously unselected package libhttp-message-perl.\n",
      "Preparing to unpack .../15-libhttp-message-perl_6.14-1_all.deb ...\n",
      "Unpacking libhttp-message-perl (6.14-1) ...\n",
      "Selecting previously unselected package libhtml-form-perl.\n",
      "Preparing to unpack .../16-libhtml-form-perl_6.03-1_all.deb ...\n",
      "Unpacking libhtml-form-perl (6.03-1) ...\n",
      "Selecting previously unselected package libhtml-tree-perl.\n",
      "Preparing to unpack .../17-libhtml-tree-perl_5.07-1_all.deb ...\n",
      "Unpacking libhtml-tree-perl (5.07-1) ...\n",
      "Selecting previously unselected package libhtml-format-perl.\n",
      "Preparing to unpack .../18-libhtml-format-perl_2.12-1_all.deb ...\n",
      "Unpacking libhtml-format-perl (2.12-1) ...\n",
      "Selecting previously unselected package libhttp-cookies-perl.\n",
      "Preparing to unpack .../19-libhttp-cookies-perl_6.04-1_all.deb ...\n",
      "Unpacking libhttp-cookies-perl (6.04-1) ...\n",
      "Selecting previously unselected package libhttp-daemon-perl.\n",
      "Preparing to unpack .../20-libhttp-daemon-perl_6.01-1_all.deb ...\n",
      "Unpacking libhttp-daemon-perl (6.01-1) ...\n",
      "Selecting previously unselected package libhttp-negotiate-perl.\n",
      "Preparing to unpack .../21-libhttp-negotiate-perl_6.00-2_all.deb ...\n",
      "Unpacking libhttp-negotiate-perl (6.00-2) ...\n",
      "Selecting previously unselected package perl-openssl-defaults:amd64.\n",
      "Preparing to unpack .../22-perl-openssl-defaults_3build1_amd64.deb ...\n",
      "Unpacking perl-openssl-defaults:amd64 (3build1) ...\n",
      "Selecting previously unselected package libnet-ssleay-perl.\n",
      "Preparing to unpack .../23-libnet-ssleay-perl_1.84-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libnet-ssleay-perl (1.84-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libio-socket-ssl-perl.\n",
      "Preparing to unpack .../24-libio-socket-ssl-perl_2.060-3~ubuntu18.04.1_all.deb ...\n",
      "Unpacking libio-socket-ssl-perl (2.060-3~ubuntu18.04.1) ...\n",
      "Selecting previously unselected package libnet-http-perl.\n",
      "Preparing to unpack .../25-libnet-http-perl_6.17-1_all.deb ...\n",
      "Unpacking libnet-http-perl (6.17-1) ...\n",
      "Selecting previously unselected package libtry-tiny-perl.\n",
      "Preparing to unpack .../26-libtry-tiny-perl_0.30-1_all.deb ...\n",
      "Unpacking libtry-tiny-perl (0.30-1) ...\n",
      "Selecting previously unselected package libwww-robotrules-perl.\n",
      "Preparing to unpack .../27-libwww-robotrules-perl_6.01-1_all.deb ...\n",
      "Unpacking libwww-robotrules-perl (6.01-1) ...\n",
      "Selecting previously unselected package libwww-perl.\n",
      "Preparing to unpack .../28-libwww-perl_6.31-1ubuntu0.1_all.deb ...\n",
      "Unpacking libwww-perl (6.31-1ubuntu0.1) ...\n",
      "Selecting previously unselected package liblwp-protocol-https-perl.\n",
      "Preparing to unpack .../29-liblwp-protocol-https-perl_6.07-2_all.deb ...\n",
      "Unpacking liblwp-protocol-https-perl (6.07-2) ...\n",
      "Selecting previously unselected package libnet-smtp-ssl-perl.\n",
      "Preparing to unpack .../30-libnet-smtp-ssl-perl_1.04-1_all.deb ...\n",
      "Unpacking libnet-smtp-ssl-perl (1.04-1) ...\n",
      "Selecting previously unselected package libmailtools-perl.\n",
      "Preparing to unpack .../31-libmailtools-perl_2.18-1_all.deb ...\n",
      "Unpacking libmailtools-perl (2.18-1) ...\n",
      "Selecting previously unselected package libxml-parser-perl.\n",
      "Preparing to unpack .../32-libxml-parser-perl_2.44-2build3_amd64.deb ...\n",
      "Unpacking libxml-parser-perl (2.44-2build3) ...\n",
      "Selecting previously unselected package libxml-twig-perl.\n",
      "Preparing to unpack .../33-libxml-twig-perl_1%3a3.50-1_all.deb ...\n",
      "Unpacking libxml-twig-perl (1:3.50-1) ...\n",
      "Selecting previously unselected package libnet-dbus-perl.\n",
      "Preparing to unpack .../34-libnet-dbus-perl_1.1.0-4build2_amd64.deb ...\n",
      "Unpacking libnet-dbus-perl (1.1.0-4build2) ...\n",
      "Selecting previously unselected package libtie-ixhash-perl.\n",
      "Preparing to unpack .../35-libtie-ixhash-perl_1.23-2_all.deb ...\n",
      "Unpacking libtie-ixhash-perl (1.23-2) ...\n",
      "Selecting previously unselected package libx11-protocol-perl.\n",
      "Preparing to unpack .../36-libx11-protocol-perl_0.56-7_all.deb ...\n",
      "Unpacking libx11-protocol-perl (0.56-7) ...\n",
      "Selecting previously unselected package libxml-xpathengine-perl.\n",
      "Preparing to unpack .../37-libxml-xpathengine-perl_0.14-1_all.deb ...\n",
      "Unpacking libxml-xpathengine-perl (0.14-1) ...\n",
      "Selecting previously unselected package x11-xserver-utils.\n",
      "Preparing to unpack .../38-x11-xserver-utils_7.7+7build1_amd64.deb ...\n",
      "Unpacking x11-xserver-utils (7.7+7build1) ...\n",
      "Selecting previously unselected package xdg-utils.\n",
      "Preparing to unpack .../39-xdg-utils_1.1.2-1ubuntu2.5_all.deb ...\n",
      "Unpacking xdg-utils (1.1.2-1ubuntu2.5) ...\n",
      "Selecting previously unselected package libauthen-sasl-perl.\n",
      "Preparing to unpack .../40-libauthen-sasl-perl_2.1600-1_all.deb ...\n",
      "Unpacking libauthen-sasl-perl (2.1600-1) ...\n",
      "Setting up libhtml-tagset-perl (3.20-3) ...\n",
      "Setting up libtry-tiny-perl (0.30-1) ...\n",
      "Setting up libfont-afm-perl (1.20-2) ...\n",
      "Setting up libencode-locale-perl (1.05-1) ...\n",
      "Setting up libtimedate-perl (2.3000-2) ...\n",
      "Setting up perl-openssl-defaults:amd64 (3build1) ...\n",
      "Setting up libipc-system-simple-perl (1.25-4) ...\n",
      "Setting up libfile-basedir-perl (0.07-1) ...\n",
      "Setting up libio-html-perl (1.001-1) ...\n",
      "Setting up libtie-ixhash-perl (1.23-2) ...\n",
      "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
      "Setting up liburi-perl (1.73-1) ...\n",
      "Setting up libdata-dump-perl (1.23-1) ...\n",
      "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
      "Setting up x11-xserver-utils (7.7+7build1) ...\n",
      "Setting up libx11-protocol-perl (0.56-7) ...\n",
      "Setting up libxml-xpathengine-perl (0.14-1) ...\n",
      "Setting up libnet-http-perl (6.17-1) ...\n",
      "Setting up xdg-utils (1.1.2-1ubuntu2.5) ...\n",
      "Setting up libwww-robotrules-perl (6.01-1) ...\n",
      "Setting up libauthen-sasl-perl (2.1600-1) ...\n",
      "Setting up libhttp-date-perl (6.02-1) ...\n",
      "Setting up libnet-ssleay-perl (1.84-1ubuntu0.2) ...\n",
      "Setting up libio-socket-ssl-perl (2.060-3~ubuntu18.04.1) ...\n",
      "Setting up libhtml-tree-perl (5.07-1) ...\n",
      "Setting up libfile-desktopentry-perl (0.22-1) ...\n",
      "Setting up libfile-listing-perl (6.04-1) ...\n",
      "Setting up libhttp-message-perl (6.14-1) ...\n",
      "Setting up libfile-mimeinfo-perl (0.28-1) ...\n",
      "Setting up libhttp-negotiate-perl (6.00-2) ...\n",
      "Setting up libnet-smtp-ssl-perl (1.04-1) ...\n",
      "Setting up libhtml-format-perl (2.12-1) ...\n",
      "Setting up libhttp-cookies-perl (6.04-1) ...\n",
      "Setting up libhttp-daemon-perl (6.01-1) ...\n",
      "Setting up libhtml-form-perl (6.03-1) ...\n",
      "Setting up libmailtools-perl (2.18-1) ...\n",
      "Setting up libwww-perl (6.31-1ubuntu0.1) ...\n",
      "Setting up liblwp-protocol-https-perl (6.07-2) ...\n",
      "Setting up libxml-parser-perl (2.44-2build3) ...\n",
      "Setting up libxml-twig-perl (1:3.50-1) ...\n",
      "Setting up libnet-dbus-perl (1.1.0-4build2) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Access token retrieved correctly.\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.12.14-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting torch-summary\n",
      "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: six>=1.10 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (4.62.3)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-6.1.1-py2.py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: urllib3 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (1.26.7)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.9-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.2.2-cp38-cp38-manylinux1_x86_64.whl (36 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 KB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from wandb) (8.0.3)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from wandb) (3.20.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->kaggle) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->kaggle) (3.1)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: kaggle, promise\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=84758965338aca31243d7a1c7e069382c43f0c78a50ec778420bfd8196b91f9f\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/29/da/11/144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=a44082bbb487ffbcbae3fce5e120b5f0512c1a38402b4bdbf29508a959383aae\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built kaggle promise\n",
      "Installing collected packages: text-unidecode, torch-summary, smmap, shortuuid, setproctitle, sentry-sdk, python-slugify, promise, docker-pycreds, kaggle, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 kaggle-1.5.12 promise-2.3 python-slugify-6.1.1 sentry-sdk-1.5.9 setproctitle-1.2.2 shortuuid-1.0.8 smmap-5.0.0 text-unidecode-1.3 torch-summary-1.4.5 wandb-0.12.14\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting kaggle\n",
      "  Using cached kaggle-1.5.12-py3-none-any.whl\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.12\n",
      "    Uninstalling kaggle-1.5.12:\n",
      "      Successfully uninstalled kaggle-1.5.12\n",
      "Successfully installed kaggle-1.5.12\n",
      "- path is now set to: /content\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install w3m -y # to act as web browser \n",
    "!sudo apt-get install -y xdg-utils\n",
    "!xdg-settings set default-web-browser w3m.desktop # to set default browser \n",
    "\n",
    "\n",
    "! google-drive-ocamlfuse /content/cmudrive\n",
    "! pip install kaggle wandb torch-summary\n",
    "! mkdir ~/.kaggle\n",
    "! cp /content/cmudrive/IDL/kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! pip install --upgrade --force-reinstall --no-deps kaggle \n",
    "! kaggle config set -n path -v /content\n",
    "\n",
    "! wandb login 4bdbe9c204105e1264fe3f54df2732fd1fff8040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZCQtZtkaTrcn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from python-Levenshtein) (59.2.0)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp38-cp38-linux_x86_64.whl size=82258 sha256=f91a68b80fa3a78a86d78345b7fc93fe583a650a3af1efc940f5612160a40afc\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/d7/0c/76/042b46eb0df65c3ccd0338f791210c55ab79d209bcc269e2c7\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting torchsummaryX\n",
      "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchsummaryX) (1.21.2)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchsummaryX) (1.3.4)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchsummaryX) (1.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->torchsummaryX) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->torchsummaryX) (2021.3)\n",
      "Requirement already satisfied: typing_extensions in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torch->torchsummaryX) (4.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.16.0)\n",
      "Installing collected packages: torchsummaryX\n",
      "Successfully installed torchsummaryX-1.3.0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=1a5bffe8903c8845f38e58db4c5521df8552dc774dd9aa47538773c72d7ce046\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting adamp\n",
      "  Downloading adamp-0.3.0.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: adamp\n",
      "  Building wheel for adamp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for adamp: filename=adamp-0.3.0-py3-none-any.whl size=5999 sha256=2537552f75be78a7a3f136aa4a6a03695ca3329621145bde42a39a89bb257c83\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/32/c3/7e/c2e3196994b608c3a33bb844e9a3ddddad6692f001a04bba7a\n",
      "Successfully built adamp\n",
      "Installing collected packages: adamp\n",
      "Successfully installed adamp-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install torchsummaryX # We also install a summary package to check our model's forward before training\n",
    "# !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip install wget\n",
    "!pip install adamp\n",
    "# %cd ctcdecode\n",
    "# !pip install .\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7AUGUa8tnFhM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 11-785-s22-hw3p2.zip to /content/competitions/11-785-s22-hw3p2\n",
      "100%|█████████████████████████████████████▉| 1.84G/1.84G [00:24<00:00, 86.4MB/s]\n",
      "100%|██████████████████████████████████████| 1.84G/1.84G [00:24<00:00, 79.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle competitions download -c 11-785-s22-hw3p2\n",
    "\n",
    "! unzip -q /content/competitions/11-785-s22-hw3p2/11-785-s22-hw3p2.zip -d /content\n",
    "! mv /content/hw3p2_student_data/hw3p2_student_data /content/speech_data\n",
    "! rm -rf /content/hw3p2_student_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4vZbDmJvMp1"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qI4qfx7tiBZt",
    "outputId": "d34de627-ac5f-4c5b-de61-452a9bde329a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as taf\n",
    "from torchsummaryX import summary\n",
    "# from torchsummary import summary\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "import yaml\n",
    "import wandb\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# imports for decoding and distance calculation\n",
    "# import ctcdecode\n",
    "import Levenshtein\n",
    "# from ctcdecode import CTCBeamDecoder\n",
    "from phonemes import PHONEME_MAP, PHONEMES\n",
    "from adamp import AdamP\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "\n",
    "import warnings\n",
    "import multiprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "def header(head):\n",
    "    print(\"-\"*80)\n",
    "    print(f\"\\t\\t\\t\\t{head.upper()}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUCKqm1ST1sU"
   },
   "source": [
    "# Dataset and dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8SndiVRVqBMa"
   },
   "outputs": [],
   "source": [
    "class LibriSamples(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path=r\"/content/speech_data\", partition=\"train\", transforms=None): # You can use partition to specify train or dev\n",
    "        partition_path = os.path.join(data_path, partition)\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.test = True if partition == \"test\" else False\n",
    "\n",
    "        self.X_dir = os.path.join(partition_path, 'mfcc')\n",
    "        self.Y_dir = os.path.join(partition_path, 'transcript')\n",
    "\n",
    "        if self.test:\n",
    "            with open(os.path.join(partition_path, 'test_order.csv'),\"r\") as f:\n",
    "                self.X_files = list(csv.reader(f))[1:]\n",
    "            self.X_data = [np.load(os.path.join(self.X_dir, xfile[0])) for xfile in tqdm(self.X_files, desc=\"Data\", position=0, leave=True)]\n",
    "        else:\n",
    "            self.X_files = os.listdir(self.X_dir)\n",
    "            self.Y_files = os.listdir(self.Y_dir)\n",
    "\n",
    "            self.X_data = [np.load(os.path.join(self.X_dir, xfile)) for xfile in tqdm(self.X_files, desc=\"Data\", position=0, leave=True)]\n",
    "            self.Y_data = [np.load(os.path.join(self.Y_dir, yfile)) for yfile in tqdm(self.Y_files, desc=\"Labels\", position=0, leave=True)]\n",
    "\n",
    "            assert(len(self.X_data) == len(self.Y_data))\n",
    "\n",
    "        self.phonemes = PHONEMES\n",
    "        self.phonemes_idx = dict(zip(PHONEMES, range(len(PHONEMES))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X_data[idx]\n",
    "        X = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "        if self.test:\n",
    "            return torch.from_numpy(X)\n",
    "        else:\n",
    "            Y = self.Y_data[idx][1:-1] \n",
    "            Yy = np.array([torch.tensor(self.phonemes_idx[t]) for t in Y])\n",
    "            return torch.from_numpy(X), torch.from_numpy(Yy)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        if self.test:\n",
    "            batch_x = [x for x in batch]\n",
    "            batch_x_pad = pad_sequence(batch_x, batch_first=True)\n",
    "            lengths_x = [len(x) for x in batch_x]\n",
    "            return batch_x_pad, torch.tensor(lengths_x)\n",
    "        else:\n",
    "            batch_x = [x for x,_ in batch]\n",
    "            batch_y = [y for _,y in batch]\n",
    "            batch_x_pad = pad_sequence(batch_x, batch_first=True)\n",
    "            batch_y_pad = pad_sequence(batch_y, batch_first=True)\n",
    "            lengths_x = [len(x) for x in batch_x]\n",
    "            lengths_y = [len(y) for y in batch_y]\n",
    "            return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4mzoYfTKu14s"
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# train_data = LibriSamples(partition='train')\n",
    "# # val_data = LibriSamples(partition='dev')\n",
    "# # test_data = LibriSamples(partition='test')\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=128, shuffle=True, drop_last=True, collate_fn = train_data.collate_fn, num_workers=2)\n",
    "# # val_loader = DataLoader(val_data, batch_size=128, shuffle=False, drop_last=True, collate_fn = val_data.collate_fn, num_workers=2)\n",
    "# # test_loader = DataLoader(test_data, batch_size=128, shuffle=False, drop_last=False, collate_fn = test_data.collate_fn)\n",
    "\n",
    "# print(\"Batch size: \", batch_size)\n",
    "# print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "# # print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "# # print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
    "\n",
    "# # Test code for checking shapes and return arguments of the train and val loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7NzQ9GPf1wOy"
   },
   "outputs": [],
   "source": [
    "# for data in train_loader:\n",
    "#     x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
    "#     print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "#     print(x[0], lx[0])\n",
    "#     print(y[0], ly[0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly4mjUUUuJhy"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA-6ZrNI_rSi"
   },
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5FgP7eJD8fdh"
   },
   "outputs": [],
   "source": [
    "# from torch.nn.modules.conv import Conv1d\n",
    "# torch.cuda.empty_cache()\n",
    "# class Block(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1, identity=None):\n",
    "#         super(Block, self).__init__()\n",
    "#         self.expansion = 2\n",
    "\n",
    "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "#         self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "#         self.conv3 = nn.Conv1d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "#         self.bn3 = nn.BatchNorm1d(out_channels * self.expansion)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#         self.identity_downsample = nn.Sequential(\n",
    "#                 nn.Conv1d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n",
    "#                 nn.BatchNorm1d(out_channels * self.expansion)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "#         identity = self.identity_downsample(identity)\n",
    "\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.bn3(x)\n",
    "        \n",
    "#         x += identity\n",
    "#         x = self.relu(x)\n",
    "#         return x\n",
    "\n",
    "# class ResNet(nn.Module): # [3, 4, 6]\n",
    "#     def __init__(self):\n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.in_channels = 13\n",
    "#         self.out_channels = 64\n",
    "#         self.stem = nn.Sequential(\n",
    "#             nn.Conv1d(self.in_channels, 64, kernel_size=3),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#         self.in_channels = 64\n",
    "\n",
    "#         self.resnet1 = self._make_layers(num_blocks=1, out_channels=64, stride=1)\n",
    "#         self.resnet2 = self._make_layers(num_blocks=1, out_channels=128, stride=2)\n",
    "#         self.resnet3 = self._make_layers(num_blocks=1, out_channels=256, stride=2)\n",
    "\n",
    "#     def _make_layers(self, num_blocks, out_channels, stride):\n",
    "#         identity_down = None\n",
    "#         layers = []\n",
    "#         if stride != 1 or self.in_channels != out_channels * 2:\n",
    "#             identity_down = nn.Sequential(\n",
    "#                 nn.Conv1d(self.in_channels, out_channels * 2, kernel_size=1),\n",
    "#                 nn.BatchNorm1d(out_channels * 2)\n",
    "#             )\n",
    "\n",
    "#         layers.append(Block(self.in_channels, out_channels, stride, identity_down))\n",
    "#         self.in_channels = out_channels*2\n",
    "\n",
    "#         for i in range(num_blocks - 1):\n",
    "#             layers.append(Block(self.in_channels, out_channels))\n",
    "\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.stem(x)\n",
    "#         x = self.resnet1(x)\n",
    "#         x = self.resnet2(x)\n",
    "#         x = self.resnet3(x)\n",
    "\n",
    "#         return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yLRX4iwPP90"
   },
   "source": [
    "### Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CGoiXd70tb5z"
   },
   "outputs": [],
   "source": [
    "# class SpeechNet(nn.Module):\n",
    "#     def __init__(self, config): # You can add any extra arguments as you wish\n",
    "#         super().__init__()\n",
    "#         lstm_params = config['arch']\n",
    "#         vocab_size = len(PHONEMES)\n",
    "\n",
    "#         self.stride = 1\n",
    "\n",
    "#         # Embedding\n",
    "#         embeddings = [13] + lstm_params['embedding_size']\n",
    "#         embedding_layers = []\n",
    "#         for i in range(len(embeddings) - 1):\n",
    "#             embedding_layers.append(nn.Conv1d(embeddings[i], embeddings[i+1], kernel_size=3, padding=1))\n",
    "#             embedding_layers.append(nn.BatchNorm1d(embeddings[i+1]))\n",
    "#             embedding_layers.append(nn.ReLU())\n",
    "#             # Maybe add a Pooling\n",
    "#         self.embeddings = nn.Sequential(*embedding_layers)\n",
    "#         embedding_size = embeddings[-1]\n",
    "\n",
    "#         # Block Embedding\n",
    "#         # self.embeddings = nn.Sequential(\n",
    "#         #     # nn.Conv1d(13, 64, kernel_size=5, stride=2, padding=2),\n",
    "#         #     # nn.BatchNorm1d(64),\n",
    "#         #     # nn.ReLU(),\n",
    "#         #     # nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "#         #     nn.Conv1d(64, 64, kernel_size=1, stride=1, padding=0),\n",
    "#         #     nn.BatchNorm1d(64),\n",
    "#         #     nn.Conv1d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "#         #     nn.BatchNorm1d(64),\n",
    "#         #     nn.Conv1d(64, 256, kernel_size=1, stride=1, padding=0),\n",
    "#         #     nn.BatchNorm1d(256),\n",
    "#         #     nn.ReLU()\n",
    "#         # )\n",
    "#         # embedding_size = 256\n",
    "\n",
    "#         # ResNet Embedding\n",
    "#         # self.embeddings = ResNet(layers=[1])\n",
    "#         # embedding_size = 256\n",
    "\n",
    "#         # RNN Block\n",
    "#         if config['rnn'] == 'GRU':\n",
    "#             self.lstm = nn.GRU(\n",
    "#                 input_size=embedding_size,\n",
    "#                 hidden_size=lstm_params['hidden_size'],\n",
    "#                 num_layers=lstm_params['num_layers'],\n",
    "#                 dropout = lstm_params['dropout'],\n",
    "#                 bidirectional=True,\n",
    "#                 batch_first=True\n",
    "#             )\n",
    "#         else:\n",
    "#             self.lstm = nn.LSTM(\n",
    "#                 input_size=embedding_size,\n",
    "#                 hidden_size=lstm_params['hidden_size'],\n",
    "#                 num_layers=lstm_params['num_layers'],\n",
    "#                 dropout = lstm_params['dropout'],\n",
    "#                 bidirectional=True,\n",
    "#                 batch_first=True\n",
    "#                 )\n",
    "\n",
    "#         # Classification\n",
    "#         self.classification = nn.Sequential(\n",
    "#             nn.Linear(lstm_params['hidden_size'] * 2, 2048),\n",
    "#             nn.Dropout(p=0.2),\n",
    "#             nn.Linear(2048, 1024),\n",
    "#             nn.Dropout(p=0.2),\n",
    "#             nn.Linear(1024, vocab_size)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, x_len): \n",
    "#         x = torch.transpose(self.embeddings(torch.transpose(x, 1, 2)), 1, 2)\n",
    "#         # x_len = torch.clamp(x_len, min=0, max=x.shape[1])\n",
    "#         packed_input = pack_padded_sequence(x, x_len, enforce_sorted=False, batch_first=True)\n",
    "#         out, _ = self.lstm(packed_input)\n",
    "#         out, lengths  = pad_packed_sequence(out, batch_first=True)\n",
    "#         out = self.classification(out)\n",
    "#         out = F.log_softmax(out, dim=2)\n",
    "#         return out, lengths\n",
    "\n",
    "#     def initialize_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv1d):\n",
    "#                 nn.init.trunc_normal_(m.weight, std=0.2)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 nn.init.trunc_normal_(m.weight, std=0.2)\n",
    "#                 nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uLjmHc4mMyB4"
   },
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     '': 'LSTM-4layer-medium',\n",
    "#     'batch_size': 128,\n",
    "#     'epochs': 100,\n",
    "#     'scheduler': 'CALR',             # CosineAnnealingLR (CALR), ReduceLRonPlateau (RLRP)\n",
    "#     'optimizer': 'AdamP',            # SGD, Adam, AdamW, AdamP\n",
    "#     'rnn': 'LSTM',                   # GRU, LSTM\n",
    "#     'optim': {'lr': 0.002},\n",
    "#     'decoder': {'beam_width': 3, 'cutoff_top_n': 40, 'cutoff_prob': 1.0},\n",
    "#     'arch': {'embedding_size': [64, 256], 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.25},\n",
    "#     'save': True,\n",
    "#     'log': True,\n",
    "#     'randomize': False,\n",
    "# }\n",
    "# model = SpeechNet(config).cuda()\n",
    "# print(model)\n",
    "# # out, out_lengths = model(x.cuda(), lx)\n",
    "# # print(out.shape, out_lengths.shape)\n",
    "# # print(y.shape, ly.shape)\n",
    "# summary(model, x.cuda(), lx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC78fom2_tfD"
   },
   "source": [
    "### ASRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Wc2fblIK_vKS"
   },
   "outputs": [],
   "source": [
    "class SpeechNet(nn.Module):\n",
    "    def __init__(self, config): # You can add any extra arguments as you wish\n",
    "        super().__init__()\n",
    "        lstm_params = config['arch']\n",
    "        vocab_size = len(PHONEMES)\n",
    "\n",
    "        self.stride = 1\n",
    "\n",
    "        # Embedding\n",
    "        # self.skip = lstm_params['skip_connect']\n",
    "        # embeddings = lstm_params['embedding_size']\n",
    "        # embedding_layers = []\n",
    "        # for i in range(len(embeddings) - 1):\n",
    "        #     conv_params = lstm_params['embedding_args'][i]\n",
    "        #     embedding_layers.append(nn.Conv1d(embeddings[i], embeddings[i+1], **conv_params))\n",
    "        #     embedding_layers.append(nn.BatchNorm1d(embeddings[i+1]))\n",
    "        #     embedding_layers.append(nn.Dropout(0.15))\n",
    "        #     if self.skip:\n",
    "        #         if i != len(embeddings) - 2:\n",
    "        #             embedding_layers.append(nn.ReLU())\n",
    "        #     else:\n",
    "        #         embedding_layers.append(nn.ReLU())\n",
    "        #     # Maybe add a Pooling\n",
    "        # self.embeddings = nn.Sequential(*embedding_layers)\n",
    "        # embedding_size = embeddings[-1]\n",
    "\n",
    "        # if self.skip:\n",
    "        #     self.residual = nn.Sequential(\n",
    "        #         nn.Conv1d(embeddings[0], embeddings[-1], **lstm_params['embedding_args'][1]),\n",
    "        #         nn.BatchNorm1d(embeddings[-1])\n",
    "        #     )\n",
    "        #     self.relu = nn.ReLU()\n",
    "\n",
    "        # Block Embedding\n",
    "        self.embeddings = nn.Sequential(\n",
    "            nn.Conv1d(13, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        embedding_size = 512\n",
    "\n",
    "        # ResNet Embedding\n",
    "        # self.embeddings = ResNet().cuda()\n",
    "        # embedding_size = 512\n",
    "\n",
    "        # RNN Block\n",
    "        if config['rnn'] == 'GRU':\n",
    "            self.lstm = nn.GRU(\n",
    "                input_size=embedding_size,\n",
    "                hidden_size=lstm_params['hidden_size'],\n",
    "                num_layers=lstm_params['num_layers'],\n",
    "                dropout = lstm_params['dropout'],\n",
    "                bidirectional=True,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=embedding_size,\n",
    "                hidden_size=lstm_params['hidden_size'],\n",
    "                num_layers=lstm_params['num_layers'],\n",
    "                dropout = lstm_params['dropout'],\n",
    "                bidirectional=True,\n",
    "                batch_first=True\n",
    "                )\n",
    "\n",
    "        # Classification\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Linear(lstm_params['hidden_size'] * 2, 2048),\n",
    "            nn.Dropout(p=0.35),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_len): \n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        # if self.skip:\n",
    "        #     residual = self.residual(x)\n",
    "        x = self.embeddings(x)\n",
    "        # if self.skip:\n",
    "        #     x = x + residual\n",
    "        #     x = self.relu(x)\n",
    "            \n",
    "        x = torch.transpose(x, 1, 2)\n",
    "#         x_len = torch.clamp(x_len, min=0, max=x.shape[1])\n",
    "        x_len = x_len // 2\n",
    "\n",
    "        packed_input = pack_padded_sequence(x, x_len, enforce_sorted=False, batch_first=True)\n",
    "        out, _ = self.lstm(packed_input)\n",
    "        out, lengths  = pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.classification(out)\n",
    "        out = F.log_softmax(out, dim=2)\n",
    "        return out, lengths\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.2)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.2)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bR_w80unSrU6"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# config = {\n",
    "#     '': 'LSTM-4layer-medium',\n",
    "#     'batch_size': 128,\n",
    "#     'epochs': 100,\n",
    "#     'scheduler': 'CALR',             # CosineAnnealingLR (CALR), ReduceLRonPlateau (RLRP)\n",
    "#     'optimizer': 'AdamP',            # SGD, Adam, AdamW, AdamP\n",
    "#     'rnn': 'LSTM',                   # GRU, LSTM\n",
    "#     'optim': {'lr': 0.002},\n",
    "#     'decoder': {'beam_width': 3, 'cutoff_top_n': 40, 'cutoff_prob': 1.0},\n",
    "#     'arch': {'embedding_size': [64, 256], 'hidden_size': 512, 'num_layers': 4, 'dropout': 0.25},\n",
    "#     'save': True,\n",
    "#     'log': True,\n",
    "#     'randomize': False,\n",
    "# }\n",
    "# model = SpeechNet(config).cuda()\n",
    "# # print(model)\n",
    "# # out, out_lengths = model(x.cuda(), lx)\n",
    "# # print(out.shape, out_lengths.shape)\n",
    "# # print(y.shape, ly.shape)\n",
    "# summary(model, x.cuda(), lx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBwunYpyugFg"
   },
   "source": [
    "# Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "27XuR3frZi79"
   },
   "outputs": [],
   "source": [
    "def convert_to_string(tokens, seq_len, phoneme_map, phonemes):\n",
    "    return \"\".join([phoneme_map[phonemes[x]] for x in tokens[0:seq_len]])\n",
    "\n",
    "def calculate_levenshtein(decoder, h, y, lh, ly):\n",
    "    phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
    "    batch_size = ly.shape[0]\n",
    "    dist = 0\n",
    "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(h, seq_lens = lh)\n",
    "\n",
    "    for i in range(batch_size): # Loop through each element in the batch\n",
    "        h_string = convert_to_string(beam_results[i][0], out_lens[i][0], phoneme_map, PHONEMES)\n",
    "        y_string = convert_to_string(y[i], ly[i], phoneme_map, PHONEMES)\n",
    "        dist += Levenshtein.distance(h_string, y_string)\n",
    "    dist/=batch_size\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kzwsqG_eqKMp"
   },
   "outputs": [],
   "source": [
    "class ModelSetup:\n",
    "    def __init__(self, config, save_path):\n",
    "        self.config = config\n",
    "        self.log = config['log']\n",
    "        self.save = config['save']\n",
    "        print(f\"Saving : {self.save} and Logging : {self.log}\")\n",
    "\n",
    "        self.phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
    "        self.phonemes_ctc = PHONEME_MAP\n",
    "\n",
    "        self.SAVE_DIR = save_path\n",
    "        self.DATA_DIR = r\"/content/speech_data\" \n",
    "\n",
    "    def __gen_model_name(self):\n",
    "        # Generate a model name based on config\n",
    "        save_name = ''\n",
    "\n",
    "        for key, val in self.config.items():\n",
    "            abbr = key[0] if len(key) > 2 else key\n",
    "            if isinstance(val, dict):\n",
    "                data = 'lr' + str(val[\"lr\"])\n",
    "                save_name += data\n",
    "                break\n",
    "            elif key == '':\n",
    "                save_name += abbr + str(val)\n",
    "                for key, val in self.config['arch'].items():\n",
    "                    save_name += '-' + str(val)\n",
    "                save_name += '_'\n",
    "            else:\n",
    "                data = abbr + str(val) + '_'\n",
    "                save_name += data\n",
    "                \n",
    "\n",
    "        if self.config['randomize']:\n",
    "            save_name = save_name + \"-v\" + str(np.random.randint(10, 1000))\n",
    "        print(\"\\nModel Name: \", save_name)\n",
    "        self.model_name = save_name\n",
    "\n",
    "    def __save_model_params(self, continue_train):\n",
    "        # Create Model Directory\n",
    "        save_path = os.path.join(self.SAVE_DIR, self.model_name)\n",
    "        if not continue_train:\n",
    "            try:\n",
    "                os.mkdir(save_path)\n",
    "            except FileExistsError:\n",
    "                d = input(\"Model name already exists. Delete existing model? (y/n)\")\n",
    "                if d == 'y':\n",
    "                    import shutil\n",
    "                    shutil.rmtree(save_path)\n",
    "                    os.mkdir(save_path)\n",
    "                else:\n",
    "                    print(\"Exiting!\")\n",
    "                    exit(0)\n",
    "                    return None\n",
    "\n",
    "            os.mkdir(os.path.join(save_path, 'Checkpoints'))\n",
    "            # Saving Model Configuration\n",
    "            with open(os.path.join(save_path, 'model_config.yaml'), 'w') as metadata:\n",
    "                yaml.dump({'Experiment': self.config['']}, metadata, indent=4, default_flow_style=False)\n",
    "                yaml.dump(self.config, metadata, indent=4, default_flow_style=False)\n",
    "            print(\"Model to be saved at: \", save_path)\n",
    "        self.model_path = save_path\n",
    "\n",
    "    def dataloaders(self): \n",
    "        # self.train_transforms = [\n",
    "        #                          taf.TimeMasking(10, p=0.25),\n",
    "        #                          taf.FrequencyMasking(1)\n",
    "\n",
    "        # ]\n",
    "        # self.val_transforms = []\n",
    "        self.train_data = LibriSamples(partition='train')\n",
    "        self.val_data = LibriSamples(partition='dev')\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_data, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            shuffle=True, \n",
    "            drop_last=True, \n",
    "            collate_fn = self.train_data.collate_fn, \n",
    "            num_workers=4,\n",
    "            pin_memory=True) \n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_data, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            shuffle=False, \n",
    "            drop_last=True, \n",
    "            collate_fn = self.val_data.collate_fn, \n",
    "            num_workers=4,\n",
    "            pin_memory=True)\n",
    "\n",
    "    def save_checkpoint(self, epoch, model, optimizer, loss):\n",
    "        print(\"Saving Checkpoint!\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(epoch) + '.pth'))\n",
    "\n",
    "    def save_model(self, epoch=None, onnx=False):\n",
    "        print(\"Saving Model!\")\n",
    "        try:\n",
    "            if not self.save:\n",
    "                self.__save_model_params()\n",
    "            if epoch is None:\n",
    "                name = os.path.join(self.model_path, \"model_\" + str(epoch) + \".pth\")\n",
    "            else:\n",
    "                name = os.path.join(self.model_path, \"model\" + \".pth\")\n",
    "            torch.save(self.model.state_dict(), name)\n",
    "            if onnx:\n",
    "                torch.onnx.export(self.model, name.split('.')[0] + '.onnx')\n",
    "                wandb.save(name.split('.')[0] + '.onnx')\n",
    "        except:\n",
    "            print(\"Model couldn't be saved!\")\n",
    "\n",
    "    def setup(self, continue_train=False, chkpt=None):\n",
    "        header(\"Model Setup\")\n",
    "\n",
    "        # Model\n",
    "        self.model = SpeechNet(self.config).cuda()\n",
    "        summary(self.model, torch.randn(128, 1692, 13).cuda(), torch.tensor([128]))\n",
    "        self.__gen_model_name()\n",
    "        if self.save: self.__save_model_params(continue_train)\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CTCLoss()\n",
    "\n",
    "        # Decoder\n",
    "#         self.decoder = CTCBeamDecoder(\n",
    "#                 self.phonemes_ctc,\n",
    "#                 blank_id=0,\n",
    "#                 log_probs_input=True,\n",
    "#                 **self.config['decoder']\n",
    "#         )\n",
    "        \n",
    "        # Optimizer\n",
    "        if self.config[\"optimizer\"] == 'SGD':\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), **self.config['optim'])\n",
    "        elif self.config[\"optimizer\"] == \"Adam\":\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        elif self.config[\"optimizer\"] == \"AdamW\":\n",
    "            self.optimizer = optim.AdamW(self.model.parameters(), **self.config['optim'])\n",
    "        elif self.config[\"optimizer\"] == \"AdamP\":\n",
    "            self.optimizer = AdamP(self.model.parameters(), **self.config['optim'])\n",
    "\n",
    "        self.chkpt = 0\n",
    "        if continue_train:\n",
    "            self.chkpt = chkpt\n",
    "            assert chkpt is not None\n",
    "\n",
    "            chkpt_path = os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(chkpt) + '.pth')\n",
    "            try:\n",
    "                checkpoint = torch.load(chkpt_path)\n",
    "            except FileNotFoundError:\n",
    "                print(\"Checkpoint not found in the directory!\")\n",
    "                print(\"Incorrect: \", chkpt_path)\n",
    "                exit(0)\n",
    "\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        # Scheduler\n",
    "        if self.config[\"scheduler\"] == 'CALR':\n",
    "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=(len(self.train_loader) * self.config['epochs']))\n",
    "        elif self.config[\"scheduler\"] == 'RLRP':\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, \n",
    "                mode='min', \n",
    "                factor=0.5, \n",
    "                patience=10,\n",
    "                threshold=0.25,\n",
    "                cooldown=15)\n",
    "        elif self.config[\"scheduler\"] == None:\n",
    "            pass\n",
    "\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    def train(self, continue_train=False, save_freq=2):\n",
    "        header(\"Training\")\n",
    "        epochs = self.config['epochs']\n",
    "        batch_size = self.config['batch_size']\n",
    "\n",
    "        if self.log:\n",
    "            wandb.init(project=\"hw3-chinmay\", entity=\"dl-study-group\", config=self.config, name=self.config[''])\n",
    "            wandb.watch(self.model, criterion=self.criterion, log=\"all\", log_freq=batch_size, idx=None)\n",
    "\n",
    "        delta_time = datetime.timedelta(seconds = 0)\n",
    "        for epoch in range(self.chkpt, epochs + 50):\n",
    "            start_time = time.time()\n",
    "\n",
    "            self.model.train()\n",
    "            print(\"\\nEpoch-\", epoch + 1)\n",
    "            batch_bar = tqdm(total=len(self.train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "            total_loss = 0\n",
    "\n",
    "            for i, data in enumerate(self.train_loader, 0):\n",
    "                torch.cuda.empty_cache()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                x, y, x_len, y_len = data\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "                with torch.cuda.amp.autocast():    \n",
    "                    output, output_len = self.model(x, x_len)    # B x T x 41\n",
    "                    output_transposed = torch.transpose(output, 0, 1)              # T x B x 41\n",
    "                    loss = self.criterion(output_transposed, y, output_len, y_len)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_bar.set_postfix(\n",
    "                    loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "                    lr=\"{:.04f}\".format(float(self.optimizer.param_groups[0]['lr']))\n",
    "                    )\n",
    "                self.scaler.scale(loss).backward() \n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                if self.config[\"scheduler\"] == 'CALR': self.scheduler.step()\n",
    "                batch_bar.update()\n",
    "  \n",
    "            batch_bar.close()\n",
    "            trainlos = float(total_loss / len(self.train_loader))\n",
    "            trainlra = float(self.optimizer.param_groups[0]['lr'])\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} | Train Loss {trainlos:.04f} | Learning Rate {trainlra:.04f}\")\n",
    "\n",
    "            if self.log:\n",
    "                    wandb.log({\"Training Loss\": trainlos, \"Learning Rate\": trainlra})\n",
    "                    \n",
    "#             lev = self.validate()\n",
    "            if self.config[\"scheduler\"] == 'RLRP': self.scheduler.step(trainlos)\n",
    "\n",
    "            delta_time += datetime.timedelta(seconds = (time.time() - start_time))\n",
    "            time_lapsed = delta_time\n",
    "            time_left = delta_time * (epochs - epoch - 1) / (epoch + 1)\n",
    "            print(f\"Time lapsed = {str(time_lapsed)}\")\n",
    "            print(f\"Time left = {str(time_left)}\")\n",
    "\n",
    "            if self.save:\n",
    "                # Save Model\n",
    "                if epoch % save_freq == 0: self.save_model(epoch)\n",
    "                # Save Checkpoint\n",
    "                self.save_checkpoint(epoch, self.model, self.optimizer, total_loss / len(self.train_loader))\n",
    "        \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        batch_bar = tqdm(total=len(self.val_loader), position=0, leave=False, desc='Val')\n",
    "        total_levenshtein = 0\n",
    "        for i, data in enumerate(self.val_loader, 0):\n",
    "            x, y, x_len, y_len = data\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            with torch.no_grad():    \n",
    "                output, output_len = self.model(x, x_len)\n",
    "\n",
    "            total_levenshtein += calculate_levenshtein(self.decoder, output, y, output_len, y_len)\n",
    "            \n",
    "            batch_bar.set_postfix(LD=\"{:.04f}\".format(float(total_levenshtein / (i + 1))))\n",
    "            batch_bar.update()\n",
    "\n",
    "        batch_bar.close()\n",
    "        val_lev = float(total_levenshtein / len(self.val_loader))\n",
    "        print(\"\\nValidation LD: {:.04f}\".format(val_lev))\n",
    "        if self.log:\n",
    "            wandb.log({\"Validation LD\": val_lev})\n",
    "\n",
    "        return val_lev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72yaABA_w8kt"
   },
   "source": [
    "# Hyperparameters and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSW09tgXxERy",
    "outputId": "adec8125-6aaa-4269-9d4a-85c54b037c83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving : True and Logging : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28539/28539 [00:09<00:00, 2855.27it/s]\n",
      "Labels: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28539/28539 [00:07<00:00, 3612.87it/s]\n",
      "Data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2703/2703 [00:00<00:00, 3079.21it/s]\n",
      "Labels: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2703/2703 [00:00<00:00, 3640.73it/s]\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    '': 'AWS_L4600-Conv3-L2-Drop6',\n",
    "    'batch_size': 128,\n",
    "    'epochs': 105,\n",
    "    'scheduler': 'CALR',             # CosineAnnealingLR (CALR), ReduceLRonPlateau (RLRP)\n",
    "    'optimizer': 'AdamP',            # SGD, Adam, AdamW, AdamP\n",
    "    'rnn': 'LSTM',                   # GRU, LSTM\n",
    "    'optim': {'lr': 0.002},\n",
    "    'decoder': {'beam_width': 3, 'cutoff_top_n': 40, 'cutoff_prob': 1.0},\n",
    "    'arch': {'hidden_size': 600, 'num_layers': 4, 'dropout':0.6},\n",
    "    'save': True,\n",
    "    'log': True,\n",
    "    'randomize': False,\n",
    "}\n",
    "\n",
    "# SpeechNet\n",
    "folder_path = r'/content/cmudrive/IDL/hw3-last'\n",
    "asr = ModelSetup(config, save_path = folder_path)\n",
    "asr.dataloaders()\n",
    "\n",
    "# # Continue Training\n",
    "# torch.cuda.empty_cache()\n",
    "# asr.setup(continue_train=True, chkpt=58)\n",
    "# asr.train(continue_train=True)\n",
    "# asr.save_model()\n",
    "# if asr.log: wandb.finish()\n",
    "# latest_model_name = asr.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJgAbnp5W2vL",
    "outputId": "2e5f473d-8908-4813-9f48-7de1d7fe2f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\t\t\t\tMODEL SETUP\n",
      "--------------------------------------------------------------------------------\n",
      "====================================================================================\n",
      "                              Kernel Shape     Output Shape     Params  \\\n",
      "Layer                                                                    \n",
      "0_embeddings.Conv1d_0         [13, 128, 5]  [128, 128, 846]     8.448k   \n",
      "1_embeddings.BatchNorm1d_1           [128]  [128, 128, 846]      256.0   \n",
      "2_embeddings.ReLU_2                      -  [128, 128, 846]          -   \n",
      "3_embeddings.Conv1d_3        [128, 256, 3]  [128, 256, 846]     98.56k   \n",
      "4_embeddings.BatchNorm1d_4           [256]  [128, 256, 846]      512.0   \n",
      "5_embeddings.ReLU_5                      -  [128, 256, 846]          -   \n",
      "6_embeddings.Conv1d_6        [256, 512, 3]  [128, 512, 846]   393.728k   \n",
      "7_embeddings.BatchNorm1d_7           [512]  [128, 512, 846]     1.024k   \n",
      "8_embeddings.ReLU_8                      -  [128, 512, 846]          -   \n",
      "9_lstm                                   -       [64, 1200]    31.296M   \n",
      "10_classification.Linear_0    [1200, 2048]    [1, 64, 2048]  2.459648M   \n",
      "11_classification.Dropout_1              -    [1, 64, 2048]          -   \n",
      "12_classification.ReLU_2                 -    [1, 64, 2048]          -   \n",
      "13_classification.Linear_3      [2048, 41]      [1, 64, 41]    84.009k   \n",
      "\n",
      "                               Mult-Adds  \n",
      "Layer                                     \n",
      "0_embeddings.Conv1d_0           7.03872M  \n",
      "1_embeddings.BatchNorm1d_1         128.0  \n",
      "2_embeddings.ReLU_2                    -  \n",
      "3_embeddings.Conv1d_3         83.165184M  \n",
      "4_embeddings.BatchNorm1d_4         256.0  \n",
      "5_embeddings.ReLU_5                    -  \n",
      "6_embeddings.Conv1d_6        332.660736M  \n",
      "7_embeddings.BatchNorm1d_7         512.0  \n",
      "8_embeddings.ReLU_8                    -  \n",
      "9_lstm                          31.2576M  \n",
      "10_classification.Linear_0       2.4576M  \n",
      "11_classification.Dropout_1            -  \n",
      "12_classification.ReLU_2               -  \n",
      "13_classification.Linear_3       83.968k  \n",
      "------------------------------------------------------------------------------------\n",
      "                           Totals\n",
      "Total params           34.342185M\n",
      "Trainable params       34.342185M\n",
      "Non-trainable params          0.0\n",
      "Mult-Adds             456.664704M\n",
      "====================================================================================\n",
      "\n",
      "Model Name:  AWS_L4600-Conv3-L2-Drop6-600-4-0.6_b128_e105_sCALR_oAdamP_rLSTM_lr0.002\n",
      "Model to be saved at:  /content/cmudrive/IDL/hw3-last/AWS_L4600-Conv3-L2-Drop6-600-4-0.6_b128_e105_sCALR_oAdamP_rLSTM_lr0.002\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "asr.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "MG4F77Nm0Am9",
    "outputId": "949ffb23-9620-4b2e-b0a8-3f3940d30f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\t\t\t\tTRAINING\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnefario7\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20220414_175203-29nkihhu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dl-study-group/hw3-chinmay/runs/29nkihhu\" target=\"_blank\">AWS_L4600-Conv3-L2-Drop6</a></strong> to <a href=\"https://wandb.ai/dl-study-group/hw3-chinmay\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch- 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/105 | Train Loss 3.5916 | Learning Rate 0.0020\n",
      "Time lapsed = 0:05:03.446878\n",
      "Time left = 8:45:58.475312\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/105 | Train Loss 2.8856 | Learning Rate 0.0020\n",
      "Time lapsed = 0:10:07.427872\n",
      "Time left = 8:41:22.535408\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/105 | Train Loss 1.2630 | Learning Rate 0.0020\n",
      "Time lapsed = 0:15:11.869954\n",
      "Time left = 8:36:43.578436\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/105 | Train Loss 0.7602 | Learning Rate 0.0020\n",
      "Time lapsed = 0:20:16.528024\n",
      "Time left = 8:31:57.332606\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/105 | Train Loss 0.6004 | Learning Rate 0.0020\n",
      "Time lapsed = 0:25:21.210176\n",
      "Time left = 8:27:04.203520\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/105 | Train Loss 0.5194 | Learning Rate 0.0020\n",
      "Time lapsed = 0:30:25.749726\n",
      "Time left = 8:22:04.870479\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/105 | Train Loss 0.4648 | Learning Rate 0.0020\n",
      "Time lapsed = 0:35:30.091937\n",
      "Time left = 8:17:01.287118\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/105 | Train Loss 0.4209 | Learning Rate 0.0020\n",
      "Time lapsed = 0:40:34.712534\n",
      "Time left = 8:12:00.889475\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/105 | Train Loss 0.3923 | Learning Rate 0.0020\n",
      "Time lapsed = 0:45:38.887191\n",
      "Time left = 8:06:54.796704\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/105 | Train Loss 0.3606 | Learning Rate 0.0020\n",
      "Time lapsed = 0:50:43.905417\n",
      "Time left = 8:01:57.101462\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/105 | Train Loss 0.3397 | Learning Rate 0.0019\n",
      "Time lapsed = 0:55:48.391182\n",
      "Time left = 7:56:53.524646\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/105 | Train Loss 0.3298 | Learning Rate 0.0019\n",
      "Time lapsed = 1:00:52.946078\n",
      "Time left = 7:51:50.332104\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/105 | Train Loss 0.3034 | Learning Rate 0.0019\n",
      "Time lapsed = 1:05:56.626750\n",
      "Time left = 7:46:40.743154\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/105 | Train Loss 0.2917 | Learning Rate 0.0019\n",
      "Time lapsed = 1:11:01.385371\n",
      "Time left = 7:41:39.004912\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/105 | Train Loss 0.2798 | Learning Rate 0.0019\n",
      "Time lapsed = 1:16:05.950067\n",
      "Time left = 7:36:35.700402\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/105 | Train Loss 0.2688 | Learning Rate 0.0019\n",
      "Time lapsed = 1:21:10.267466\n",
      "Time left = 7:31:30.862780\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/105 | Train Loss 0.2609 | Learning Rate 0.0019\n",
      "Time lapsed = 1:26:14.205335\n",
      "Time left = 7:26:24.121734\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/105 | Train Loss 0.2424 | Learning Rate 0.0019\n",
      "Time lapsed = 1:31:17.921368\n",
      "Time left = 7:21:16.619945\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/105 | Train Loss 0.2406 | Learning Rate 0.0018\n",
      "Time lapsed = 1:36:21.776282\n",
      "Time left = 7:16:10.145276\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/105 | Train Loss 0.2370 | Learning Rate 0.0018\n",
      "Time lapsed = 1:41:26.272012\n",
      "Time left = 7:11:06.656051\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/105 | Train Loss 0.2178 | Learning Rate 0.0018\n",
      "Time lapsed = 1:46:30.578656\n",
      "Time left = 7:06:02.314624\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/105 | Train Loss 0.2073 | Learning Rate 0.0018\n",
      "Time lapsed = 1:51:35.762093\n",
      "Time left = 7:01:01.284260\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/105 | Train Loss 0.2016 | Learning Rate 0.0018\n",
      "Time lapsed = 1:56:40.504165\n",
      "Time left = 6:55:58.319197\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/105 | Train Loss 0.1993 | Learning Rate 0.0018\n",
      "Time lapsed = 2:01:45.222312\n",
      "Time left = 6:50:55.125303\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/105 | Train Loss 0.1993 | Learning Rate 0.0017\n",
      "Time lapsed = 2:06:49.424190\n",
      "Time left = 6:45:50.157408\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/105 | Train Loss 0.1868 | Learning Rate 0.0017\n",
      "Time lapsed = 2:11:54.410317\n",
      "Time left = 6:40:47.631348\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/105 | Train Loss 0.1767 | Learning Rate 0.0017\n",
      "Time lapsed = 2:16:59.132958\n",
      "Time left = 6:35:44.161879\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/105 | Train Loss 0.1711 | Learning Rate 0.0017\n",
      "Time lapsed = 2:22:03.920658\n",
      "Time left = 6:30:40.781810\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/105 | Train Loss 0.1641 | Learning Rate 0.0016\n",
      "Time lapsed = 2:27:09.543797\n",
      "Time left = 6:25:39.494089\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/105 | Train Loss 0.1570 | Learning Rate 0.0016\n",
      "Time lapsed = 2:32:14.099360\n",
      "Time left = 6:20:35.248400\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/105 | Train Loss 0.1491 | Learning Rate 0.0016\n",
      "Time lapsed = 2:37:18.294342\n",
      "Time left = 6:15:30.121978\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/105 | Train Loss 0.1459 | Learning Rate 0.0016\n",
      "Time lapsed = 2:42:22.853430\n",
      "Time left = 6:10:25.884387\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/105 | Train Loss 0.1422 | Learning Rate 0.0016\n",
      "Time lapsed = 2:47:27.706814\n",
      "Time left = 6:05:22.269412\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/105 | Train Loss 0.1360 | Learning Rate 0.0015\n",
      "Time lapsed = 2:52:31.693393\n",
      "Time left = 6:00:16.771497\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/105 | Train Loss 0.1325 | Learning Rate 0.0015\n",
      "Time lapsed = 2:57:36.116781\n",
      "Time left = 5:55:12.233562\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/105 | Train Loss 0.1249 | Learning Rate 0.0015\n",
      "Time lapsed = 3:02:40.987645\n",
      "Time left = 5:50:08.559653\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/105 | Train Loss 0.1203 | Learning Rate 0.0014\n",
      "Time lapsed = 3:07:45.806248\n",
      "Time left = 5:45:04.724996\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/105 | Train Loss 0.1193 | Learning Rate 0.0014\n",
      "Time lapsed = 3:12:49.980751\n",
      "Time left = 5:39:59.702903\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/105 | Train Loss 0.1146 | Learning Rate 0.0014\n",
      "Time lapsed = 3:17:54.317066\n",
      "Time left = 5:34:54.998112\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/105 | Train Loss 0.1057 | Learning Rate 0.0014\n",
      "Time lapsed = 3:22:59.134007\n",
      "Time left = 5:29:51.092761\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/105 | Train Loss 0.1027 | Learning Rate 0.0013\n",
      "Time lapsed = 3:28:03.677686\n",
      "Time left = 5:24:46.716388\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/105 | Train Loss 0.0972 | Learning Rate 0.0013\n",
      "Time lapsed = 3:33:06.664924\n",
      "Time left = 5:19:39.997386\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/105 | Train Loss 0.0946 | Learning Rate 0.0013\n",
      "Time lapsed = 3:38:10.136554\n",
      "Time left = 5:14:34.150380\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/105 | Train Loss 0.0963 | Learning Rate 0.0013\n",
      "Time lapsed = 3:43:13.509164\n",
      "Time left = 5:09:28.274068\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/105 | Train Loss 0.0945 | Learning Rate 0.0012\n",
      "Time lapsed = 3:48:17.644531\n",
      "Time left = 5:04:23.526041\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/105 | Train Loss 0.0874 | Learning Rate 0.0012\n",
      "Time lapsed = 3:53:20.704605\n",
      "Time left = 4:59:17.425472\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/105 | Train Loss 0.0823 | Learning Rate 0.0012\n",
      "Time lapsed = 3:58:24.841334\n",
      "Time left = 4:54:12.782923\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/105 | Train Loss 0.0774 | Learning Rate 0.0011\n",
      "Time lapsed = 4:03:29.482455\n",
      "Time left = 4:49:08.760415\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/105 | Train Loss 0.0761 | Learning Rate 0.0011\n",
      "Time lapsed = 4:08:33.929317\n",
      "Time left = 4:44:04.490648\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/105 | Train Loss 0.0718 | Learning Rate 0.0011\n",
      "Time lapsed = 4:13:38.686519\n",
      "Time left = 4:39:00.555171\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/105 | Train Loss 0.0714 | Learning Rate 0.0010\n",
      "Time lapsed = 4:18:43.140735\n",
      "Time left = 4:33:56.266661\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/105 | Train Loss 0.0686 | Learning Rate 0.0010\n",
      "Time lapsed = 4:23:47.825955\n",
      "Time left = 4:28:52.207223\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/105 | Train Loss 0.0644 | Learning Rate 0.0010\n",
      "Time lapsed = 4:28:52.300030\n",
      "Time left = 4:23:47.917011\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/105 | Train Loss 0.0601 | Learning Rate 0.0010\n",
      "Time lapsed = 4:33:56.624264\n",
      "Time left = 4:18:43.478472\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/105 | Train Loss 0.0574 | Learning Rate 0.0009\n",
      "Time lapsed = 4:39:01.831798\n",
      "Time left = 4:13:39.847089\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/105 | Train Loss 0.0540 | Learning Rate 0.0009\n",
      "Time lapsed = 4:44:06.747825\n",
      "Time left = 4:08:35.904347\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/105 | Train Loss 0.0518 | Learning Rate 0.0009\n",
      "Time lapsed = 4:49:11.236602\n",
      "Time left = 4:03:31.567665\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/105 | Train Loss 0.0507 | Learning Rate 0.0008\n",
      "Time lapsed = 4:54:15.457828\n",
      "Time left = 3:58:27.008930\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/105 | Train Loss 0.0493 | Learning Rate 0.0008\n",
      "Time lapsed = 4:59:20.168141\n",
      "Time left = 3:53:22.842957\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/105 | Train Loss 0.0465 | Learning Rate 0.0008\n",
      "Time lapsed = 5:04:25.476875\n",
      "Time left = 3:48:19.107656\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/105 | Train Loss 0.0430 | Learning Rate 0.0007\n",
      "Time lapsed = 5:09:29.610588\n",
      "Time left = 3:43:14.473211\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/105 | Train Loss 0.0397 | Learning Rate 0.0007\n",
      "Time lapsed = 5:14:33.854324\n",
      "Time left = 3:38:09.931225\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/105 | Train Loss 0.0384 | Learning Rate 0.0007\n",
      "Time lapsed = 5:19:37.322419\n",
      "Time left = 3:33:04.881613\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/105 | Train Loss 0.0370 | Learning Rate 0.0007\n",
      "Time lapsed = 5:24:42.159877\n",
      "Time left = 3:28:00.758671\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/105 | Train Loss 0.0352 | Learning Rate 0.0006\n",
      "Time lapsed = 5:29:47.042005\n",
      "Time left = 3:22:56.641234\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/105 | Train Loss 0.0324 | Learning Rate 0.0006\n",
      "Time lapsed = 5:34:51.890296\n",
      "Time left = 3:17:52.480629\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/105 | Train Loss 0.0320 | Learning Rate 0.0006\n",
      "Time lapsed = 5:39:56.505817\n",
      "Time left = 3:12:48.167478\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/105 | Train Loss 0.0292 | Learning Rate 0.0006\n",
      "Time lapsed = 5:45:01.176821\n",
      "Time left = 3:07:43.875623\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/105 | Train Loss 0.0282 | Learning Rate 0.0005\n",
      "Time lapsed = 5:50:05.516803\n",
      "Time left = 3:02:39.400071\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/105 | Train Loss 0.0263 | Learning Rate 0.0005\n",
      "Time lapsed = 5:55:10.143561\n",
      "Time left = 2:57:35.071780\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/105 | Train Loss 0.0249 | Learning Rate 0.0005\n",
      "Time lapsed = 6:00:14.516412\n",
      "Time left = 2:52:30.613493\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/105 | Train Loss 0.0241 | Learning Rate 0.0004\n",
      "Time lapsed = 6:05:18.914833\n",
      "Time left = 2:47:26.169298\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/105 | Train Loss 0.0217 | Learning Rate 0.0004\n",
      "Time lapsed = 6:10:23.308218\n",
      "Time left = 2:42:21.724150\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/105 | Train Loss 0.0211 | Learning Rate 0.0004\n",
      "Time lapsed = 6:15:28.097425\n",
      "Time left = 2:37:17.446219\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/105 | Train Loss 0.0203 | Learning Rate 0.0004\n",
      "Time lapsed = 6:20:32.589249\n",
      "Time left = 2:32:13.035700\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/105 | Train Loss 0.0192 | Learning Rate 0.0004\n",
      "Time lapsed = 6:25:36.257558\n",
      "Time left = 2:27:08.308805\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/105 | Train Loss 0.0185 | Learning Rate 0.0003\n",
      "Time lapsed = 6:30:40.277364\n",
      "Time left = 2:22:03.737223\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/105 | Train Loss 0.0171 | Learning Rate 0.0003\n",
      "Time lapsed = 6:35:44.774623\n",
      "Time left = 2:16:59.345062\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/105 | Train Loss 0.0160 | Learning Rate 0.0003\n",
      "Time lapsed = 6:40:49.394622\n",
      "Time left = 2:11:54.990635\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/105 | Train Loss 0.0151 | Learning Rate 0.0003\n",
      "Time lapsed = 6:45:53.951295\n",
      "Time left = 2:06:50.609780\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/105 | Train Loss 0.0140 | Learning Rate 0.0002\n",
      "Time lapsed = 6:50:58.311665\n",
      "Time left = 2:01:46.166419\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/105 | Train Loss 0.0134 | Learning Rate 0.0002\n",
      "Time lapsed = 6:56:03.088762\n",
      "Time left = 1:56:41.841970\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/105 | Train Loss 0.0129 | Learning Rate 0.0002\n",
      "Time lapsed = 7:01:07.517744\n",
      "Time left = 1:51:37.414342\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/105 | Train Loss 0.0121 | Learning Rate 0.0002\n",
      "Time lapsed = 7:06:13.028456\n",
      "Time left = 1:46:33.257114\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  69%|██████████████████████████████████████████████████████████████████████████████████▌                                    | 154/222 [03:31<01:34,  1.39s/it, loss=0.0115, lr=0.0002]wandb: Network error (ReadTimeout), entering retry loop.\n",
      "Train:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 219/222 [05:00<00:04,  1.34s/it, loss=0.0116, lr=0.0002]wandb: ERROR Error while calling W&B API: Error 1040: Too many connections (<Response [500]>)\n",
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/105 | Train Loss 0.0116 | Learning Rate 0.0002\n",
      "Time lapsed = 7:11:17.763613\n",
      "Time left = 1:41:28.885556\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/105 | Train Loss 0.0112 | Learning Rate 0.0002\n",
      "Time lapsed = 7:16:22.105282\n",
      "Time left = 1:36:24.418609\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/105 | Train Loss 0.0110 | Learning Rate 0.0001\n",
      "Time lapsed = 7:21:26.923024\n",
      "Time left = 1:31:20.053039\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/105 | Train Loss 0.0103 | Learning Rate 0.0001\n",
      "Time lapsed = 7:26:31.428140\n",
      "Time left = 1:26:15.616800\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/105 | Train Loss 0.0096 | Learning Rate 0.0001\n",
      "Time lapsed = 7:31:35.487512\n",
      "Time left = 1:21:11.098879\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/105 | Train Loss 0.0094 | Learning Rate 0.0001\n",
      "Time lapsed = 7:36:39.289224\n",
      "Time left = 1:16:06.548204\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/105 | Train Loss 0.0091 | Learning Rate 0.0001\n",
      "Time lapsed = 7:41:43.736763\n",
      "Time left = 1:11:02.113348\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/105 | Train Loss 0.0088 | Learning Rate 0.0001\n",
      "Time lapsed = 7:46:48.489276\n",
      "Time left = 1:05:57.721311\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/105 | Train Loss 0.0084 | Learning Rate 0.0001\n",
      "Time lapsed = 7:51:54.252630\n",
      "Time left = 1:00:53.451952\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/105 | Train Loss 0.0082 | Learning Rate 0.0001\n",
      "Time lapsed = 7:56:59.157958\n",
      "Time left = 0:55:49.050399\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/105 | Train Loss 0.0081 | Learning Rate 0.0000\n",
      "Time lapsed = 8:02:03.901304\n",
      "Time left = 0:50:44.621190\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/105 | Train Loss 0.0078 | Learning Rate 0.0000\n",
      "Time lapsed = 8:07:09.474324\n",
      "Time left = 0:45:40.263218\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/105 | Train Loss 0.0076 | Learning Rate 0.0000\n",
      "Time lapsed = 8:12:14.382512\n",
      "Time left = 0:40:35.825362\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/105 | Train Loss 0.0075 | Learning Rate 0.0000\n",
      "Time lapsed = 8:17:18.353516\n",
      "Time left = 0:35:31.310965\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/105 | Train Loss 0.0074 | Learning Rate 0.0000\n",
      "Time lapsed = 8:22:23.067800\n",
      "Time left = 0:30:26.852594\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/105 | Train Loss 0.0074 | Learning Rate 0.0000\n",
      "Time lapsed = 8:27:27.341440\n",
      "Time left = 0:25:22.367072\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/105 | Train Loss 0.0073 | Learning Rate 0.0000\n",
      "Time lapsed = 8:32:31.818275\n",
      "Time left = 0:20:17.893793\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/105 | Train Loss 0.0073 | Learning Rate 0.0000\n",
      "Time lapsed = 8:37:36.422064\n",
      "Time left = 0:15:13.424178\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/105 | Train Loss 0.0072 | Learning Rate 0.0000\n",
      "Time lapsed = 8:42:41.579603\n",
      "Time left = 0:10:08.962711\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/105 | Train Loss 0.0073 | Learning Rate 0.0000\n",
      "Time lapsed = 8:47:44.881506\n",
      "Time left = 0:05:04.470014\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/105 | Train Loss 0.0072 | Learning Rate 0.0000\n",
      "Time lapsed = 8:52:49.047834\n",
      "Time left = 0:00:00\n",
      "Saving Model!\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/105 | Train Loss 0.0072 | Learning Rate 0.0000\n",
      "Time lapsed = 8:57:52.887909\n",
      "Time left = -1 day, 23:54:55.538793\n",
      "Saving Checkpoint!\n",
      "\n",
      "Epoch- 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  18%|█████████████████████                                                                                                   | 39/222 [00:54<04:04,  1.33s/it, loss=0.0070, lr=0.0000]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23025/1618221290.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0masr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0masr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0masr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23025/3750582557.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, continue_train, save_freq)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{:.04f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     )\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "asr.train()\n",
    "asr.save_model()\n",
    "\n",
    "if asr.log: wandb.finish()\n",
    "\n",
    "latest_model_name = asr.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2ptgaeHadVN"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMSwlCIDacCP"
   },
   "outputs": [],
   "source": [
    "# # def calculate_levenshtein(h, y, lh, ly, decoder):\n",
    "# #     # TODO: call the decoder's decode method and get beam_results and out_len (Read the docs about the decode method's outputs)\n",
    "# #     # Input to the decode method will be h and its lengths lh \n",
    "# #     # You need to pass lh for the 'seq_lens' parameter. This is not explicitly mentioned in the git repo of ctcdecode.\n",
    "\n",
    "# #     batch_size = 128\n",
    "# #     dist = 0\n",
    "# #     phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
    "# #     beam_results, beam_scores, timesteps, out_lens = decoder.decode(h)\n",
    "\n",
    "# #     for i in range(batch_size): \n",
    "# #         h_sliced = beam_results[i][0][:out_lens[i][0]]\n",
    "# #         h_string = ''.join([phoneme_map[PHONEMES[idx]] for idx in h_sliced])\n",
    "\n",
    "# #         y_sliced = y[i][:ly[i]]\n",
    "# #         y_string = ''.join([phoneme_map[PHONEMES[idx]] for idx in y_sliced])\n",
    "        \n",
    "# #         dist += Levenshtein.distance(h_string, y_string)\n",
    "\n",
    "# #     dist/=batch_size\n",
    "# #     print(dist)\n",
    "\n",
    "# #     # return dist\n",
    "\n",
    "# config = {\n",
    "#     '': '',\n",
    "#     'batch_size': 128,\n",
    "#     'epochs': 1,\n",
    "#     'scheduler': None,              # CosineAnnealingLR, ReduceLRonPlateau\n",
    "#     'optimizer': 'Adam',            # SGD, Adam, AdamW\n",
    "#     'optim': {'lr': 0.002},\n",
    "#     'decoder': {'beam_width': 5, 'cutoff_top_n': 40, 'cutoff_prob': 1.0},\n",
    "#     'arch': {'input_size': 13, 'embedding_size': [64, 128], 'hidden_size': 256, 'num_layers': 4},\n",
    "#     'save': True,\n",
    "#     'log': True,\n",
    "#     'randomize': False,\n",
    "# }\n",
    "# torch.cuda.empty_cache()\n",
    "# decoder = CTCBeamDecoder(\n",
    "#         PHONEMES,\n",
    "#         alpha=0,\n",
    "#         beta=0,\n",
    "#         cutoff_top_n=40,\n",
    "#         cutoff_prob=1.0,\n",
    "#         beam_width=5,\n",
    "#         blank_id=0,\n",
    "#         log_probs_input=True\n",
    "# )\n",
    "# model = SpeechNet(config).cuda()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "# criterion = nn.CTCLoss().cuda()\n",
    "\n",
    "# for i, data in enumerate(train_loader):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     x, y, x_len, y_len = data\n",
    "#     print(\"Iteration \", i, \"-\"*50)\n",
    "#     print(\"Input = \", x.shape, x_len.shape)\n",
    "#     print(\"Target = \", y.shape, y_len.shape)\n",
    "\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     x = x.cuda()\n",
    "#     y = y.cuda()\n",
    "#     output, output_len = model(x.cuda(), x_len)\n",
    "#     # print(output[0], output_len[0])\n",
    "#     # print(y[0], y_len[0])\n",
    "#     print(\"Output = \", output.shape, output_len.shape)\n",
    "#     loss = criterion(torch.transpose(output, 0, 1), y, output_len, y_len)\n",
    "#     print(\"Training Loss =\", loss)\n",
    "\n",
    "#     # output = torch.transpose(output, 0, 1)\n",
    "#     # print(\"Output Transposed = \", output.shape)\n",
    "#     ld = calculate_levenshtein(decoder, output, y, output_len, y_len)\n",
    "#     print(ld)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Write a test code do perform a single forward pass and also compute the Levenshtein distance\n",
    "#     # Make sure that you are able to get this right before going on to the actual training\n",
    "#     # You may encounter a lot of shape errors\n",
    "#     # Printing out the shapes will help in debugging\n",
    "#     # Keep in mind that the Loss which you will use requires the input to be in a different format and the decoder expects it in a different format\n",
    "#     # Make sure to read the corresponding docs about it\n",
    "# del model\n",
    "# # beam_results, beam_scores, timesteps, out_lens = decoder.decode(output)\n",
    "# # print(\"Decoder = \", beam_results.shape, beam_scores.shape, timesteps.shape, out_lens.shape)\n",
    "# # phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
    "# # # First datapoint in batch\n",
    "# # test_beam = beam_results[0]\n",
    "# # test_len = out_lens[0]\n",
    "\n",
    "# # print(test_beam.shape)\n",
    "# # print(test_len.shape)\n",
    "\n",
    "# # # First Beam\n",
    "# # h_sliced = test_beam[0][:test_len[0]]\n",
    "# # print(\"Top beam = \", h_sliced.shape)\n",
    "# # h_string = ''.join([phoneme_map[PHONEMES[idx]] for idx in h_sliced])\n",
    "# # print(\"Top beam mapping = \", h_string)\n",
    "# # print(\"Top beam length = \", len(h_string))\n",
    "\n",
    "# # print(\"\\n\")\n",
    "\n",
    "# # # Target Output\n",
    "# # print(y.shape)\n",
    "# # print(y_len.shape)\n",
    "# # y_sliced = y[0][:y_len[0]]\n",
    "# # print(\"Target Seq = \", y_sliced.shape)\n",
    "# # y_string = ''.join([phoneme_map[PHONEMES[idx]] for idx in y_sliced])\n",
    "# # print(\"Target Seq mapping = \", y_string)\n",
    "# # print(\"Target Seq length = \", len(y_string))\n",
    "\n",
    "# # # Score\n",
    "# # print(\"\\n\\nLevenshtein Distance = \", Levenshtein.distance(h_string, y_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcvuIBpl7jlq",
    "outputId": "7229d79f-c5c7-48bc-c589-53f41eb70c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print(output[0][0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROrqXnNqzJSc"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brfFR9m2v-8u"
   },
   "outputs": [],
   "source": [
    "class SpeechInference():\n",
    "    def __init__(self, bw=30):\n",
    "        self.phonemes = PHONEMES\n",
    "        self.phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
    "\n",
    "        self.drive_dir = r'/content/cmudrive/IDL'\n",
    "        # Dataset and dataloader\n",
    "        self.test_data = LibriSamples(partition='test')\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_data, \n",
    "            batch_size=256, \n",
    "            shuffle=False, \n",
    "            drop_last=False, \n",
    "            collate_fn = self.test_data.collate_fn\n",
    "            )\n",
    "        \n",
    "        self.decoder = CTCBeamDecoder(\n",
    "                PHONEME_MAP,\n",
    "                blank_id=0,\n",
    "                log_probs_input=True,\n",
    "                beam_width=bw, \n",
    "                cutoff_top_n= 40,\n",
    "                cutoff_prob= 1.0\n",
    "        )\n",
    "        \n",
    "    def get_predictions(self, model):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        batch_bar = tqdm(total=len(self.test_loader), position=0, leave=False, desc='Test')\n",
    "        with torch.no_grad():\n",
    "            for i, data in tqdm(enumerate(self.test_loader)):\n",
    "                x, x_len = data\n",
    "                x = x.cuda()\n",
    "\n",
    "                with torch.no_grad():    \n",
    "                    output, output_len = model(x, x_len)\n",
    "\n",
    "                beam_results, beam_scores, timesteps, out_lens = self.decoder.decode(output, seq_lens=output_len)\n",
    "                for b in range(output.shape[0]):\n",
    "                    predictions.append(convert_to_string(beam_results[b][0], out_lens[b][0], self.phoneme_map, self.phonemes))\n",
    "                \n",
    "                batch_bar.update()\n",
    "            batch_bar.close()\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def load_model(self, model_name, model_type): \n",
    "        meta_path = os.path.join(self.drive_dir,  model_type, model_name, 'model_config.yaml')\n",
    "        with open(meta_path, 'r') as meta:\n",
    "            args = yaml.safe_load(meta)\n",
    "\n",
    "        model_path = os.path.join(self.drive_dir, model_type, model_name, 'model.pth')\n",
    "        model = SpeechNet(args).cuda()\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        return model, args\n",
    "\n",
    "    def simple_inference(self, model_name, model_type):\n",
    "        print(\"Running inference...\")\n",
    "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        model, args = self.load_model(model_name, model_type)\n",
    "        preds = self.get_predictions(model)\n",
    "        \n",
    "        return preds\n",
    "\n",
    "    def ensemble_inference(self, model_names, model_type):\n",
    "        print(\"Running ensembled inference...\")\n",
    "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "        prelim_labels = []\n",
    "        for name in model_names:\n",
    "            print(\"\\n\\n\\tModel : \", name)\n",
    "            model, args = self.load_model(name, model_type)\n",
    "            prelim_labels.append(self.__get_labels(model, args))\n",
    "\n",
    "        accs = [86.146, 85.79, 84.95]\n",
    "        w = accs / np.sum(accs)\n",
    "\n",
    "        print(\"Combining predictions...\")\n",
    "        labels_df = pd.DataFrame(prelim_labels)\n",
    "        labels_df = labels_df.transpose()\n",
    "        ensembled_labels = labels_df.mode(axis=1, dropna=False).iloc[:, 0].tolist()\n",
    "        # ensembled_labels = np.where((df.iloc[:,1] == df.iloc[:, 2]), df.iloc[:, 1], df.iloc[:, 0]).tolist()\n",
    "\n",
    "        return labels_df, ensembled_labels\n",
    "\n",
    "    def generate_submission(self, save_path, preds): \n",
    "        print(\"Generating Submission CSV...\")\n",
    "        sub_dir = os.path.join(self.drive_dir, save_path + self.timestamp)\n",
    "        try:\n",
    "            os.mkdir(sub_dir)\n",
    "        except:\n",
    "            print(\"Couldn't create folder for submission.csv\")\n",
    "            \n",
    "        sub_path = os.path.join(sub_dir, 'submission.csv')\n",
    "\n",
    "        with open(sub_path, 'w') as f:\n",
    "            csvwrite = csv.writer(f)\n",
    "            csvwrite.writerow(['id', 'predictions'])\n",
    "            for i in range(len(preds)):\n",
    "                csvwrite.writerow([i, preds[i]])\n",
    "\n",
    "        print(f\"File saved at : {sub_path}\")\n",
    "        return sub_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "id": "nswfp1XMwz3a",
    "outputId": "dd48e4b5-8c51-4c4d-a6ba-22b9bd08a3fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: 100%|██████████| 2620/2620 [00:01<00:00, 2466.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "Test:   9%|▉         | 1/11 [00:12<02:01, 12.15s/it]\n",
      "Test:  18%|█▊        | 2/11 [00:27<02:05, 13.93s/it]\n",
      "Test:  27%|██▋       | 3/11 [00:45<02:07, 15.88s/it]\n",
      "Test:  36%|███▋      | 4/11 [01:01<01:51, 15.87s/it]\n",
      "Test:  45%|████▌     | 5/11 [01:18<01:38, 16.48s/it]\n",
      "Test:  55%|█████▍    | 6/11 [01:35<01:22, 16.52s/it]\n",
      "Test:  64%|██████▎   | 7/11 [01:49<01:03, 15.85s/it]\n",
      "Test:  73%|███████▎  | 8/11 [02:05<00:46, 15.63s/it]\n",
      "Test:  82%|████████▏ | 9/11 [02:21<00:31, 15.74s/it]\n",
      "Test:  91%|█████████ | 10/11 [02:40<00:16, 16.73s/it]\n",
      "Test: 100%|██████████| 11/11 [02:43<00:00, 12.72s/it]\n",
      "11it [02:43, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Submission CSV...\n",
      "File saved at : /content/cmudrive/IDL/hw3-submission/2022-04-13_16-20-47/submission.csv\n",
      "Preview of submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0b32edd7-e951-4d5d-9cfc-1d26e636d52b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>.HIbIgAnhkhnf?UzdkhmplEnthgenstDhWizrd.HUHAdvA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>.kivnatsO.rnisthmyndthDIzmhmrIzcyld.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>.hgOldhnfoRchnhndhHApIlyf.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>.HIWhzlykhpthmyfaDrinhWE.DAnd?etWhznatmyfaDr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>..olsODeRWhzhstRipliNpEj.HUtrnditthmd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>.DisWhzsOsWIthlEdIsr.AndinshmmAnrydUTiNkSIdyd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>.bhtDenDhpikcrWhzgonezkWiklIezhkEm..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>..sistrnOdU?UHIRDIzmaRvhlz.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>.tEk?UoRplEshndlethsIWhtDhkRisphlkhnSOth?U.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>.lykiznat?hNmAstr.DO.yAmhnOldmAn..</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b32edd7-e951-4d5d-9cfc-1d26e636d52b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0b32edd7-e951-4d5d-9cfc-1d26e636d52b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0b32edd7-e951-4d5d-9cfc-1d26e636d52b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   id                                        predictions\n",
       "0   0  .HIbIgAnhkhnf?UzdkhmplEnthgenstDhWizrd.HUHAdvA...\n",
       "1   1               .kivnatsO.rnisthmyndthDIzmhmrIzcyld.\n",
       "2   2                         .hgOldhnfoRchnhndhHApIlyf.\n",
       "3   3      .HIWhzlykhpthmyfaDrinhWE.DAnd?etWhznatmyfaDr.\n",
       "4   4             ..olsODeRWhzhstRipliNpEj.HUtrnditthmd.\n",
       "5   5     .DisWhzsOsWIthlEdIsr.AndinshmmAnrydUTiNkSIdyd.\n",
       "6   6               .bhtDenDhpikcrWhzgonezkWiklIezhkEm..\n",
       "7   7                        ..sistrnOdU?UHIRDIzmaRvhlz.\n",
       "8   8        .tEk?UoRplEshndlethsIWhtDhkRisphlkhnSOth?U.\n",
       "9   9                 .lykiznat?hNmAstr.DO.yAmhnOldmAn.."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'L4512-Conv2-3L-ReLU-512-4-0.4_b64_e125_sCALR_oAdamP_rLSTM_lr0.002'\n",
    "# model_name = latest_model_name\n",
    "# models = [\n",
    "#           r'full_Finale_b16384_e25_c16_bnafter_wNone_oadamp_sNone_d0.25_lr0.001-ver81',\n",
    "#           r'full_Ensemble1_b16384_e20_c16_bnafter_wNone_oadamp_sNone_d0.25_lr0.001-ver48',\n",
    "#           r'full_Ensemble2_b16384_e15_c32_bnafter_wNone_oadamp_sNone_d0.25_lr0.001-ver81'\n",
    "#           ]\n",
    "\n",
    "model_type = r'hw3-final'\n",
    "sub_path = r'hw3-submission/'\n",
    "inference = SpeechInference(bw=40)\n",
    "\n",
    "predictions = inference.simple_inference(model_name, model_type)\n",
    "submission_path = inference.generate_submission(sub_path, predictions)\n",
    "\n",
    "# Simple and Ensemble Inference\n",
    "# labels = inference.simple_inference(model_name, model_type)\n",
    "# labels_df, labels = inference.ensemble_inference(models, model_type)\n",
    "\n",
    "print(f\"Preview of submission.csv\")\n",
    "df = pd.read_csv(submission_path)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03-w9it1pXIi",
    "outputId": "a7144eeb-bb67-48c6-bc3b-006d032b3063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/cmudrive/IDL/hw3-submission/2022-04-13_16-20-47/submission.csv\n",
      "100% 215k/215k [00:05<00:00, 43.9kB/s]\n",
      "Successfully submitted to Automatic Speech Recognition (Slack)"
     ]
    }
   ],
   "source": [
    "print(submission_path)\n",
    "! kaggle competitions submit -c automatic-speech-recognition-slack -f $submission_path -m \"Finale 40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35NrnQkQj42k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "f_fwJWcpqJDR",
    "z4vZbDmJvMp1",
    "vUCKqm1ST1sU",
    "yA-6ZrNI_rSi",
    "7yLRX4iwPP90",
    "IBwunYpyugFg",
    "I2ptgaeHadVN"
   ],
   "name": "HW3P2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

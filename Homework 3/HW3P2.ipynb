{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nefario7/cmu-deeplearning/blob/working-hw4/Homework%203/HW3P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_fwJWcpqJDR"
      },
      "source": [
        "# Prelimilaries and Kaggle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbAnqEppm6MK"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output \n",
        "# NOTWORKING\n",
        "# ! apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# ! add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# ! apt-get update -qq 2>&1 > /dev/null\n",
        "# ! apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# from google.colab import auth\n",
        "# import getpass\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# auth.authenticate_user()\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "\n",
        "\n",
        "# ! google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# ! echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# WORKING\n",
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa\n",
        "!sudo apt update && sudo apt install google-drive-ocamlfuse\n",
        "!google-drive-ocamlfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfcjHRWK0ncO"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install w3m # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser \n",
        "\n",
        "% cd /content\n",
        "! mkdir cmudrive\n",
        "% cd ..\n",
        "! google-drive-ocamlfuse /content/cmudrive\n",
        "! pip install kaggle wandb torch-summary\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/cmudrive/IDL/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle \n",
        "! kaggle config set -n path -v /content\n",
        "\n",
        "! wandb login 4bdbe9c204105e1264fe3f54df2732fd1fff8040\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCQtZtkaTrcn"
      },
      "outputs": [],
      "source": [
        "!pip install python-Levenshtein\n",
        "!pip install torchsummaryX # We also install a summary package to check our model's forward before training\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "!pip install adamp\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AUGUa8tnFhM"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions download -c 11-785-s22-hw3p2\n",
        "\n",
        "! unzip -q /content/competitions/11-785-s22-hw3p2/11-785-s22-hw3p2.zip -d /content\n",
        "! mv /content/hw3p2_student_data/hw3p2_student_data /content/speech_data\n",
        "! rm -rf /content/hw3p2_student_data\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "420f0f63-94ec-436d-b839-d22e469a45be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio.transforms as taf\n",
        "from torchsummaryX import summary\n",
        "# from torchsummary import summary\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import csv\n",
        "import yaml\n",
        "import wandb\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "from phonemes import PHONEME_MAP, PHONEMES\n",
        "from adamp import AdamP\n",
        "\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "torch.autograd.profiler.profile(False)\n",
        "torch.autograd.profiler.emit_nvtx(False)\n",
        "\n",
        "import warnings\n",
        "import multiprocessing\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)\n",
        "\n",
        "def header(head):\n",
        "    print(\"-\"*80)\n",
        "    print(f\"\\t\\t\\t\\t{head.upper()}\")\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUCKqm1ST1sU"
      },
      "source": [
        "# Dataset and dataloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvUeQsFVsuim"
      },
      "outputs": [],
      "source": [
        "# from phonemes import PHONEME_MAP, PHONEMES\n",
        "# mfcc = \"/content/speech_data/train/mfcc\"\n",
        "# tran = \"/content/speech_data/train/transcript\"\n",
        "# print(len(os.listdir(mfcc)))\n",
        "# for _ in range(1):\n",
        "#     r = np.random.randint(0, 500)\n",
        "\n",
        "#     rfile = os.listdir(mfcc)[r]\n",
        "#     datax = np.load(os.path.join(mfcc, rfile))\n",
        "#     datay = np.load(os.path.join(tran, rfile))\n",
        "#     print(rfile)\n",
        "#     print(datax.shape, datay.shape)\n",
        "#     print(datay[1:-1])\n",
        "#     p = dict(zip(PHONEMES, range(len(PHONEMES))))\n",
        "#     print(p)\n",
        "#     pm = [*map(p.get, datay[1:-1])]\n",
        "#     print([torch.tensor(t) for t in pm])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path=r\"/content/speech_data\", partition=\"train\", transforms=None): # You can use partition to specify train or dev\n",
        "        partition_path = os.path.join(data_path, partition)\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.test = True if partition == \"test\" else False\n",
        "\n",
        "        self.X_dir = os.path.join(partition_path, 'mfcc')\n",
        "        self.Y_dir = os.path.join(partition_path, 'transcript')\n",
        "\n",
        "        if self.test:\n",
        "            with open(os.path.join(partition_path, 'test_order.csv'),\"r\") as f:\n",
        "                self.X_files = list(csv.reader(f))[1:]\n",
        "            self.X_data = [np.load(os.path.join(self.X_dir, xfile[0])) for xfile in tqdm(self.X_files, desc=\"Data\", position=0, leave=True)]\n",
        "        else:\n",
        "            self.X_files = os.listdir(self.X_dir)\n",
        "            self.Y_files = os.listdir(self.Y_dir)\n",
        "\n",
        "            self.X_data = [np.load(os.path.join(self.X_dir, xfile)) for xfile in tqdm(self.X_files, desc=\"Data\", position=0, leave=True)]\n",
        "            self.Y_data = [np.load(os.path.join(self.Y_dir, yfile)) for yfile in tqdm(self.Y_files, desc=\"Labels\", position=0, leave=True)]\n",
        "\n",
        "            assert(len(self.X_data) == len(self.Y_data))\n",
        "\n",
        "        self.phonemes = PHONEMES\n",
        "        self.phonemes_idx = dict(zip(PHONEMES, range(len(PHONEMES))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X_data[idx]\n",
        "        X = (X - X.mean(axis=0))/X.std(axis=0)\n",
        "        if self.test:\n",
        "            return torch.from_numpy(X)\n",
        "        else:\n",
        "            Y = self.Y_data[idx][1:-1] \n",
        "            Yy = np.array([torch.tensor(self.phonemes_idx[t]) for t in Y])\n",
        "            return torch.from_numpy(X), torch.from_numpy(Yy)\n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        if self.test:\n",
        "            batch_x = [x for x in batch]\n",
        "            batch_x_pad = pad_sequence(batch_x, batch_first=True)\n",
        "            lengths_x = [len(x) for x in batch_x]\n",
        "            return batch_x_pad, torch.tensor(lengths_x)\n",
        "        else:\n",
        "            batch_x = [x for x,_ in batch]\n",
        "            batch_y = [y for _,y in batch]\n",
        "            batch_x_pad = pad_sequence(batch_x, batch_first=True)\n",
        "            batch_y_pad = pad_sequence(batch_y, batch_first=True)\n",
        "            lengths_x = [len(x) for x in batch_x]\n",
        "            lengths_y = [len(y) for y in batch_y]\n",
        "            return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mzoYfTKu14s"
      },
      "outputs": [],
      "source": [
        "# batch_size = 64\n",
        "# train_data = LibriSamples(partition='train')\n",
        "# # val_data = LibriSamples(partition='dev')\n",
        "# # test_data = LibriSamples(partition='test')\n",
        "\n",
        "# train_loader = DataLoader(train_data, batch_size=128, shuffle=True, drop_last=True, collate_fn = train_data.collate_fn, num_workers=2)\n",
        "# # val_loader = DataLoader(val_data, batch_size=128, shuffle=False, drop_last=True, collate_fn = val_data.collate_fn, num_workers=2)\n",
        "# # test_loader = DataLoader(test_data, batch_size=128, shuffle=False, drop_last=False, collate_fn = test_data.collate_fn)\n",
        "\n",
        "# print(\"Batch size: \", batch_size)\n",
        "# print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "# # print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "# # print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
        "\n",
        "# # Test code for checking shapes and return arguments of the train and val loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NzQ9GPf1wOy"
      },
      "outputs": [],
      "source": [
        "# for data in train_loader:\n",
        "#     x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
        "#     print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "#     print(x[0], lx[0])\n",
        "#     print(y[0], ly[0])\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA-6ZrNI_rSi"
      },
      "source": [
        "### ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FgP7eJD8fdh"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.conv import Conv1d\n",
        "torch.cuda.empty_cache()\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, identity=None):\n",
        "        super(Block, self).__init__()\n",
        "        self.expansion = 2\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.conv3 = nn.Conv1d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm1d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.identity_downsample = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm1d(out_channels * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        identity = self.identity_downsample(identity)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        \n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ResNet(nn.Module): # [3, 4, 6]\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 13\n",
        "        self.out_channels = 64\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(self.in_channels, 64, kernel_size=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.resnet1 = self._make_layers(num_blocks=1, out_channels=64, stride=1)\n",
        "        self.resnet2 = self._make_layers(num_blocks=1, out_channels=128, stride=2)\n",
        "        self.resnet3 = self._make_layers(num_blocks=1, out_channels=256, stride=2)\n",
        "\n",
        "    def _make_layers(self, num_blocks, out_channels, stride):\n",
        "        identity_down = None\n",
        "        layers = []\n",
        "        if stride != 1 or self.in_channels != out_channels * 2:\n",
        "            identity_down = nn.Sequential(\n",
        "                nn.Conv1d(self.in_channels, out_channels * 2, kernel_size=1),\n",
        "                nn.BatchNorm1d(out_channels * 2)\n",
        "            )\n",
        "\n",
        "        layers.append(Block(self.in_channels, out_channels, stride, identity_down))\n",
        "        self.in_channels = out_channels*2\n",
        "\n",
        "        for i in range(num_blocks - 1):\n",
        "            layers.append(Block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.resnet1(x)\n",
        "        x = self.resnet2(x)\n",
        "        x = self.resnet3(x)\n",
        "\n",
        "        return x "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yLRX4iwPP90"
      },
      "source": [
        "### Arch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGoiXd70tb5z"
      },
      "outputs": [],
      "source": [
        "# class SpeechNet(nn.Module):\n",
        "#     def __init__(self, config): # You can add any extra arguments as you wish\n",
        "#         super().__init__()\n",
        "#         lstm_params = config['arch']\n",
        "#         vocab_size = len(PHONEMES)\n",
        "\n",
        "#         self.stride = 1\n",
        "\n",
        "#         # Embedding\n",
        "#         embeddings = [13] + lstm_params['embedding_size']\n",
        "#         embedding_layers = []\n",
        "#         for i in range(len(embeddings) - 1):\n",
        "#             embedding_layers.append(nn.Conv1d(embeddings[i], embeddings[i+1], kernel_size=3, padding=1))\n",
        "#             embedding_layers.append(nn.BatchNorm1d(embeddings[i+1]))\n",
        "#             embedding_layers.append(nn.ReLU())\n",
        "#             # Maybe add a Pooling\n",
        "#         self.embeddings = nn.Sequential(*embedding_layers)\n",
        "#         embedding_size = embeddings[-1]\n",
        "\n",
        "#         # Block Embedding\n",
        "#         # self.embeddings = nn.Sequential(\n",
        "#         #     # nn.Conv1d(13, 64, kernel_size=5, stride=2, padding=2),\n",
        "#         #     # nn.BatchNorm1d(64),\n",
        "#         #     # nn.ReLU(),\n",
        "#         #     # nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
        "#         #     nn.Conv1d(64, 64, kernel_size=1, stride=1, padding=0),\n",
        "#         #     nn.BatchNorm1d(64),\n",
        "#         #     nn.Conv1d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "#         #     nn.BatchNorm1d(64),\n",
        "#         #     nn.Conv1d(64, 256, kernel_size=1, stride=1, padding=0),\n",
        "#         #     nn.BatchNorm1d(256),\n",
        "#         #     nn.ReLU()\n",
        "#         # )\n",
        "#         # embedding_size = 256\n",
        "\n",
        "#         # ResNet Embedding\n",
        "#         # self.embeddings = ResNet(layers=[1])\n",
        "#         # embedding_size = 256\n",
        "\n",
        "#         # RNN Block\n",
        "#         if config['rnn'] == 'GRU':\n",
        "#             self.lstm = nn.GRU(\n",
        "#                 input_size=embedding_size,\n",
        "#                 hidden_size=lstm_params['hidden_size'],\n",
        "#                 num_layers=lstm_params['num_layers'],\n",
        "#                 dropout = lstm_params['dropout'],\n",
        "#                 bidirectional=True,\n",
        "#                 batch_first=True\n",
        "#             )\n",
        "#         else:\n",
        "#             self.lstm = nn.LSTM(\n",
        "#                 input_size=embedding_size,\n",
        "#                 hidden_size=lstm_params['hidden_size'],\n",
        "#                 num_layers=lstm_params['num_layers'],\n",
        "#                 dropout = lstm_params['dropout'],\n",
        "#                 bidirectional=True,\n",
        "#                 batch_first=True\n",
        "#                 )\n",
        "\n",
        "#         # Classification\n",
        "#         self.classification = nn.Sequential(\n",
        "#             nn.Linear(lstm_params['hidden_size'] * 2, 2048),\n",
        "#             nn.Dropout(p=0.2),\n",
        "#             nn.Linear(2048, 1024),\n",
        "#             nn.Dropout(p=0.2),\n",
        "#             nn.Linear(1024, vocab_size)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, x_len): \n",
        "#         x = torch.transpose(self.embeddings(torch.transpose(x, 1, 2)), 1, 2)\n",
        "#         # x_len = torch.clamp(x_len, min=0, max=x.shape[1])\n",
        "#         packed_input = pack_padded_sequence(x, x_len, enforce_sorted=False, batch_first=True)\n",
        "#         out, _ = self.lstm(packed_input)\n",
        "#         out, lengths  = pad_packed_sequence(out, batch_first=True)\n",
        "#         out = self.classification(out)\n",
        "#         out = F.log_softmax(out, dim=2)\n",
        "#         return out, lengths\n",
        "\n",
        "#     def initialize_weights(self):\n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Conv1d):\n",
        "#                 nn.init.trunc_normal_(m.weight, std=0.2)\n",
        "#             elif isinstance(m, nn.BatchNorm2d):\n",
        "#                 nn.init.constant_(m.weight, 1)\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#             elif isinstance(m, nn.Linear):\n",
        "#                 nn.init.trunc_normal_(m.weight, std=0.2)\n",
        "#                 nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLjmHc4mMyB4"
      },
      "outputs": [],
      "source": [
        "# config = {\n",
        "#     '': 'LSTM-4layer-medium',\n",
        "#     'batch_size': 128,\n",
        "#     'epochs': 100,\n",
        "#     'scheduler': 'CALR',             # CosineAnnealingLR (CALR), ReduceLRonPlateau (RLRP)\n",
        "#     'optimizer': 'AdamP',            # SGD, Adam, AdamW, AdamP\n",
        "#     'rnn': 'LSTM',                   # GRU, LSTM\n",
        "#     'optim': {'lr': 0.002},\n",
        "#     'decoder': {'beam_width': 3, 'cutoff_top_n': 40, 'cutoff_prob': 1.0},\n",
        "#     'arch': {'embedding_size': [64, 256], 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.25},\n",
        "#     'save': True,\n",
        "#     'log': True,\n",
        "#     'randomize': False,\n",
        "# }\n",
        "# model = SpeechNet(config).cuda()\n",
        "# print(model)\n",
        "# # out, out_lengths = model(x.cuda(), lx)\n",
        "# # print(out.shape, out_lengths.shape)\n",
        "# # print(y.shape, ly.shape)\n",
        "# summary(model, x.cuda(), lx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC78fom2_tfD"
      },
      "source": [
        "### ASRNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc2fblIK_vKS"
      },
      "outputs": [],
      "source": [
        "class SpeechNet(nn.Module):\n",
        "    def __init__(self, config): # You can add any extra arguments as you wish\n",
        "        super().__init__()\n",
        "        lstm_params = config['arch']\n",
        "        vocab_size = len(PHONEMES)\n",
        "\n",
        "        self.stride = 1\n",
        "\n",
        "        # Embedding\n",
        "        # self.skip = lstm_params['skip_connect']\n",
        "        # embeddings = lstm_params['embedding_size']\n",
        "        # embedding_layers = []\n",
        "        # for i in range(len(embeddings) - 1):\n",
        "        #     conv_params = lstm_params['embedding_args'][i]\n",
        "        #     embedding_layers.append(nn.Conv1d(embeddings[i], embeddings[i+1], **conv_params))\n",
        "        #     embedding_layers.append(nn.BatchNorm1d(embeddings[i+1]))\n",
        "        #     embedding_layers.append(nn.Dropout(0.15))\n",
        "        #     if self.skip:\n",
        "        #         if i != len(embeddings) - 2:\n",
        "        #             embedding_layers.append(nn.ReLU())\n",
        "        #     else:\n",
        "        #         embedding_layers.append(nn.ReLU())\n",
        "        #     # Maybe add a Pooling\n",
        "        # self.embeddings = nn.Sequential(*embedding_layers)\n",
        "        # embedding_size = embeddings[-1]\n",
        "\n",
        "        # if self.skip:\n",
        "        #     self.residual = nn.Sequential(\n",
        "        #         nn.Conv1d(embeddings[0], embeddings[-1], **lstm_params['embedding_args'][1]),\n",
        "        #         nn.BatchNorm1d(embeddings[-1])\n",
        "        #     )\n",
        "        #     self.relu = nn.ReLU()\n",
        "\n",
        "        # Block Embedding\n",
        "        self.embeddings = nn.Sequential(\n",
        "            nn.Conv1d(13, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding='same'),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding='same'),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        embedding_size = 512\n",
        "\n",
        "        # ResNet Embedding\n",
        "        # self.embeddings = ResNet().cuda()\n",
        "        # embedding_size = 512\n",
        "\n",
        "        # RNN Block\n",
        "        if config['rnn'] == 'GRU':\n",
        "            self.lstm = nn.GRU(\n",
        "                input_size=embedding_size,\n",
        "                hidden_size=lstm_params['hidden_size'],\n",
        "                num_layers=lstm_params['num_layers'],\n",
        "                dropout = lstm_params['dropout'],\n",
        "                bidirectional=True,\n",
        "                batch_first=True\n",
        "            )\n",
        "        else:\n",
        "            self.lstm = nn.LSTM(\n",
        "                input_size=embedding_size,\n",
        "                hidden_size=lstm_params['hidden_size'],\n",
        "                num_layers=lstm_params['num_layers'],\n",
        "                dropout = lstm_params['dropout'],\n",
        "                bidirectional=True,\n",
        "                batch_first=True\n",
        "                )\n",
        "\n",
        "        # Classification\n",
        "        self.classification = nn.Sequential(\n",
        "            nn.Linear(lstm_params['hidden_size'] * 2, 2048),\n",
        "            nn.Dropout(p=0.35),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, vocab_size)\n",
        "        )\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def forward(self, x, x_len): \n",
        "        x = torch.transpose(x, 1, 2)\n",
        "        # if self.skip:\n",
        "        #     residual = self.residual(x)\n",
        "        x = self.embeddings(x)\n",
        "        # if self.skip:\n",
        "        #     x = x + residual\n",
        "        #     x = self.relu(x)\n",
        "            \n",
        "        x = torch.transpose(x, 1, 2)\n",
        "        x_len = x_len // 2\n",
        "\n",
        "        packed_input = pack_padded_sequence(x, x_len, enforce_sorted=False, batch_first=True)\n",
        "        out, _ = self.lstm(packed_input)\n",
        "        out, lengths  = pad_packed_sequence(out, batch_first=True)\n",
        "        out = self.classification(out)\n",
        "        out = F.log_softmax(out, dim=2)\n",
        "        return out, lengths\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.2)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.2)\n",
        "                nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()\n",
        "config = {\n",
        "    '': '',\n",
        "    'batch_size': 64,\n",
        "    'epochs': 100,\n",
        "    'scheduler': 'CALR',             # CosineAnnealingLR (CALR), ReduceLRonPlateau (RLRP)\n",
        "    'optimizer': 'AdamP',            # SGD, Adam, AdamW, AdamP\n",
        "    'rnn': 'LSTM',                   # GRU, LSTM\n",
        "    'optim': {'lr': 0.042},\n",
        "    'decoder': {'beam_width': 3, 'cutoff_top_n': 40, 'cutoff_prob': 1.0},\n",
        "    'arch': {'embedding_size': [64, 256], 'hidden_size': 512, 'num_layers': 4, 'dropout': 0.25},\n",
        "    'save': True,\n",
        "    'log': True,\n",
        "    'randomize': False,\n",
        "}\n",
        "# model = SpeechNet(config).cuda()\n",
        "# # print(model)\n",
        "# # out, out_lengths = model(x.cuda(), lx)\n",
        "# # print(out.shape, out_lengths.shape)\n",
        "# # print(y.shape, ly.shape)\n",
        "# summary(model, x.cuda(), lx)"
      ],
      "metadata": {
        "id": "bR_w80unSrU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27XuR3frZi79"
      },
      "outputs": [],
      "source": [
        "def convert_to_string(tokens, seq_len, phoneme_map, phonemes):\n",
        "    return \"\".join([phoneme_map[phonemes[x]] for x in tokens[0:seq_len]])\n",
        "\n",
        "def calculate_levenshtein(decoder, h, y, lh, ly):\n",
        "    phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
        "    batch_size = ly.shape[0]\n",
        "    dist = 0\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(h, seq_lens = lh)\n",
        "\n",
        "    for i in range(batch_size): # Loop through each element in the batch\n",
        "        h_string = convert_to_string(beam_results[i][0], out_lens[i][0], phoneme_map, PHONEMES)\n",
        "        y_string = convert_to_string(y[i], ly[i], phoneme_map, PHONEMES)\n",
        "        dist += Levenshtein.distance(h_string, y_string)\n",
        "    dist/=batch_size\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzwsqG_eqKMp"
      },
      "outputs": [],
      "source": [
        "class ModelSetup:\n",
        "    def __init__(self, config, save_path):\n",
        "        self.config = config\n",
        "        self.log = config['log']\n",
        "        self.save = config['save']\n",
        "        print(f\"Saving : {self.save} and Logging : {self.log}\")\n",
        "\n",
        "        self.phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
        "        self.phonemes_ctc = PHONEME_MAP\n",
        "\n",
        "        self.SAVE_DIR = save_path\n",
        "        self.DATA_DIR = r\"/content/speech_data\" \n",
        "\n",
        "    def __gen_model_name(self):\n",
        "        # Generate a model name based on config\n",
        "        save_name = ''\n",
        "\n",
        "        for key, val in self.config.items():\n",
        "            abbr = key[0] if len(key) > 2 else key\n",
        "            if isinstance(val, dict):\n",
        "                data = 'lr' + str(val[\"lr\"])\n",
        "                save_name += data\n",
        "                break\n",
        "            elif key == '':\n",
        "                save_name += abbr + str(val)\n",
        "                for key, val in self.config['arch'].items():\n",
        "                    save_name += '-' + str(val)\n",
        "                save_name += '_'\n",
        "            else:\n",
        "                data = abbr + str(val) + '_'\n",
        "                save_name += data\n",
        "                \n",
        "\n",
        "        if self.config['randomize']:\n",
        "            save_name = save_name + \"-v\" + str(np.random.randint(10, 1000))\n",
        "        print(\"\\nModel Name: \", save_name)\n",
        "        self.model_name = save_name\n",
        "\n",
        "    def __save_model_params(self, continue_train):\n",
        "        # Create Model Directory\n",
        "        save_path = os.path.join(self.SAVE_DIR, self.model_name)\n",
        "        if not continue_train:\n",
        "            try:\n",
        "                os.mkdir(save_path)\n",
        "            except FileExistsError:\n",
        "                d = input(\"Model name already exists. Delete existing model? (y/n)\")\n",
        "                if d == 'y':\n",
        "                    import shutil\n",
        "                    shutil.rmtree(save_path)\n",
        "                    os.mkdir(save_path)\n",
        "                else:\n",
        "                    print(\"Exiting!\")\n",
        "                    exit(0)\n",
        "                    return None\n",
        "\n",
        "            os.mkdir(os.path.join(save_path, 'Checkpoints'))\n",
        "            # Saving Model Configuration\n",
        "            with open(os.path.join(save_path, 'model_config.yaml'), 'w') as metadata:\n",
        "                yaml.dump({'Experiment': self.config['']}, metadata, indent=4, default_flow_style=False)\n",
        "                yaml.dump(self.config, metadata, indent=4, default_flow_style=False)\n",
        "            print(\"Model to be saved at: \", save_path)\n",
        "        self.model_path = save_path\n",
        "\n",
        "    def dataloaders(self): \n",
        "        # self.train_transforms = [\n",
        "        #                          taf.TimeMasking(10, p=0.25),\n",
        "        #                          taf.FrequencyMasking(1)\n",
        "\n",
        "        # ]\n",
        "        # self.val_transforms = []\n",
        "        self.train_data = LibriSamples(partition='train')\n",
        "        self.val_data = LibriSamples(partition='dev')\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_data, \n",
        "            batch_size=self.config['batch_size'], \n",
        "            shuffle=True, \n",
        "            drop_last=True, \n",
        "            collate_fn = self.train_data.collate_fn, \n",
        "            num_workers=4,\n",
        "            pin_memory=True) \n",
        "        self.val_loader = DataLoader(\n",
        "            self.val_data, \n",
        "            batch_size=self.config['batch_size'], \n",
        "            shuffle=False, \n",
        "            drop_last=True, \n",
        "            collate_fn = self.val_data.collate_fn, \n",
        "            num_workers=4,\n",
        "            pin_memory=True)\n",
        "\n",
        "    def save_checkpoint(self, epoch, model, optimizer, loss):\n",
        "        print(\"Saving Checkpoint!\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(epoch) + '.pth'))\n",
        "\n",
        "    def save_model(self, epoch=None, onnx=False):\n",
        "        print(\"Saving Model!\")\n",
        "        try:\n",
        "            if not self.save:\n",
        "                self.__save_model_params()\n",
        "            if epoch is None:\n",
        "                name = os.path.join(self.model_path, \"model_\" + str(epoch) + \".pth\")\n",
        "            else:\n",
        "                name = os.path.join(self.model_path, \"model\" + \".pth\")\n",
        "            torch.save(self.model.state_dict(), name)\n",
        "            if onnx:\n",
        "                torch.onnx.export(self.model, name.split('.')[0] + '.onnx')\n",
        "                wandb.save(name.split('.')[0] + '.onnx')\n",
        "        except:\n",
        "            print(\"Model couldn't be saved!\")\n",
        "\n",
        "    def setup(self, continue_train=False, chkpt=None):\n",
        "        header(\"Model Setup\")\n",
        "\n",
        "        # Model\n",
        "        self.model = SpeechNet(self.config).cuda()\n",
        "        summary(self.model, torch.randn(128, 1692, 13).cuda(), torch.tensor([128]))\n",
        "        self.__gen_model_name()\n",
        "        if self.save: self.__save_model_params(continue_train)\n",
        "\n",
        "        # Loss\n",
        "        self.criterion = nn.CTCLoss()\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = CTCBeamDecoder(\n",
        "                self.phonemes_ctc,\n",
        "                blank_id=0,\n",
        "                log_probs_input=True,\n",
        "                **self.config['decoder']\n",
        "        )\n",
        "        \n",
        "        # Optimizer\n",
        "        if self.config[\"optimizer\"] == 'SGD':\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), **self.config['optim'])\n",
        "        elif self.config[\"optimizer\"] == \"Adam\":\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), **self.config['optim'])\n",
        "        elif self.config[\"optimizer\"] == \"AdamW\":\n",
        "            self.optimizer = optim.AdamW(self.model.parameters(), **self.config['optim'])\n",
        "        elif self.config[\"optimizer\"] == \"AdamP\":\n",
        "            self.optimizer = AdamP(self.model.parameters(), **self.config['optim'])\n",
        "\n",
        "        self.chkpt = 0\n",
        "        if continue_train:\n",
        "            self.chkpt = chkpt\n",
        "            assert chkpt is not None\n",
        "\n",
        "            chkpt_path = os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(chkpt) + '.pth')\n",
        "            try:\n",
        "                checkpoint = torch.load(chkpt_path)\n",
        "            except FileNotFoundError:\n",
        "                print(\"Checkpoint not found in the directory!\")\n",
        "                print(\"Incorrect: \", chkpt_path)\n",
        "                exit(0)\n",
        "\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            # self.optimizer.param_groups[0]['lr'] = 0.001\n",
        "            self.optimizer.param_groups[0]['lr'] = 0.001 / 2\n",
        "\n",
        "        # Scheduler\n",
        "        if self.config[\"scheduler\"] == 'CALR':\n",
        "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=(len(self.train_loader) * self.config['epochs']))\n",
        "        elif self.config[\"scheduler\"] == 'RLRP':\n",
        "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                self.optimizer, \n",
        "                mode='min', \n",
        "                factor=0.5, \n",
        "                patience=1,\n",
        "                threshold=0.15,\n",
        "                min_lr=0.0000625,\n",
        "                cooldown=3)\n",
        "        elif self.config[\"scheduler\"] == \"MultiStep\":\n",
        "             self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[12, 25, 35, 45], gamma=0.5, verbose=True)\n",
        "        elif self.config[\"scheduler\"] == None:\n",
        "            pass\n",
        "\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    def train(self, continue_train=False, save_freq=2):\n",
        "        header(\"Training\")\n",
        "        epochs = self.config['epochs']\n",
        "        batch_size = self.config['batch_size']\n",
        "\n",
        "        if self.log:\n",
        "            wandb.init(project=\"hw3-chinmay\", entity=\"dl-study-group\", config=self.config, name=self.config[''])\n",
        "            wandb.watch(self.model, criterion=self.criterion, log=\"all\", log_freq=batch_size, idx=None)\n",
        "\n",
        "        delta_time = datetime.timedelta(seconds = 0)\n",
        "        for epoch in range(self.chkpt, epochs + 50):\n",
        "            start_time = time.time()\n",
        "\n",
        "            self.model.train()\n",
        "            print(\"\\nEpoch-\", epoch + 1)\n",
        "            batch_bar = tqdm(total=len(self.train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "            total_loss = 0\n",
        "\n",
        "            for i, data in enumerate(self.train_loader, 0):\n",
        "                torch.cuda.empty_cache()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                x, y, x_len, y_len = data\n",
        "                x = x.cuda()\n",
        "                y = y.cuda()\n",
        "\n",
        "                with torch.cuda.amp.autocast():    \n",
        "                    output, output_len = self.model(x, x_len)    # B x T x 41\n",
        "                    output_transposed = torch.transpose(output, 0, 1)              # T x B x 41\n",
        "                    loss = self.criterion(output_transposed, y, output_len, y_len)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                batch_bar.set_postfix(\n",
        "                    loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "                    lr=\"{:.04f}\".format(float(self.optimizer.param_groups[0]['lr']))\n",
        "                    )\n",
        "                self.scaler.scale(loss).backward() \n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                if self.config[\"scheduler\"] == 'CALR': self.scheduler.step()\n",
        "                batch_bar.update()\n",
        "  \n",
        "            batch_bar.close()\n",
        "            trainlos = float(total_loss / len(self.train_loader))\n",
        "            trainlra = float(self.optimizer.param_groups[0]['lr'])\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} | Train Loss {trainlos:.04f} | Learning Rate {trainlra:.04f}\")\n",
        "\n",
        "            if self.log:\n",
        "                    wandb.log({\"Training Loss\": trainlos, \"Learning Rate\": trainlra})\n",
        "                    \n",
        "            lev = self.validate()\n",
        "            if self.config[\"scheduler\"] == 'RLRP': self.scheduler.step(lev)\n",
        "\n",
        "            delta_time += datetime.timedelta(seconds = (time.time() - start_time))\n",
        "            time_lapsed = delta_time\n",
        "            time_left = delta_time * (epochs - epoch - 1) / (epoch + 1)\n",
        "            print(f\"Time lapsed = {str(time_lapsed)}\")\n",
        "            print(f\"Time left = {str(time_left)}\")\n",
        "\n",
        "            if self.save:\n",
        "                # Save Model\n",
        "                if epoch % save_freq == 0: self.save_model(epoch)\n",
        "                # Save Checkpoint\n",
        "                self.save_checkpoint(epoch, self.model, self.optimizer, total_loss / len(self.train_loader))\n",
        "        \n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        batch_bar = tqdm(total=len(self.val_loader), position=0, leave=False, desc='Val')\n",
        "        total_levenshtein = 0\n",
        "        for i, data in enumerate(self.val_loader, 0):\n",
        "            x, y, x_len, y_len = data\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            with torch.no_grad():    \n",
        "                output, output_len = self.model(x, x_len)\n",
        "\n",
        "            total_levenshtein += calculate_levenshtein(self.decoder, output, y, output_len, y_len)\n",
        "            \n",
        "            batch_bar.set_postfix(LD=\"{:.04f}\".format(float(total_levenshtein / (i + 1))))\n",
        "            batch_bar.update()\n",
        "\n",
        "        batch_bar.close()\n",
        "        val_lev = float(total_levenshtein / len(self.val_loader))\n",
        "        print(\"\\nValidation LD: {:.04f}\".format(val_lev))\n",
        "        if self.log:\n",
        "            wandb.log({\"Validation LD\": val_lev})\n",
        "\n",
        "        return val_lev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72yaABA_w8kt"
      },
      "source": [
        "# Hyperparameters and Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSW09tgXxERy"
      },
      "outputs": [],
      "source": [
        "# config = {\n",
        "#     '': 'L4512-Conv2-L2-ReLU-init',\n",
        "#     'batch_size': 64,\n",
        "#     'epochs': 125,\n",
        "#     'scheduler': 'RLRP',             # CosineAnnealingLR (CALR), ReduceLRonPlateau (RLRP), MultiStep\n",
        "#     'optimizer': 'AdamP',            # SGD, Adam, AdamW, AdamP\n",
        "#     'rnn': 'LSTM',                   # GRU, LSTM\n",
        "#     'optim': {'lr': 0.002},\n",
        "#     'decoder': {'beam_width': 3, 'cutoff_top_n': 40, 'cutoff_prob': 1.0},\n",
        "#     'arch': {\n",
        "#         # 'embedding_size': [13, 32, 64, 128, 256], \n",
        "#         # 'embedding_args': [\n",
        "#         #                    {'kernel_size':3, 'stride':1, 'padding':'same'},\n",
        "#         #                    {'kernel_size':3, 'stride':1, 'padding':'same'},\n",
        "#         #                    {'kernel_size':3, 'stride':1, 'padding':'same'},\n",
        "#         #                    {'kernel_size':3, 'stride':1, 'padding':'same'}\n",
        "#         #                    ], \n",
        "#         # 'skip_connect': False,\n",
        "#         'hidden_size': 512, \n",
        "#         'num_layers': 4, \n",
        "#         'dropout':0.5},\n",
        "#     'save': True,\n",
        "#     'log': True,\n",
        "#     'randomize': False,\n",
        "# }\n",
        "config = {\n",
        "    '': 'AWS_L4512-Conv3-L2-ReLU',\n",
        "    'batch_size': 64,\n",
        "    'epochs': 100,\n",
        "    'scheduler': 'CALR',             # CosineAnnealingLR (CALR), ReduceLRonPlateau (RLRP)\n",
        "    'optimizer': 'AdamP',            # SGD, Adam, AdamW, AdamP\n",
        "    'rnn': 'LSTM',                   # GRU, LSTM\n",
        "    'optim': {'lr': 0.002},\n",
        "    'decoder': {'beam_width': 3, 'cutoff_top_n': 40, 'cutoff_prob': 1.0},\n",
        "    'arch': {\n",
        "        # 'embedding_size': [13, 32, 64, 128, 256],\n",
        "        # 'embedding_args': [\n",
        "        #                    {'kernel_size':3, 'stride':1, 'padding':'same'},\n",
        "        #                    {'kernel_size':3, 'stride':1, 'padding':'same'},\n",
        "        #                    {'kernel_size':3, 'stride':1, 'padding':'same'},\n",
        "        #                    {'kernel_size':3, 'stride':1, 'padding':'same'}\n",
        "        #                    ], \n",
        "        # 'skip_connect': False,\n",
        "        'hidden_size': 512, \n",
        "        'num_layers': 4, \n",
        "        'dropout':0.5},\n",
        "    'save': True,\n",
        "    'log': True,\n",
        "    'randomize': False,\n",
        "}\n",
        "\n",
        "# SpeechNet\n",
        "folder_path = r'/content/cmudrive/IDL/hw3-final'\n",
        "asr = ModelSetup(config, save_path = folder_path)\n",
        "asr.dataloaders()\n",
        "\n",
        "# # Continue Training\n",
        "torch.cuda.empty_cache()\n",
        "asr.setup(continue_train=True, chkpt=17)\n",
        "asr.train(continue_train=True)\n",
        "asr.save_model()\n",
        "if asr.log: wandb.finish()\n",
        "latest_model_name = asr.model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJgAbnp5W2vL"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "asr.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG4F77Nm0Am9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "asr.train()\n",
        "asr.save_model()\n",
        "\n",
        "if asr.log: wandb.finish()\n",
        "\n",
        "latest_model_name = asr.model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2ptgaeHadVN"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMSwlCIDacCP"
      },
      "outputs": [],
      "source": [
        "# # def calculate_levenshtein(h, y, lh, ly, decoder):\n",
        "# #     # TODO: call the decoder's decode method and get beam_results and out_len (Read the docs about the decode method's outputs)\n",
        "# #     # Input to the decode method will be h and its lengths lh \n",
        "# #     # You need to pass lh for the 'seq_lens' parameter. This is not explicitly mentioned in the git repo of ctcdecode.\n",
        "\n",
        "# #     batch_size = 128\n",
        "# #     dist = 0\n",
        "# #     phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
        "# #     beam_results, beam_scores, timesteps, out_lens = decoder.decode(h)\n",
        "\n",
        "# #     for i in range(batch_size): \n",
        "# #         h_sliced = beam_results[i][0][:out_lens[i][0]]\n",
        "# #         h_string = ''.join([phoneme_map[PHONEMES[idx]] for idx in h_sliced])\n",
        "\n",
        "# #         y_sliced = y[i][:ly[i]]\n",
        "# #         y_string = ''.join([phoneme_map[PHONEMES[idx]] for idx in y_sliced])\n",
        "        \n",
        "# #         dist += Levenshtein.distance(h_string, y_string)\n",
        "\n",
        "# #     dist/=batch_size\n",
        "# #     print(dist)\n",
        "\n",
        "# #     # return dist\n",
        "\n",
        "# config = {\n",
        "#     '': '',\n",
        "#     'batch_size': 128,\n",
        "#     'epochs': 1,\n",
        "#     'scheduler': None,              # CosineAnnealingLR, ReduceLRonPlateau\n",
        "#     'optimizer': 'Adam',            # SGD, Adam, AdamW\n",
        "#     'optim': {'lr': 0.002},\n",
        "#     'decoder': {'beam_width': 5, 'cutoff_top_n': 40, 'cutoff_prob': 1.0},\n",
        "#     'arch': {'input_size': 13, 'embedding_size': [64, 128], 'hidden_size': 256, 'num_layers': 4},\n",
        "#     'save': True,\n",
        "#     'log': True,\n",
        "#     'randomize': False,\n",
        "# }\n",
        "# torch.cuda.empty_cache()\n",
        "# decoder = CTCBeamDecoder(\n",
        "#         PHONEMES,\n",
        "#         alpha=0,\n",
        "#         beta=0,\n",
        "#         cutoff_top_n=40,\n",
        "#         cutoff_prob=1.0,\n",
        "#         beam_width=5,\n",
        "#         blank_id=0,\n",
        "#         log_probs_input=True\n",
        "# )\n",
        "# model = SpeechNet(config).cuda()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
        "# criterion = nn.CTCLoss().cuda()\n",
        "\n",
        "# for i, data in enumerate(train_loader):\n",
        "#     torch.cuda.empty_cache()\n",
        "#     x, y, x_len, y_len = data\n",
        "#     print(\"Iteration \", i, \"-\"*50)\n",
        "#     print(\"Input = \", x.shape, x_len.shape)\n",
        "#     print(\"Target = \", y.shape, y_len.shape)\n",
        "\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     x = x.cuda()\n",
        "#     y = y.cuda()\n",
        "#     output, output_len = model(x.cuda(), x_len)\n",
        "#     # print(output[0], output_len[0])\n",
        "#     # print(y[0], y_len[0])\n",
        "#     print(\"Output = \", output.shape, output_len.shape)\n",
        "#     loss = criterion(torch.transpose(output, 0, 1), y, output_len, y_len)\n",
        "#     print(\"Training Loss =\", loss)\n",
        "\n",
        "#     # output = torch.transpose(output, 0, 1)\n",
        "#     # print(\"Output Transposed = \", output.shape)\n",
        "#     ld = calculate_levenshtein(decoder, output, y, output_len, y_len)\n",
        "#     print(ld)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "#     # Write a test code do perform a single forward pass and also compute the Levenshtein distance\n",
        "#     # Make sure that you are able to get this right before going on to the actual training\n",
        "#     # You may encounter a lot of shape errors\n",
        "#     # Printing out the shapes will help in debugging\n",
        "#     # Keep in mind that the Loss which you will use requires the input to be in a different format and the decoder expects it in a different format\n",
        "#     # Make sure to read the corresponding docs about it\n",
        "# del model\n",
        "# # beam_results, beam_scores, timesteps, out_lens = decoder.decode(output)\n",
        "# # print(\"Decoder = \", beam_results.shape, beam_scores.shape, timesteps.shape, out_lens.shape)\n",
        "# # phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
        "# # # First datapoint in batch\n",
        "# # test_beam = beam_results[0]\n",
        "# # test_len = out_lens[0]\n",
        "\n",
        "# # print(test_beam.shape)\n",
        "# # print(test_len.shape)\n",
        "\n",
        "# # # First Beam\n",
        "# # h_sliced = test_beam[0][:test_len[0]]\n",
        "# # print(\"Top beam = \", h_sliced.shape)\n",
        "# # h_string = ''.join([phoneme_map[PHONEMES[idx]] for idx in h_sliced])\n",
        "# # print(\"Top beam mapping = \", h_string)\n",
        "# # print(\"Top beam length = \", len(h_string))\n",
        "\n",
        "# # print(\"\\n\")\n",
        "\n",
        "# # # Target Output\n",
        "# # print(y.shape)\n",
        "# # print(y_len.shape)\n",
        "# # y_sliced = y[0][:y_len[0]]\n",
        "# # print(\"Target Seq = \", y_sliced.shape)\n",
        "# # y_string = ''.join([phoneme_map[PHONEMES[idx]] for idx in y_sliced])\n",
        "# # print(\"Target Seq mapping = \", y_string)\n",
        "# # print(\"Target Seq length = \", len(y_string))\n",
        "\n",
        "# # # Score\n",
        "# # print(\"\\n\\nLevenshtein Distance = \", Levenshtein.distance(h_string, y_string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcvuIBpl7jlq",
        "outputId": "7229d79f-c5c7-48bc-c589-53f41eb70c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# print(output[0][0].sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROrqXnNqzJSc"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brfFR9m2v-8u"
      },
      "outputs": [],
      "source": [
        "class SpeechInference():\n",
        "    def __init__(self, bw=30):\n",
        "        self.phonemes = PHONEMES\n",
        "        self.phoneme_map = dict(zip(PHONEMES, PHONEME_MAP))\n",
        "\n",
        "        self.drive_dir = r'/content/cmudrive/IDL'\n",
        "        # Dataset and dataloader\n",
        "        self.test_data = LibriSamples(partition='test')\n",
        "        self.test_loader = DataLoader(\n",
        "            self.test_data, \n",
        "            batch_size=64, \n",
        "            shuffle=False, \n",
        "            drop_last=False, \n",
        "            collate_fn = self.test_data.collate_fn\n",
        "            )\n",
        "        \n",
        "        self.decoder = CTCBeamDecoder(\n",
        "                PHONEME_MAP,\n",
        "                blank_id=0,\n",
        "                log_probs_input=True,\n",
        "                beam_width=bw, \n",
        "                cutoff_top_n= 40,\n",
        "                cutoff_prob= 1.0\n",
        "        )\n",
        "        \n",
        "    def get_predictions(self, model):\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        batch_bar = tqdm(total=len(self.test_loader), position=0, leave=False, desc='Test')\n",
        "        with torch.no_grad():\n",
        "            for i, data in tqdm(enumerate(self.test_loader)):\n",
        "                x, x_len = data\n",
        "                x = x.cuda()\n",
        "\n",
        "                with torch.no_grad():    \n",
        "                    output, output_len = model(x, x_len)\n",
        "\n",
        "                beam_results, beam_scores, timesteps, out_lens = self.decoder.decode(output, seq_lens=output_len)\n",
        "                for b in range(output.shape[0]):\n",
        "                    predictions.append(convert_to_string(beam_results[b][0], out_lens[b][0], self.phoneme_map, self.phonemes))\n",
        "                \n",
        "                batch_bar.update()\n",
        "            batch_bar.close()\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def load_model(self, model_name, model_type): \n",
        "        meta_path = os.path.join(self.drive_dir,  model_type, model_name, 'model_config.yaml')\n",
        "        with open(meta_path, 'r') as meta:\n",
        "            args = yaml.safe_load(meta)\n",
        "\n",
        "        model_path = os.path.join(self.drive_dir, model_type, model_name, 'model.pth')\n",
        "        model = SpeechNet(args).cuda()\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        return model, args\n",
        "\n",
        "    def simple_inference(self, model_name, model_type):\n",
        "        print(\"Running inference...\")\n",
        "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        model, args = self.load_model(model_name, model_type)\n",
        "        preds = self.get_predictions(model)\n",
        "        \n",
        "        return preds\n",
        "\n",
        "    def ensemble_inference(self, model_names, model_type):\n",
        "        print(\"Running ensembled inference...\")\n",
        "        self.timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "        prelim_labels = []\n",
        "        for name in model_names:\n",
        "            print(\"\\n\\n\\tModel : \", name)\n",
        "            model, args = self.load_model(name, model_type)\n",
        "            prelim_labels.append(self.__get_labels(model, args))\n",
        "\n",
        "        accs = [86.146, 85.79, 84.95]\n",
        "        w = accs / np.sum(accs)\n",
        "\n",
        "        print(\"Combining predictions...\")\n",
        "        labels_df = pd.DataFrame(prelim_labels)\n",
        "        labels_df = labels_df.transpose()\n",
        "        ensembled_labels = labels_df.mode(axis=1, dropna=False).iloc[:, 0].tolist()\n",
        "        # ensembled_labels = np.where((df.iloc[:,1] == df.iloc[:, 2]), df.iloc[:, 1], df.iloc[:, 0]).tolist()\n",
        "\n",
        "        return labels_df, ensembled_labels\n",
        "\n",
        "    def generate_submission(self, save_path, preds): \n",
        "        print(\"Generating Submission CSV...\")\n",
        "        sub_dir = os.path.join(self.drive_dir, save_path + self.timestamp)\n",
        "        try:\n",
        "            os.mkdir(sub_dir)\n",
        "        except:\n",
        "            print(\"Couldn't create folder for submission.csv\")\n",
        "            \n",
        "        sub_path = os.path.join(sub_dir, 'submission.csv')\n",
        "\n",
        "        with open(sub_path, 'w') as f:\n",
        "            csvwrite = csv.writer(f)\n",
        "            csvwrite.writerow(['id', 'predictions'])\n",
        "            for i in range(len(preds)):\n",
        "                csvwrite.writerow([i, preds[i]])\n",
        "\n",
        "        print(f\"File saved at : {sub_path}\")\n",
        "        return sub_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nswfp1XMwz3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d32d6ba-0dd9-4289-f146-2ab7ce946482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Data: 100%|| 2620/2620 [00:00<00:00, 3407.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test:   0%|          | 0/41 [00:00<?, ?it/s]\n",
            "Test:   2%|         | 1/41 [00:01<01:05,  1.65s/it]\n",
            "Test:   5%|         | 2/41 [00:04<01:31,  2.35s/it]\n",
            "Test:   7%|         | 3/41 [00:06<01:31,  2.41s/it]\n",
            "Test:  10%|         | 4/41 [00:08<01:21,  2.20s/it]\n",
            "Test:  12%|        | 5/41 [00:11<01:22,  2.30s/it]\n",
            "Test:  15%|        | 6/41 [00:14<01:26,  2.46s/it]\n",
            "Test:  17%|        | 7/41 [00:17<01:31,  2.70s/it]\n",
            "Test:  20%|        | 8/41 [00:20<01:29,  2.72s/it]\n",
            "Test:  22%|       | 9/41 [00:24<01:39,  3.12s/it]\n",
            "Test:  24%|       | 10/41 [00:28<01:46,  3.44s/it]\n",
            "Test:  27%|       | 11/41 [00:31<01:41,  3.38s/it]\n",
            "Test:  29%|       | 12/41 [00:34<01:31,  3.17s/it]\n",
            "Test:  32%|      | 13/41 [00:35<01:16,  2.71s/it]\n",
            "Test:  34%|      | 14/41 [00:38<01:13,  2.74s/it]\n",
            "Test:  37%|      | 15/41 [00:42<01:17,  2.97s/it]\n",
            "Test:  39%|      | 16/41 [00:45<01:21,  3.24s/it]\n",
            "Test:  41%|     | 17/41 [00:47<01:08,  2.85s/it]\n",
            "Test:  44%|     | 18/41 [00:51<01:09,  3.03s/it]\n",
            "Test:  46%|     | 19/41 [00:55<01:11,  3.24s/it]\n",
            "Test:  49%|     | 20/41 [00:59<01:12,  3.44s/it]\n",
            "Test:  51%|     | 21/41 [01:03<01:14,  3.71s/it]\n",
            "Test:  54%|    | 22/41 [01:05<01:02,  3.28s/it]\n",
            "Test:  56%|    | 23/41 [01:07<00:50,  2.82s/it]\n",
            "Test:  59%|    | 24/41 [01:11<00:52,  3.10s/it]\n",
            "Test:  61%|    | 25/41 [01:13<00:47,  2.99s/it]\n",
            "Test:  63%|   | 26/41 [01:17<00:47,  3.16s/it]\n",
            "Test:  66%|   | 27/41 [01:19<00:41,  2.98s/it]\n",
            "Test:  68%|   | 28/41 [01:22<00:35,  2.75s/it]\n",
            "Test:  71%|   | 29/41 [01:26<00:37,  3.16s/it]\n",
            "Test:  73%|  | 30/41 [01:28<00:32,  2.94s/it]\n",
            "Test:  76%|  | 31/41 [01:30<00:26,  2.68s/it]\n",
            "Test:  78%|  | 32/41 [01:33<00:23,  2.63s/it]\n",
            "Test:  80%|  | 33/41 [01:36<00:21,  2.75s/it]\n",
            "Test:  83%| | 34/41 [01:39<00:19,  2.86s/it]\n",
            "Test:  85%| | 35/41 [01:42<00:16,  2.80s/it]\n",
            "Test:  88%| | 36/41 [01:45<00:14,  2.84s/it]\n",
            "Test:  90%| | 37/41 [01:50<00:13,  3.50s/it]\n",
            "Test:  93%|| 38/41 [01:54<00:10,  3.64s/it]\n",
            "Test:  95%|| 39/41 [01:56<00:06,  3.36s/it]\n",
            "Test:  98%|| 40/41 [01:59<00:03,  3.09s/it]\n",
            "Test: 100%|| 41/41 [02:02<00:00,  3.01s/it]\n",
            "41it [02:02,  2.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Submission CSV...\n",
            "File saved at : /content/cmudrive/IDL/hw3-submission/2022-04-15_03-01-17/submission.csv\n",
            "Preview of submission.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                        predictions\n",
              "0   0  .HIbIgAnhkhnf?UzdkhmplEnthgenstDhWizrd.HUHAdvA...\n",
              "1   1               .kUvnatsO.rnisthmyndtiDIzmhmrIzcyld.\n",
              "2   2                        .hgOldhnfoRcUnhndhHApIlyf..\n",
              "3   3      .HIWhzlykhnthmyfaDrhndhWE.And?etWhznatmyfaDr.\n",
              "4   4            .olsODeRWhzhstRipliNpEj.HUtrnditthhmEd.\n",
              "5   5   .DisWhzsOsWIthlldEtIsr.AndinshmmAnrydUTiNkSIdyd.\n",
              "6   6               .bhtDenDhpikcrWhzgonezkWiklIezikEm..\n",
              "7   7                        ..sistrnOdU?UHIRDIzmaRvhlz.\n",
              "8   8       .tEk?oRplEsHhndlethssIWhtDhkRisphlkhnSOth?U.\n",
              "9   9                 .lykiznat?hNmAstr.DO.yAmhnOldmAn.."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1fa8fd44-8aad-4b1d-9771-77ffc31afee8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>.HIbIgAnhkhnf?UzdkhmplEnthgenstDhWizrd.HUHAdvA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>.kUvnatsO.rnisthmyndtiDIzmhmrIzcyld.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>.hgOldhnfoRcUnhndhHApIlyf..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>.HIWhzlykhnthmyfaDrhndhWE.And?etWhznatmyfaDr.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>.olsODeRWhzhstRipliNpEj.HUtrnditthhmEd.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>.DisWhzsOsWIthlldEtIsr.AndinshmmAnrydUTiNkSIdyd.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>.bhtDenDhpikcrWhzgonezkWiklIezikEm..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>..sistrnOdU?UHIRDIzmaRvhlz.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>.tEk?oRplEsHhndlethssIWhtDhkRisphlkhnSOth?U.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>.lykiznat?hNmAstr.DO.yAmhnOldmAn..</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fa8fd44-8aad-4b1d-9771-77ffc31afee8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1fa8fd44-8aad-4b1d-9771-77ffc31afee8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1fa8fd44-8aad-4b1d-9771-77ffc31afee8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model_name = 'AWS_L4600-Conv3-L2-Drop6-600-4-0.6_b128_e105_sCALR_oAdamP_rLSTM_lr0.002'\n",
        "# model_name = latest_model_name\n",
        "\n",
        "model_type = r'hw3-last'\n",
        "sub_path = r'hw3-submission/'\n",
        "inference = SpeechInference(bw=41)\n",
        "\n",
        "predictions = inference.simple_inference(model_name, model_type)\n",
        "submission_path = inference.generate_submission(sub_path, predictions)\n",
        "\n",
        "# Simple and Ensemble Inference\n",
        "# labels = inference.simple_inference(model_name, model_type)\n",
        "# labels_df, labels = inference.ensemble_inference(models, model_type)\n",
        "\n",
        "print(f\"Preview of submission.csv\")\n",
        "df = pd.read_csv(submission_path)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03-w9it1pXIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff7cccd-90c8-414c-c541-c94795356603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cmudrive/IDL/hw3-submission/2022-04-15_03-01-17/submission.csv\n",
            "100% 215k/215k [00:02<00:00, 108kB/s]  \n",
            "Successfully submitted to Automatic Speech Recognition (Slack)"
          ]
        }
      ],
      "source": [
        "print(submission_path)\n",
        "! kaggle competitions submit -c automatic-speech-recognition-slack -f $submission_path -m \"Final\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'AWS_L4600-Conv3-L2-Drop6-600-4-0.6_b128_e105_sCALR_oAdamP_rLSTM_lr0.002'\n",
        "# model_name = latest_model_name\n",
        "\n",
        "model_type = r'hw3-last'\n",
        "sub_path = r'hw3-submission/'\n",
        "inference = SpeechInference(bw=41)\n",
        "\n",
        "predictions = inference.simple_inference(model_name, model_type)\n",
        "submission_path = inference.generate_submission(sub_path, predictions)\n",
        "\n",
        "# Simple and Ensemble Inference\n",
        "# labels = inference.simple_inference(model_name, model_type)\n",
        "# labels_df, labels = inference.ensemble_inference(models, model_type)\n",
        "\n",
        "print(f\"Preview of submission.csv\")\n",
        "df = pd.read_csv(submission_path)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "35NrnQkQj42k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86195d51-3a2b-4692-c3c8-7afdee18f644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Data: 100%|| 2620/2620 [00:00<00:00, 3423.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test:   0%|          | 0/41 [00:00<?, ?it/s]\n",
            "Test:   2%|         | 1/41 [00:02<01:28,  2.22s/it]\n",
            "Test:   5%|         | 2/41 [00:05<02:02,  3.14s/it]\n",
            "Test:   7%|         | 3/41 [00:09<02:02,  3.21s/it]\n",
            "Test:  10%|         | 4/41 [00:11<01:49,  2.97s/it]\n",
            "Test:  12%|        | 5/41 [00:15<01:50,  3.08s/it]\n",
            "Test:  15%|        | 6/41 [00:18<01:52,  3.22s/it]\n",
            "Test:  17%|        | 7/41 [00:22<01:59,  3.52s/it]\n",
            "Test:  20%|        | 8/41 [00:26<01:58,  3.61s/it]\n",
            "Test:  22%|       | 9/41 [00:32<02:14,  4.19s/it]\n",
            "Test:  24%|       | 10/41 [00:37<02:24,  4.65s/it]\n",
            "Test:  27%|       | 11/41 [00:42<02:18,  4.63s/it]\n",
            "Test:  29%|       | 12/41 [00:45<02:05,  4.33s/it]\n",
            "Test:  32%|      | 13/41 [00:48<01:43,  3.68s/it]\n",
            "Test:  34%|      | 14/41 [00:51<01:39,  3.70s/it]\n",
            "Test:  37%|      | 15/41 [00:56<01:44,  4.02s/it]\n",
            "Test:  39%|      | 16/41 [01:01<01:50,  4.41s/it]\n",
            "Test:  41%|     | 17/41 [01:04<01:33,  3.88s/it]\n",
            "Test:  44%|     | 18/41 [01:09<01:34,  4.11s/it]\n",
            "Test:  46%|     | 19/41 [01:14<01:37,  4.44s/it]\n",
            "Test:  49%|     | 20/41 [01:19<01:38,  4.71s/it]\n",
            "Test:  51%|     | 21/41 [01:25<01:42,  5.11s/it]\n",
            "Test:  54%|    | 22/41 [01:29<01:25,  4.52s/it]\n",
            "Test:  56%|    | 23/41 [01:31<01:09,  3.86s/it]\n",
            "Test:  59%|    | 24/41 [01:36<01:11,  4.22s/it]\n",
            "Test:  61%|    | 25/41 [01:40<01:04,  4.05s/it]\n",
            "Test:  63%|   | 26/41 [01:44<01:04,  4.31s/it]\n",
            "Test:  66%|   | 27/41 [01:48<00:56,  4.04s/it]\n",
            "Test:  68%|   | 28/41 [01:51<00:48,  3.70s/it]\n",
            "Test:  71%|   | 29/41 [01:56<00:51,  4.29s/it]\n",
            "Test:  73%|  | 30/41 [02:00<00:44,  4.05s/it]\n",
            "Test:  76%|  | 31/41 [02:03<00:36,  3.67s/it]\n",
            "Test:  78%|  | 32/41 [02:06<00:32,  3.57s/it]\n",
            "Test:  80%|  | 33/41 [02:10<00:29,  3.70s/it]\n",
            "Test:  83%| | 34/41 [02:14<00:26,  3.84s/it]\n",
            "Test:  85%| | 35/41 [02:18<00:22,  3.74s/it]\n",
            "Test:  88%| | 36/41 [02:22<00:19,  3.80s/it]\n",
            "Test:  90%| | 37/41 [02:29<00:18,  4.73s/it]\n",
            "Test:  93%|| 38/41 [02:34<00:14,  4.94s/it]\n",
            "Test:  95%|| 39/41 [02:38<00:09,  4.57s/it]\n",
            "Test:  98%|| 40/41 [02:41<00:04,  4.21s/it]\n",
            "Test: 100%|| 41/41 [02:45<00:00,  4.12s/it]\n",
            "41it [02:45,  4.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Submission CSV...\n",
            "File saved at : /content/cmudrive/IDL/hw3-submission/2022-04-15_03-06-00/submission.csv\n",
            "Preview of submission.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                        predictions\n",
              "0   0  .HIbIgAnhkhnf?UzdkhmplEnthgenstDhWizrd.HUHAdvA...\n",
              "1   1               .kUvnatsO.rnisthmyndtiDIzmhmrIzcyld.\n",
              "2   2                        .hgOldhnfoRcUnhndhHApIlyf..\n",
              "3   3      .HIWhzlykhnthmyfaDrhndhWE.And?etWhznatmyfaDr.\n",
              "4   4            .olsODeRWhzhstRipliNpEj.HUtrnditthhmEd.\n",
              "5   5   .DisWhzsOsWIthlldEtIsr.AndinshmmAnrydUTiNkSIdyd.\n",
              "6   6               .bhtDenDhpikcrWhzgonezkWiklIezikEm..\n",
              "7   7                        ..sistrnOdU?UHIRDIzmaRvhlz.\n",
              "8   8       .tEk?oRplEsHhndlethssIWhtDhkRisphlkhnSOth?U.\n",
              "9   9                 .lykiznat?hNmAstr.DO.yAmhnOldmAn.."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3ed278c-1069-47d7-9a25-c16c1780784a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>.HIbIgAnhkhnf?UzdkhmplEnthgenstDhWizrd.HUHAdvA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>.kUvnatsO.rnisthmyndtiDIzmhmrIzcyld.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>.hgOldhnfoRcUnhndhHApIlyf..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>.HIWhzlykhnthmyfaDrhndhWE.And?etWhznatmyfaDr.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>.olsODeRWhzhstRipliNpEj.HUtrnditthhmEd.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>.DisWhzsOsWIthlldEtIsr.AndinshmmAnrydUTiNkSIdyd.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>.bhtDenDhpikcrWhzgonezkWiklIezikEm..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>..sistrnOdU?UHIRDIzmaRvhlz.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>.tEk?oRplEsHhndlethssIWhtDhkRisphlkhnSOth?U.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>.lykiznat?hNmAstr.DO.yAmhnOldmAn..</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3ed278c-1069-47d7-9a25-c16c1780784a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d3ed278c-1069-47d7-9a25-c16c1780784a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d3ed278c-1069-47d7-9a25-c16c1780784a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission_path)\n",
        "! kaggle competitions submit -c automatic-speech-recognition-slack -f $submission_path -m \"Gg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lL_jvl9K2MN",
        "outputId": "07919306-b1a0-4682-f421-ef7b81ddd177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cmudrive/IDL/hw3-submission/2022-04-15_03-06-00/submission.csv\n",
            "100% 215k/215k [00:02<00:00, 109kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (Slack)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MkAQgk3gL23m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "f_fwJWcpqJDR",
        "z4vZbDmJvMp1",
        "vUCKqm1ST1sU",
        "yA-6ZrNI_rSi",
        "7yLRX4iwPP90",
        "IBwunYpyugFg",
        "72yaABA_w8kt",
        "I2ptgaeHadVN"
      ],
      "name": "HW3P2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
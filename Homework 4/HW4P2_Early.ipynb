{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nefario7/cmu-deeplearning/blob/working-hw4/Homework%204/HW4P2_Early.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufLFWkqEh-5e"
      },
      "source": [
        "## Drive and Kaggle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk4LwVAQh-5g",
        "outputId": "fb9bb560-8e88-47cb-d392-19d199b20c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connected to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ubu\r0% [1 InRelease gpgv 1,575 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 1,575 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:7 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:11 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,496 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,167 kB]\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic/main amd64 Packages [1,343 B]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,733 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,953 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,001 kB]\n",
            "Fetched 12.9 MB in 4s (3,035 kB/s)\n",
            "Reading package lists... Done\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:9 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:10 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following NEW packages will be installed:\n",
            "  google-drive-ocamlfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,330 kB of archives.\n",
            "After this operation, 7,023 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic/main amd64 google-drive-ocamlfuse amd64 0.7.27-0ubuntu1~ubuntu18.04.1 [1,330 kB]\n",
            "Fetched 1,330 kB in 1s (1,120 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155203 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: www-browser: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links2: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: elinks: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: lynx: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=U1cfZ94dmdJuVh5UXB4rElfo0LxZ-47Egz8hWSd2Myc'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Cannot retrieve auth tokens.\n",
            "Failure(\"Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=U1cfZ94dmdJuVh5UXB4rElfo0LxZ-47Egz8hWSd2Myc\")\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output \n",
        "# NOTWORKING\n",
        "# ! apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# ! add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# ! apt-get update -qq 2>&1 > /dev/null\n",
        "# ! apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# from google.colab import auth\n",
        "# import getpass\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# auth.authenticate_user()\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "\n",
        "\n",
        "# ! google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# ! echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# WORKING\n",
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa\n",
        "!sudo apt update\n",
        "!sudo apt install google-drive-ocamlfuse\n",
        "!google-drive-ocamlfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfzGVsbah-5i"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install w3m # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser \n",
        "\n",
        "% cd /content\n",
        "! mkdir cmudrive\n",
        "% cd ..\n",
        "! google-drive-ocamlfuse /content/cmudrive\n",
        "! pip install kaggle wandb torch-summary\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/cmudrive/IDL/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle \n",
        "! kaggle config set -n path -v /content\n",
        "\n",
        "! wandb login 4bdbe9c204105e1264fe3f54df2732fd1fff8040\n",
        "! pip install python-Levenshtein\n",
        "! pip install torchsummaryX\n",
        "! pip install wget\n",
        "! pip install adamp\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nOjNvNah-5j"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions download -c 11-785-s22-hw4p2\n",
        "\n",
        "! unzip -q /content/competitions/11-785-s22-hw4p2/11-785-s22-hw4p2.zip -d /content\n",
        "! mv /content/hw4p2_student_data/hw4p2_student_data /content/actual_data\n",
        "! mv /content/hw4p2_simple/hw4p2_simple /content/toy_data\n",
        "! rm -rf /content/hw4p2_student_data\n",
        "! rm -rf /content/hw4p2_simple\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBrQ8OHjW6qC"
      },
      "source": [
        "## Libraries and Initial Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HTGPr98x0yjO",
        "outputId": "bc885db0-471c-40f5-8964-86f4bc7e19b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True 3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "Cuda = True with num_workers = 4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import yaml\n",
        "import wandb\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import Levenshtein\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.utils as utils\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from torchsummaryX import summary\n",
        "# from torchsummary import summary\n",
        "from adamp import AdamP\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "import multiprocessing\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "torch.autograd.profiler.profile(False)\n",
        "torch.autograd.profiler.emit_nvtx(False)\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "print(cuda, sys.version)\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "NUM_WORKERS = 4 if cuda else 0\n",
        "print(\"Cuda = \" + str(cuda)+\" with num_workers = \" + str(NUM_WORKERS))\n",
        "np.random.seed(11785)\n",
        "torch.manual_seed(11785)\n",
        "\n",
        "# The labels of the dataset contain letters in LETTER_LIST.\n",
        "# You should use this to convert the letters to the corresponding indices\n",
        "# and train your model with numerical labels.\n",
        "LETTER_LIST = ['<sos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \\\n",
        "         'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ', '<eos>']\n",
        "\n",
        "def header(head):\n",
        "    print(\"-\"*80)\n",
        "    print(f\"\\t\\t\\t\\t{head.upper()}\")\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xc7E_BuxZ1g"
      },
      "outputs": [],
      "source": [
        "def create_dictionaries(letter_list):\n",
        "    '''\n",
        "    Create dictionaries for letter2index and index2letter transformations\n",
        "    based on LETTER_LIST\n",
        "\n",
        "    Args:\n",
        "        letter_list: LETTER_LIST\n",
        "\n",
        "    Return:\n",
        "        letter2index: Dictionary mapping from letters to indices\n",
        "        index2letter: Dictionary mapping from indices to letters\n",
        "    '''\n",
        "    letter2index = dict(zip(letter_list, list(range(len(letter_list)))))\n",
        "    index2letter = dict(zip(list(range(len(letter_list))), letter_list))\n",
        "    # TODO\n",
        "    return letter2index, index2letter\n",
        "    \n",
        "\n",
        "def transform_index_to_letter(batch_indices):\n",
        "    '''\n",
        "    Transforms numerical index input to string output by converting each index \n",
        "    to its corresponding letter from LETTER_LIST\n",
        "\n",
        "    Args:\n",
        "        batch_indices: List of indices from LETTER_LIST with the shape of (N, )\n",
        "    \n",
        "    Return:\n",
        "        transcripts: List of converted string transcripts. This would be a list with a length of N\n",
        "    '''\n",
        "    transcripts = []\n",
        "    for i, indices in enumerate(batch_indices):\n",
        "        transcript_string = ''.join([LETTER_LIST[k] for k in indices])\n",
        "        transcripts.append(transcript_string)\n",
        "        \n",
        "    return transcripts\n",
        "        \n",
        "# Create the letter2index and index2letter dictionary\n",
        "letter2index, index2letter = create_dictionaries(LETTER_LIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jopP5si1qlKt",
        "outputId": "e06b69af-5f4b-479b-d517-652adb738408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '<sos>', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J', 11: 'K', 12: 'L', 13: 'M', 14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'S', 20: 'T', 21: 'U', 22: 'V', 23: 'W', 24: 'X', 25: 'Y', 26: 'Z', 27: \"'\", 28: ' ', 29: '<eos>'}\n"
          ]
        }
      ],
      "source": [
        "print(index2letter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ioyn6ldQB9"
      },
      "source": [
        "# Dataset and Dataloading (TODO)\n",
        "\n",
        "You will need to implement the Dataset class by your own. You can implement it similar to HW3P2. However, you are welcomed to do it your own way if it is more comfortable or efficient.\n",
        "\n",
        "Note that you need to use LETTER_LIST to convert the transcript into numerical labels for the model.\n",
        "\n",
        "\n",
        "Example of raw transcript:\n",
        "\n",
        "    ['<sos>', 'N', 'O', 'R', 'T', 'H', 'A', 'N', 'G', 'E', 'R', ' ','A', 'B', 'B', 'E', 'Y', '<eos>']\n",
        "\n",
        "Example of converted transcript ready to process for the model:\n",
        "\n",
        "    [0, 14, 15, 18, 20, 8, 1, 14, 7, 5, 18, 28, 1, 2, 2, 5, 25, 29]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHV6nkk6ODzY"
      },
      "outputs": [],
      "source": [
        "# transcripts = os.listdir(r\"/content/actual_data/train/transcript\")\n",
        "# temp = np.load(os.path.join(r\"/content/actual_data/train/transcript\", transcripts[0]))\n",
        "# print(np.array([*map(letter2index.get, temp)]))\n",
        "\n",
        "# y = np.load('/content/toy_data/dev_transcripts.npy', allow_pickle=True)\n",
        "# print(type(y))\n",
        "# print(len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path=r\"/content/actual_data\", partition=\"train\"):\n",
        "        self.test = True if partition == \"test\" else False\n",
        "\n",
        "        if data_path == '/content/actual_data':\n",
        "            partition_path = os.path.join(data_path, partition)\n",
        "            self.X_dir = os.path.join(partition_path, 'mfcc')\n",
        "            self.Y_dir = os.path.join(partition_path, 'transcript')\n",
        "\n",
        "            if self.test:\n",
        "                with open(os.path.join(partition_path, 'test_order.csv'),\"r\") as f:\n",
        "                    self.X_files = list(csv.reader(f))[1:]\n",
        "                self.X_data = [np.load(os.path.join(self.X_dir, xfile[0])) for xfile in tqdm(self.X_files, desc=\"MFCC\", position=0, leave=True)]\n",
        "            else:\n",
        "                self.X_files = os.listdir(self.X_dir)\n",
        "                self.Y_files = os.listdir(self.Y_dir)\n",
        "\n",
        "                self.X_data = [np.load(os.path.join(self.X_dir, xfile)) for xfile in tqdm(self.X_files, desc=\"MFCC\", position=0, leave=True)]\n",
        "                self.Y_data = [np.array([*map(letter2index.get, np.load(os.path.join(self.Y_dir, yfile)))]) for yfile in tqdm(self.Y_files, desc=\"Transcript\", position=0, leave=True)]\n",
        "        else:\n",
        "            self.X_data = np.load(os.path.join(data_path, partition + '.npy'), allow_pickle=True)\n",
        "            transcripts = np.load(os.path.join(data_path, partition + '_transcripts.npy'), allow_pickle=True)\n",
        "            self.Y_data = [np.array([*map(letter2index.get, t)]) for t in transcripts]\n",
        "\n",
        "        if not self.test:\n",
        "            assert(len(self.X_data) == len(self.Y_data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X_data[idx]\n",
        "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "        if self.test:\n",
        "            return torch.from_numpy(X)\n",
        "            # return torch.from_numpy(X).type(torch.LongTensor)\n",
        "        else:\n",
        "            Y = self.Y_data[idx]\n",
        "            return torch.from_numpy(X), torch.from_numpy(Y)\n",
        "            # return torch.from_numpy(X).type(torch.LongTensor), torch.from_numpy(Y).type(torch.LongTensor)\n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        if self.test:\n",
        "            batch_x = [x for x in batch]\n",
        "            batch_x_pad = pad_sequence(batch_x, batch_first=True)\n",
        "            lengths_x = [len(x) for x in batch_x]\n",
        "            return batch_x_pad, torch.tensor(lengths_x)\n",
        "        else:\n",
        "            batch_x = [x for x,_ in batch]\n",
        "            batch_y = [y for _,y in batch]\n",
        "            batch_x_pad = pad_sequence(batch_x, batch_first=True)\n",
        "            batch_y_pad = pad_sequence(batch_y, batch_first=True, padding_value=letter2index['<eos>'])\n",
        "            lengths_x = [len(x) for x in batch_x]\n",
        "            lengths_y = [len(y) for y in batch_y]\n",
        "            \n",
        "            return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mzoYfTKu14s"
      },
      "outputs": [],
      "source": [
        "# batch_size = 64\n",
        "\n",
        "# train_data = LibriSamples(data_path=r\"/content/toy_data\", partition='train')\n",
        "# val_data = LibriSamples(data_path=r\"/content/toy_data\", partition='dev')\n",
        "# # test_data = LibriSamples(partition='test')\n",
        "\n",
        "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn = train_data.collate_fn, num_workers=2)\n",
        "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn = val_data.collate_fn, num_workers=2)\n",
        "# # test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn = test_data.collate_fn)\n",
        "\n",
        "# print(\"Batch size: \", batch_size)\n",
        "# print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "# print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "# # print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--VjKlEhwi8"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpSduz4H_f4O"
      },
      "source": [
        "#### pBLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7W4b15f4AuG"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "    def forward(self, x):\n",
        "        if self.training and self.dropout:\n",
        "            m = x.data.new(1, x.shape[1], x.shape[2]).bernoulli_(1 - self.dropout)\n",
        "            mask = Variable(m, requires_grad=False) / (1 - self.dropout)\n",
        "            mask = mask.expand_as(x)\n",
        "            return mask * x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfpIMUDzCvT3"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed\n",
        "    2. Truncate the input length dimension by concatenating feature dimension\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, dropout=0, downsampling=2, locked_dropout=0.0):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.downsampling = downsampling\n",
        "        self.locked_dropout = locked_dropout\n",
        "\n",
        "        self.blstm = nn.LSTM(\n",
        "            input_size=input_dim, \n",
        "            hidden_size=hidden_dim, \n",
        "            num_layers=1, \n",
        "            dropout=dropout, \n",
        "            bidirectional=True, \n",
        "            batch_first=True\n",
        "            )\n",
        "        if self.locked_dropout:\n",
        "            self.ld = LockedDropout(self.locked_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        padded_x, padded_x_len = rnn_utils.pad_packed_sequence(x, batch_first=True)\n",
        "        padded_x = padded_x.to(device)\n",
        "\n",
        "        # print(padded_x.shape, padded_x_len.shape)\n",
        "        timesteps = padded_x.size(1)\n",
        "        trim =  timesteps - timesteps % self.downsampling\n",
        "        padded_x = padded_x[:, :trim, :]\n",
        "        padded_x_len = padded_x_len // self.downsampling\n",
        "        # print(\"Downsampled\", padded_x.shape, padded_x_len.shape)\n",
        "        \n",
        "        B, T, F = padded_x.size()\n",
        "        # Simple\n",
        "        reshaped_x = padded_x.reshape(B, T // self.downsampling, F * self.downsampling)\n",
        "        reshaped_x = reshaped_x.to(device)\n",
        "\n",
        "        if self.locked_dropout:\n",
        "            reshaped_x = self.ld.forward(reshaped_x)\n",
        "        # Alternative with mean/ max\n",
        "        # reshaped_x = padded_x.reshape(B, T // self.downsampling, self.downsampling, F)\n",
        "        # reshaped_x = torch.mean(reshaped, dim=2)\n",
        "        # reshaped_x = torch.max(reshaped, dim=2)\n",
        "\n",
        "        packed_x = rnn_utils.pack_padded_sequence(reshaped_x, lengths=padded_x_len, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        out, _ = self.blstm(packed_x.to(device))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22nBIrW0d7yo"
      },
      "outputs": [],
      "source": [
        "# b, t, f = x.shape\n",
        "# res = x.reshape(b, t // 2, 2, f)\n",
        "\n",
        "# print(res[0][0], res.shape)\n",
        "\n",
        "# men = torch.mean(res, 2)\n",
        "# print(men[0][0], men.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_i4nnrZ_h0P"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEmFI9It54mk"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key, value and unpacked_x_len.\n",
        "\n",
        "    '''\n",
        "    def __init__(self, input_dim, encoder_hidden_dim, encoder_layers=1, key_value_size=128, dropout=0.2, downsampling=2, locked_dropout=0.0):\n",
        "        super(Encoder, self).__init__()\n",
        "        # The first LSTM layer at the bottom\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=encoder_hidden_dim, num_layers=encoder_layers, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Define the blocks of pBLSTMs\n",
        "        # Dimensions should be chosen carefully\n",
        "        # Hint: Bidirectionality, truncation...\n",
        "        self.pblstm = nn.Sequential(\n",
        "            pBLSTM(encoder_hidden_dim*2*downsampling, encoder_hidden_dim, dropout, downsampling, locked_dropout=locked_dropout),\n",
        "            pBLSTM(encoder_hidden_dim*2*downsampling, encoder_hidden_dim, dropout, downsampling, locked_dropout=locked_dropout),\n",
        "            pBLSTM(encoder_hidden_dim*2*downsampling, encoder_hidden_dim, dropout, downsampling, locked_dropout=0.0),\n",
        "        )\n",
        "         \n",
        "        # The linear transformations for producing Key and Value for attention\n",
        "        # Hint: Dimensions when bidirectional lstm?\n",
        "        self.key_network = nn.Linear(encoder_hidden_dim * 2, key_value_size)\n",
        "        self.value_network = nn.Linear(encoder_hidden_dim * 2, key_value_size)\n",
        "        \n",
        "        self.weight_initialization()\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        1. Pack your input and pass it through the first LSTM layer (no truncation)\n",
        "        2. Pass it through the pyramidal LSTM layer\n",
        "        3. Pad your input back to (B, T, *) or (T, B, *) shape\n",
        "        4. Output Key, Value, and truncated input lens\n",
        "\n",
        "        Key and value could be\n",
        "            (i) Concatenated hidden vectors from all time steps (key == value).\n",
        "            (ii) Linear projections of the output from the last pBLSTM network.\n",
        "                If you choose this way, you can use the final output of\n",
        "                your pBLSTM network.\n",
        "        \"\"\"\n",
        "        # print(\"x: {}, x_len: {}\".format(x.shape, x_len.shape))\n",
        "        packed_input = rnn_utils.pack_padded_sequence(x, lengths=x_len, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        lstm_output, _ = self.lstm(packed_input.to(device))\n",
        "        lstm_output = lstm_output.to(device)\n",
        "        pblstm_output = self.pblstm(lstm_output)\n",
        "        pblstm_output = pblstm_output.to(device)\n",
        "\n",
        "        encoder_output, encoder_lens = rnn_utils.pad_packed_sequence(pblstm_output, batch_first=True)\n",
        "\n",
        "        keys_output = self.key_network(encoder_output.to(device)).to(device)\n",
        "        value_output = self.value_network(encoder_output.to(device)).to(device)\n",
        "\n",
        "        return keys_output, value_output, encoder_lens\n",
        "\n",
        "    def weight_initialization(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.LSTM):\n",
        "                nn.init.xavier_uniform_(m.weight_ih_l0, gain=0.1)\n",
        "                nn.init.xavier_uniform_(m.weight_ih_l0_reverse, gain=0.1)\n",
        "                nn.init.xavier_uniform_(m.weight_hh_l0, gain=0.1)\n",
        "                nn.init.xavier_uniform_(m.weight_hh_l0_reverse, gain=0.1)\n",
        "                # print(m.__dict__[\"_parameters\"].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr2L3gvJ_vqH"
      },
      "source": [
        "#### Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqu-MUM8TjUO"
      },
      "outputs": [],
      "source": [
        "def plot_attention(attention):\n",
        "    # utility function for debugging\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key and value from encoder and query from decoder.\n",
        "    Here are different ways to compute attention and context:\n",
        "    1. Dot-product attention\n",
        "        energy = bmm(key, query) \n",
        "        # Optional: Scaled dot-product by normalizing with sqrt key dimension\n",
        "        # Check \"attention is all you need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore...\n",
        "    2. Cosine attention\n",
        "        energy = cosine(query, key) # almost the same as dot-product xD \n",
        "    3. Bi-linear attention\n",
        "        W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "        energy = bmm(key @ W, query)\n",
        "    4. Multi-layer perceptron\n",
        "        # Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "    \n",
        "    After obtaining unnormalized attention weights (energy), compute and return attention and context, i.e.,\n",
        "    energy = mask(energy) # mask out padded elements with big negative number (e.g. -1e9)\n",
        "    attention = softmax(energy)\n",
        "    context = bmm(attention, value)\n",
        "\n",
        "    5. Multi-Head Attention\n",
        "        # Check \"attention is all you need\" Section 3.2.2\n",
        "        h = Number of heads\n",
        "        W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "        W_O: d_v -> d_v\n",
        "\n",
        "        Reshape K: (B, T, d_k)\n",
        "        to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "        Reshape V: (B, T, d_v)\n",
        "        to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "        Reshape Q: (B, d_q)\n",
        "        to (B, h, d_q // h)\n",
        "\n",
        "        energy = Q @ K^T\n",
        "        energy = mask(energy)\n",
        "        attention = softmax(energy)\n",
        "        multi_head = attention @ V\n",
        "        multi_head = multi_head reshaped to (B, d_v)\n",
        "        context = multi_head @ W_O\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        # Optional: dropout\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            key: (batch_size, seq_len, d_k) #(N, S, 128)\n",
        "            value: (batch_size, seq_len, d_v) #(N, S, 128)\n",
        "            query: (batch_size, d_q)\n",
        "        * Hint: d_k == d_v == d_q is often true if you use linear projections\n",
        "        return:\n",
        "            context: (batch_size, key_val_dim)\n",
        "        \n",
        "        \"\"\"\n",
        "        energy = torch.bmm(key, query.unsqueeze(dim=2)) # (B x S x 128) * (B x 128 x 1) -> (B x 260 x 1)\n",
        "        energy = energy.squeeze(dim=2)  # (B x S)\n",
        "\n",
        "        masked_energy = energy * mask.int().float()\n",
        "        # masked_energy = energy.masked_fill(mask, -1e9)\n",
        "        \n",
        "        normal_energy = masked_energy / np.sqrt(key.size(2)) # (B, S)\n",
        "\n",
        "        attention = F.softmax(normal_energy, dim=1) # (B, S)\n",
        "        # print(attention.shape)\n",
        "        context = torch.bmm(attention.unsqueeze(1), value).squeeze(1)   # (B x 1 x S) * (B x S x 128) -> (B x S)\n",
        "        # print(\"context\", context.shape, \"attention\", attention.shape)\n",
        "\n",
        "        return context, attention\n",
        "        # we return attention weights for plotting (for debugging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrTUB8HMMvh-"
      },
      "outputs": [],
      "source": [
        "# q = torch.randn(64, 128)\n",
        "# energy = torch.bmm(k, q.unsqueeze(dim=2)) # (B x S x 128) * (B x 128 x 1) -> (B x 260 x 1)\n",
        "# print(\"Energy : \", energy.shape)\n",
        "# energy = energy.squeeze(dim=2)  # (B x S)\n",
        "# print(\"Energy : \", energy.shape)\n",
        "# masked_energy = energy * mask.int().float()\n",
        "# print(\"Masked Energy : \", energy.shape)\n",
        "# normal_energy = masked_energy / np.sqrt(k.size(2)) # (B, S)\n",
        "# print(\"Normal Energy : \", energy.shape)\n",
        "\n",
        "# attention = F.softmax(normal_energy, dim=1) # (B, S)\n",
        "# context = torch.bmm(attention.unsqueeze(1), v).squeeze(1)   # (B x 1 x S) * (B x S x 128) -> (B x S)\n",
        "# print(\"Attention : \", attention.shape)\n",
        "# print(\"Value : \", v.shape)\n",
        "# print(\"Context : \", torch.bmm(attention.unsqueeze(1), v).squeeze(1)\n",
        "\n",
        "# print(type(energy))\n",
        "# print(mask)\n",
        "# print(type(energy * mask.int().float()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czdBv4XB_xHR"
      },
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5V424ZumF6e"
      },
      "outputs": [],
      "source": [
        "# encoder = Encoder(13, 64)\n",
        "# k, v, l = encoder(x, lx)\n",
        "# print(k.shape, v.shape, l.shape)\n",
        "# print(k[0], v[0], l)\n",
        "\n",
        "# B, T, dkv = k.shape\n",
        "# mask =torch.arange(end=T).unsqueeze(0) <= l.unsqueeze(1)\n",
        "# mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcTC4cK95TYT"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step.\n",
        "    Thus we use LSTMCell instead of LSTM here.\n",
        "    The output from the last LSTMCell can be used as a query for calculating attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, decoder_hidden_dim, embed_dim, key_value_size=128): # 0 enforcing = False\n",
        "        super(Decoder, self).__init__()\n",
        "        self.key_value_size = key_value_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Hint: Be careful with the padding_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=letter2index['<eos>'])\n",
        "        self.lstm1 = nn.LSTMCell(input_size=embed_dim + key_value_size, hidden_size=decoder_hidden_dim)  #concatenate(transcript and context)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=decoder_hidden_dim, hidden_size=key_value_size)\n",
        "        self.attention = Attention()     \n",
        "        self.character_prob = nn.Linear(key_value_size + key_value_size, vocab_size) #: d_v -> vocab_size\n",
        "\n",
        "        # Weight tying\n",
        "        self.character_prob.weight = self.embedding.weight\n",
        "\n",
        "    def forward(self, key, value, encoder_len, y=None, mode='train', teacher_enforcing=0.95):\n",
        "        '''\n",
        "        Args:\n",
        "            key :(B, T, d_k) - Output of the Encoder (possibly from the Key projection layer)\n",
        "            value: (B, T, d_v) - Output of the Encoder (possibly from the Value projection layer)\n",
        "            y: (B, text_len) - Batch input of text with text_length\n",
        "            mode: Train or eval mode for teacher forcing\n",
        "        Return:\n",
        "            predictions: the character perdiction probability \n",
        "        '''\n",
        "        B, key_seq_max_len, key_value_size = key.shape\n",
        "\n",
        "        if mode == 'train':\n",
        "            max_len =  y.shape[1]\n",
        "            char_embeddings = self.embedding(y)\n",
        "        else:\n",
        "            max_len = 600\n",
        "\n",
        "        # TODO: Create the attention mask here (outside the for loop rather than inside) to aviod repetition\n",
        "        B, T, dkv = key.shape\n",
        "        mask = torch.arange(end=T).unsqueeze(0) <= encoder_len.unsqueeze(1) # range(T).shape(T, 1) <= len.shape(B, 1) i.e. max_len for each batch  \n",
        "        mask = mask.to(device) # (B x T)\n",
        "        \n",
        "        predictions = []\n",
        "        # This is the first input to the decoder. What should the fill_value be?\n",
        "        prediction = torch.full((B,), fill_value=letter2index['<sos>'], device=device)\n",
        "        # The length of hidden_states vector should depend on the number of LSTM Cells defined in init. The paper uses 2\n",
        "        hidden_states = [None, None] \n",
        "        \n",
        "        # TODO: Initialize the context\n",
        "        # context = torch.zeros((B, dkv)).to(device)\n",
        "        context = torch.ones((B, dkv)).to(device)\n",
        "\n",
        "        attention_plot = [] # this is for debugging\n",
        "\n",
        "        # print(\"char_embeddings : \", char_embeddings.shape)\n",
        "        # print(\"mask : \", mask.shape)\n",
        "        # print(\"prediction : \", prediction.shape)\n",
        "        # print(\"context : \", context.shape)\n",
        "        # print(\"\\n-----------------------LOOP STARTS\")\n",
        "\n",
        "        for i in range(max_len):\n",
        "            if mode == 'train':\n",
        "                # TODO: Implement Teacher Forcing\n",
        "                enforce = True if np.random.random() > (1 - teacher_enforcing) else False\n",
        "                if enforce:\n",
        "                    if i == 0:  # First time step\n",
        "                        # Hint: How did you initialize \"prediction\" variable above?\n",
        "                        char_embed = self.embedding(prediction)\n",
        "                    else:\n",
        "                        char_embed = char_embeddings[:, i-1, :]\n",
        "                        # Otherwise, feed the label of the **previous** time step\n",
        "                else:\n",
        "                    char_embed = self.embedding(prediction)\n",
        "            else:\n",
        "                char_embed = self.embedding(prediction)\n",
        "            \n",
        "            y_context = torch.cat([char_embed, context], dim=1)\n",
        "            # context and hidden states of lstm 1 from the previous time step should be fed\n",
        "            # Input (input feature, [hidden state, cell state])\n",
        "            # Output (next hidden state, next cell state)\n",
        "            # hidden states of lstm1 and hidden states of lstm2 from the previous time step should be fed\n",
        "            hidden_states[0] = self.lstm1(y_context, hidden_states[0])  \n",
        "            hidden_states[1] = self.lstm2(hidden_states[0][0], hidden_states[1])\n",
        "            query = hidden_states[1][0]\n",
        "            \n",
        "            # Compute attention from the output of the second LSTM Cell\n",
        "            context, attention = self.attention(query, key, value, mask)\n",
        "            # print(attention.shape)\n",
        "\n",
        "            attention_plot.append(attention[0].detach().cpu())\n",
        "            \n",
        "            output_context = torch.cat([query, context], dim=1)\n",
        "            prediction = self.character_prob(output_context)\n",
        "            predictions.append(prediction.unsqueeze(dim=1))\n",
        "            \n",
        "            prediction = prediction.argmax(dim=1)\n",
        "        \n",
        "        # Concatenate the attention and predictions to return\n",
        "        attentions = torch.stack(attention_plot, dim=0)\n",
        "        predictions = torch.cat(predictions, dim=1)\n",
        "        return predictions, attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOUZJUE6_0zv"
      },
      "source": [
        "#### Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d35FEZhz5Uhx"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(\n",
        "        self, \n",
        "        input_dim, \n",
        "        vocab_size, \n",
        "        encoder_hidden_dim, \n",
        "        decoder_hidden_dim, \n",
        "        embed_dim, \n",
        "        encoder_layers=1,\n",
        "        key_value_size=128, \n",
        "        teacher_enforcing=None, \n",
        "        dropout=0.2, \n",
        "        downsampling=2,\n",
        "        locked_dropout=0.0\n",
        "        ): \n",
        "        # enforcing : 0 = False and 0.95 = True with 95% prob\n",
        "        super(Seq2Seq,self).__init__()\n",
        "        self.encoder = Encoder(\n",
        "            input_dim, \n",
        "            encoder_hidden_dim, \n",
        "            encoder_layers, \n",
        "            key_value_size, \n",
        "            dropout, \n",
        "            downsampling, \n",
        "            locked_dropout\n",
        "            )\n",
        "        \n",
        "        self.decoder = Decoder(vocab_size, decoder_hidden_dim, embed_dim, key_value_size)\n",
        "\n",
        "        # for name, param in self.named_parameters():\n",
        "        #     if 'weight' in name:\n",
        "        #         nn.init.orthogonal_(param.data)\n",
        "        #     else:\n",
        "        #         nn.init.constant_(param.data, 0)\n",
        "\n",
        "    def forward(self, x, x_len, y=None, mode='train', teacher_enforcing=0.95):\n",
        "        key, value, encoder_len = self.encoder(x, x_len)\n",
        "        # print(\"Encoder Output : \", key.shape, value.shape, encoder_len.shape)\n",
        "        predictions, attentions = self.decoder(key, value, encoder_len, y=y, mode=mode, teacher_enforcing=teacher_enforcing)\n",
        "        # print(\"Decoder Output : \", predictions.shape, attentions.shape)\n",
        "\n",
        "        return predictions, attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9h-vz1fT09K"
      },
      "outputs": [],
      "source": [
        "# train_data = LibriSamples(data_path=r\"/content/toy_data\", partition='train')\n",
        "# # train_data = LibriSamples(data_path=r\"/content/actual_data\", partition='train')\n",
        "# train_loader = DataLoader(train_data, batch_size=128, shuffle=True, drop_last=False, collate_fn = train_data.collate_fn, num_workers=NUM_WORKERS)\n",
        "# for data in train_loader:\n",
        "#     x, y, lx, ly = data\n",
        "#     break\n",
        "\n",
        "# x, y, lx, ly = next(iter(train_loader))\n",
        "# print(\"Loader : \", x.shape, y.shape, lx.shape, ly.shape)\n",
        "\n",
        "model_params = {\n",
        "        'input_dim': 13, \n",
        "        'vocab_size': len(LETTER_LIST), \n",
        "        'encoder_hidden_dim': 256, \n",
        "        'encoder_layers': 1,\n",
        "        'decoder_hidden_dim': 512, \n",
        "        'embed_dim': 256, \n",
        "        'key_value_size': 128,\n",
        "        'teacher_enforcing': 0.90,\n",
        "        'dropout': 0.2,\n",
        "        'downsampling': 1,\n",
        "        'locked_dropout': 0.1\n",
        "    }  # Weight Tying = True, Label Smoothing = 0.2\n",
        "model = Seq2Seq(**model_params)\n",
        "model = model.to(device)\n",
        "# print(model)\n",
        "# print(x.shape, lx.shape, y.shape)\n",
        "# summary(model, x.to(device), lx, y.to(device))\n",
        "# preds, attentions = model(x.to(device), lx, y.to(device))\n",
        "# print(preds.shape, attentions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHMzR6fLht5n"
      },
      "source": [
        "# Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QARct4H1AO87"
      },
      "outputs": [],
      "source": [
        "def beam_search():\n",
        "    pass\n",
        "\n",
        "def convert_to_string(batch_indices):\n",
        "    transcripts = []\n",
        "    for indices in batch_indices:\n",
        "        transcript_string = \"\"\n",
        "        for k in indices:\n",
        "            if k == letter2index['<eos>']:\n",
        "                break\n",
        "            if k == letter2index['<sos>']:\n",
        "                continue\n",
        "            else:\n",
        "                transcript_string += index2letter[k]\n",
        "        transcripts.append(transcript_string)\n",
        "    \n",
        "    return transcripts\n",
        "\n",
        "def calculate_levenshtein(predictions, target):\n",
        "    batch_size = predictions.shape[0]\n",
        "\n",
        "    predictions_text = convert_to_string(predictions.argmax(-1).detach().cpu().numpy())\n",
        "    target_text = convert_to_string(target.detach().cpu().numpy())\n",
        "\n",
        "    dist = 0\n",
        "    for b in range(batch_size):\n",
        "        dist += Levenshtein.distance(predictions_text[b], target_text[b])\n",
        "    dist/=batch_size\n",
        "    return dist, len(predictions_text)\n",
        "\n",
        "def model_warm_up(trate, lev):\n",
        "    if lev < 50 and trate > 0.5:\n",
        "        trate *= 0.96\n",
        "    return trate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F5gFb_Sh-5s"
      },
      "outputs": [],
      "source": [
        "class ModelSetup:\n",
        "    def __init__(self, config, save_path, toy=False):\n",
        "        self.config = config\n",
        "        self.log = config['log']\n",
        "        self.save = config['save']\n",
        "        self.toy = toy\n",
        "        self.teacher_enforcing_rate = config[\"seq2seq\"][\"teacher_enforcing\"]\n",
        "        print(f\"Saving : {self.save} and Logging : {self.log}\")\n",
        "\n",
        "        self.SAVE_DIR = save_path\n",
        "        self.DATA_DIR = r\"/content/actual_data\" if not toy else r\"/content/toy_data\"\n",
        "\n",
        "    def __gen_model_name(self):\n",
        "        # Generate a model name based on config\n",
        "        save_name = ''\n",
        "\n",
        "        for key, val in self.config.items():\n",
        "            abbr = key[0] if len(key) > 2 else key\n",
        "            if key == 'optim':\n",
        "                data = 'lr' + str(val[\"lr\"])\n",
        "                save_name += data\n",
        "                break\n",
        "            elif key == 'sched':\n",
        "                continue\n",
        "            elif key == 'seq2seq':\n",
        "                seqdata = 'S2S'\n",
        "                for k, v in val.items():\n",
        "                    seqdata += '-' + k[0] +str(v)\n",
        "                    save_name += seqdata \n",
        "            elif key == '':\n",
        "                save_name += abbr + str(val)\n",
        "                for key, val in self.config['seq2seq'].items():\n",
        "                    save_name += '-' + str(val)\n",
        "                save_name += '_'\n",
        "            elif not isinstance(val, dict):\n",
        "                data = abbr + str(val) + '_'\n",
        "                save_name += data\n",
        "                \n",
        "        if self.config['randomize']:\n",
        "            save_name = save_name + \"-v\" + str(np.random.randint(10, 1000))\n",
        "        print(\"\\nModel Name: \", save_name)\n",
        "        self.model_name = save_name\n",
        "\n",
        "    def __save_model_params(self, continue_train):\n",
        "        # Create Model Directory\n",
        "        save_path = os.path.join(self.SAVE_DIR, self.model_name)\n",
        "        if not continue_train:\n",
        "            try:\n",
        "                os.mkdir(save_path)\n",
        "            except FileExistsError:\n",
        "                d = input(\"Model name already exists. Delete existing model? (y/n)\")\n",
        "                if d == 'y':\n",
        "                    import shutil\n",
        "                    shutil.rmtree(save_path)\n",
        "                    os.mkdir(save_path)\n",
        "                else:\n",
        "                    print(\"Exiting!\")\n",
        "                    exit(0)\n",
        "                    return None\n",
        "\n",
        "            os.mkdir(os.path.join(save_path, 'Checkpoints'))\n",
        "            # Saving Model Configuration\n",
        "            with open(os.path.join(save_path, 'model_config.yaml'), 'w') as metadata:\n",
        "                yaml.dump({'Experiment': self.config['']}, metadata, indent=4, default_flow_style=False)\n",
        "                yaml.dump(self.config, metadata, indent=4, default_flow_style=False)\n",
        "            print(\"Model to be saved at: \", save_path)\n",
        "        self.model_path = save_path\n",
        "\n",
        "    def __continue_train(self, chkpt):\n",
        "        self.chkpt = chkpt\n",
        "        assert chkpt is not None\n",
        "\n",
        "        chkpt_path = os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(chkpt) + '.pth')\n",
        "\n",
        "        try:\n",
        "            checkpoint = torch.load(chkpt_path)\n",
        "        except FileNotFoundError:\n",
        "            print(\"Checkpoint not found in the directory!\")\n",
        "            print(\"Incorrect: \", chkpt_path)\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    def dataloaders(self): \n",
        "        self.train_data = LibriSamples(data_path=self.DATA_DIR, partition='train')\n",
        "        self.val_data = LibriSamples(data_path=self.DATA_DIR, partition='dev')\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_data, \n",
        "            batch_size=self.config['batch_size'],\n",
        "            shuffle=True,\n",
        "            drop_last=True, \n",
        "            collate_fn = self.train_data.collate_fn, \n",
        "            num_workers=NUM_WORKERS,\n",
        "            pin_memory=True\n",
        "            ) \n",
        "        self.val_loader = DataLoader(\n",
        "            self.val_data, \n",
        "            batch_size=self.config['batch_size'], \n",
        "            shuffle=False, \n",
        "            drop_last=True, \n",
        "            collate_fn = self.val_data.collate_fn, \n",
        "            num_workers=NUM_WORKERS,\n",
        "            pin_memory=True\n",
        "            )\n",
        "\n",
        "    def save_checkpoint(self, epoch, model, optimizer, loss):\n",
        "        print(\"Saving Checkpoint!\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, os.path.join(self.model_path, 'Checkpoints', 'chkpt_' + str(epoch) + '.pth'))\n",
        "\n",
        "    def save_model(self, epoch=None, onnx=False):\n",
        "        print(\"Saving Model!\")\n",
        "        try:\n",
        "            if not self.save:\n",
        "                self.__save_model_params()\n",
        "            if epoch != self.config[\"epochs\"] or epoch is None:\n",
        "                name = os.path.join(self.model_path, \"model_\" + str(epoch) + \".pth\")\n",
        "            else:\n",
        "                name = os.path.join(self.model_path, \"model\" + \".pth\")\n",
        "            torch.save(self.model.state_dict(), name)\n",
        "        except:\n",
        "            print(\"Model couldn't be saved!\")\n",
        "\n",
        "    def setup(self, continue_train=False, chkpt=None):\n",
        "        header(\"Model Setup\")\n",
        "\n",
        "        # Model\n",
        "        self.model = Seq2Seq(**self.config[\"seq2seq\"]).to(device)\n",
        "        # summary(self.model, x.to(device), lx, y.to(device))\n",
        "        \n",
        "        self.__gen_model_name()\n",
        "        if self.save: self.__save_model_params(continue_train)\n",
        "\n",
        "        # Loss\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='none', label_smoothing=0.1)\n",
        "        \n",
        "        # Optimizer\n",
        "        if self.config[\"optimizer\"] == 'SGD':\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), **self.config['optim'])\n",
        "        elif self.config[\"optimizer\"] == \"Adam\":\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), **self.config['optim'])\n",
        "        elif self.config[\"optimizer\"] == \"AdamW\":\n",
        "            self.optimizer = optim.AdamW(self.model.parameters(), **self.config['optim'])\n",
        "        elif self.config[\"optimizer\"] == \"AdamP\":\n",
        "            self.optimizer = AdamP(self.model.parameters(), **self.config['optim'])\n",
        "\n",
        "        self.chkpt = 0\n",
        "        if continue_train: \n",
        "            self.__continue_train(chkpt)\n",
        "\n",
        "        # Scheduler\n",
        "        if self.config[\"scheduler\"] == 'CALR':\n",
        "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=(len(self.train_loader) * self.config['epochs']))\n",
        "        elif self.config[\"scheduler\"] == 'RLRP':\n",
        "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, **self.config['sched'])\n",
        "        elif self.config[\"scheduler\"] == 'MultiStep':\n",
        "            self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, **self.config['sched'])\n",
        "        elif self.config[\"scheduler\"] == None:\n",
        "            self.scheduler = None\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    # Training Loop\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        batch_bar = tqdm(total=len(self.train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "        running_loss = 0\n",
        "        for i, (x, y, x_len, y_len) in enumerate(self.train_loader):\n",
        "            torch.cuda.empty_cache()\n",
        "            self.optimizer.zero_grad(set_to_none=True)\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "            y_len = torch.tensor(y_len).to(device, non_blocking=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                predictions, attentions = self.model(x, x_len, y, 'train', self.teacher_enforcing_rate)\n",
        "            \n",
        "                # 3) Generate a mask based on target length. This is to mark padded elements\n",
        "                # so that we can exclude them from computing loss. Ensure that the mask is on the device and is the correct shape.\n",
        "                max_len = torch.max(y_len)\n",
        "                inst = y_len.shape[0]\n",
        "                mask = torch.arange(0, max_len).repeat(inst, 1).to(device) >= y_len.reshape(inst, 1)\n",
        "                # mask = torch.arange(0, max_len).repeat(inst, 1).to(device) < y_len.unsqueeze(1).expand(inst, max_len)\n",
        "                # mask = mask.long()\n",
        "                mask = mask.to(device, non_blocking=True)\n",
        "\n",
        "                loss = self.criterion(predictions.permute(0, 2, 1), y)\n",
        "                # loss = self.criterion(predictions.view(-1, predictions.shape[2]), y.view(-1))\n",
        "                \n",
        "                masked_loss = loss.masked_fill_(mask,0)\n",
        "                masked_loss = torch.mean(masked_loss)\n",
        "                # masked_loss = torch.sum(loss * mask.view(-1)) / torch.sum(mask)\n",
        "\n",
        "                running_loss += masked_loss.item()\n",
        "\n",
        "            # 5) backprop\n",
        "            # Optional: Gradient clipping\n",
        "            # When computing Levenshtein distance, make sure you truncate prediction/target\n",
        "\n",
        "            if i % 50 == 0 and self.toy: plot_attention(attentions)\n",
        "            if self.config[\"scheduler\"] in ['CALR', 'MultiStep']: self.scheduler.step()\n",
        "            batch_bar.set_postfix(\n",
        "                loss=\"{:.04f}\".format(float(running_loss / (i + 1))),\n",
        "                lr=\"{:.04f}\".format(float(self.optimizer.param_groups[0]['lr']))\n",
        "                )\n",
        "            \n",
        "            self.scaler.scale(masked_loss).backward()\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "\n",
        "            batch_bar.update()\n",
        "            \n",
        "        batch_bar.close()\n",
        "        # plot_attention(attentions)\n",
        "        trainlos = float(running_loss / len(self.train_loader))\n",
        "        trainlra = float(self.optimizer.param_groups[0]['lr'])\n",
        "        print(\"Updated Teacher Enforcing Rate = \", self.teacher_enforcing_rate)\n",
        "\n",
        "        return trainlos, trainlra\n",
        "        \n",
        "    # Validation Function\n",
        "    def validate(self, model):\n",
        "        model.eval()\n",
        "        batch_bar = tqdm(total=len(self.val_loader), position=0, leave=False, desc='Val')\n",
        "        total_levenshtein = 0\n",
        "        num_strings = 0\n",
        "        for i, (x, y, x_len, y_len) in enumerate(self.val_loader):\n",
        "            torch.cuda.empty_cache()\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            y_len = torch.tensor(y_len).to(device)\n",
        "\n",
        "            with torch.no_grad():    \n",
        "                val_preds, val_attentions = model(x, x_len, y, mode='eval')\n",
        "                ld, len_preds = calculate_levenshtein(val_preds, y)\n",
        "                total_levenshtein += ld\n",
        "                num_strings += len_preds\n",
        "            \n",
        "            batch_bar.set_postfix(LD=\"{:.04f}\".format(float(total_levenshtein / (i + 1))))\n",
        "            batch_bar.update()\n",
        "\n",
        "        batch_bar.close()\n",
        "        val_lev = float(total_levenshtein / (len(self.val_loader) + 1))\n",
        "\n",
        "        return val_lev\n",
        "\n",
        "    # Training Function\n",
        "    def training(self, continue_train=False, save_freq=2):\n",
        "        header(\"Training\")\n",
        "        epochs = self.config['epochs']\n",
        "        batch_size = self.config['batch_size']\n",
        "\n",
        "        if self.log:\n",
        "            wandb.init(project=\"hw4-chinmay\", entity=\"dl-study-group\", config=self.config, name=self.model_name)\n",
        "            wandb.watch(self.model, criterion=self.criterion, log=\"all\", log_freq=batch_size, idx=None)\n",
        "\n",
        "        delta_time = datetime.timedelta(seconds = 0)\n",
        "        for epoch in range(self.chkpt, epochs + self.chkpt):\n",
        "            print(\"\\nEpoch \", epoch + 1)\n",
        "            start_time = time.time()\n",
        "\n",
        "            trainlos, trainlra = self.train()\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} | Train Loss {trainlos:.04f} | Learning Rate {trainlra:.04f}\")\n",
        "\n",
        "            lev = self.validate(self.model)\n",
        "            print(\"\\nValidation LD: {:.04f}\".format(lev))\n",
        "\n",
        "            if self.config[\"scheduler\"] == 'RLRP': self.scheduler.step(lev)\n",
        "            if self.log: wandb.log({\"Training Loss\": trainlos, \"Learning Rate\": trainlra, \"Validation LD\": lev})\n",
        "\n",
        "            tr = self.teacher_enforcing_rate\n",
        "            lr = self.optimizer.param_groups[0]['lr']\n",
        "            self.teacher_enforcing_rate = model_warm_up(tr, lev)\n",
        "\n",
        "            delta_time += datetime.timedelta(seconds = (time.time() - start_time))\n",
        "            print(f\"Time lapsed = {str(delta_time)}\")\n",
        "            print(f\"Time left = {str(delta_time * (epochs - epoch - 1) / (epoch + 1))}\")\n",
        "\n",
        "            if self.save:\n",
        "                if epoch % save_freq == 0: \n",
        "                    self.save_model(epoch)\n",
        "                self.save_checkpoint(epoch, self.model, self.optimizer, trainlos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coyD8lmah-5u"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "309gjJwTSBQG",
        "outputId": "ceae662d-87f4-416a-9bfe-a9580127eb3e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nTips for passing A from B (from easy to hard):\\n** You need to implement all of these yourself without utilizing any library **\\n(1) Increase model capacity. E.g. increase num_layer of lstm\\n(2) LR and Teacher Forcing are also very important, you can tune them or their scheduler as well. \\nDo NOT change lr or tf during the warm-up stage!\\n(3) Weight tying\\n________________________________________________________________________________________________________\\n(4) Locked Dropout - insert between the plstm layers\\n(5) Pre-training decoder or train an LM to help make predictions\\n(5) Pre-training decoder to speed up the convergence: \\n    disable your encoder and only train the decoder like train a language model\\n(6) Better weight initialization technique\\n(7) Batch Norm between plstm. You definitely can try other positions as well\\n(8) Data Augmentation. Time-masking, frequency masking\\n(9) Weight smoothing (avg the last few epoch's weight)\\n(10) You can try CNN + Maxpooling (Avg). \\nSome students replace the entire plstm blocks with it and some just combine them together.\\n(11) Beam Search\\n\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Tips for passing A from B (from easy to hard):\n",
        "** You need to implement all of these yourself without utilizing any library **\n",
        "(1) Increase model capacity. E.g. increase num_layer of lstm\n",
        "(2) LR and Teacher Forcing are also very important, you can tune them or their scheduler as well. \n",
        "Do NOT change lr or tf during the warm-up stage!\n",
        "(3) Weight tying\n",
        "________________________________________________________________________________________________________\n",
        "(4) Locked Dropout - insert between the plstm layers\n",
        "(5) Pre-training decoder or train an LM to help make predictions\n",
        "(5) Pre-training decoder to speed up the convergence: \n",
        "    disable your encoder and only train the decoder like train a language model\n",
        "(6) Better weight initialization technique\n",
        "(7) Batch Norm between plstm. You definitely can try other positions as well\n",
        "(8) Data Augmentation. Time-masking, frequency masking\n",
        "(9) Weight smoothing (avg the last few epoch's weight)\n",
        "(10) You can try CNN + Maxpooling (Avg). \n",
        "Some students replace the entire plstm blocks with it and some just combine them together.\n",
        "(11) Beam Search\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GnbyHSrRwU5"
      },
      "source": [
        "#### Experiments\n",
        "Baseline : E256-1 | D512-256 | A128 | LSTMDrop 0.2 | Down 2 | Locked 0.0 | TF1.0 (50 LD, 0.95) | WeightTying\n",
        "\n",
        "1. LSTM Layers - 2\n",
        "2. Locked Dropout - 0.4 (Good)\n",
        "\n",
        "x. Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgmRkIcLh-5u",
        "outputId": "bf4f14ec-e2f9-4e2f-a16c-cc14edadcc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Couldn't delete!  name 'attentionsr' is not defined\n",
            "Saving : True and Logging : True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MFCC: 100%|██████████| 28539/28539 [00:09<00:00, 3047.50it/s]\n",
            "Transcript: 100%|██████████| 28539/28539 [00:09<00:00, 3058.23it/s]\n",
            "MFCC: 100%|██████████| 2703/2703 [00:00<00:00, 3439.17it/s]\n",
            "Transcript: 100%|██████████| 2703/2703 [00:00<00:00, 3397.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "\t\t\t\tMODEL SETUP\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Model Name:  LAS-bigger-highlr-13-30-512-2-512-512-256-1.0-2-0.0-0.4_b64_e50_sMultiStep_oAdamP_lr0.001\n",
            "Model name already exists. Delete existing model? (y/n)y\n",
            "Model to be saved at:  /content/cmudrive/IDL/hw4-ablations/LAS-bigger-highlr-13-30-512-2-512-512-256-1.0-2-0.0-0.4_b64_e50_sMultiStep_oAdamP_lr0.001\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    '':'LAS-bigger-highlr',\n",
        "    'batch_size': 64,\n",
        "    'epochs': 50,\n",
        "    'scheduler': 'MultiStep',        # CALR, RLRP, MultiStep\n",
        "    'optimizer': 'AdamP',            # SGD, Adam, AdamW, AdamP\n",
        "    'sched': {'milestones' :[20*445, 35*445], 'gamma':0.5},   # MultiStep\n",
        "    # 'sched': {'mode':'min', 'factor':0.5, 'patience':15, 'threshold':0.25, 'cooldown':15},  #RLRP     \n",
        "    'optim': {'lr': 0.001, 'weight_decay': 5e-7},\n",
        "    'seq2seq': {\n",
        "        'input_dim': 13,\n",
        "        'vocab_size': len(LETTER_LIST),\n",
        "        'encoder_hidden_dim': 512, \n",
        "        'encoder_layers': 2,\n",
        "        'decoder_hidden_dim': 512,\n",
        "        'embed_dim': 512,\n",
        "        'key_value_size': 256,\n",
        "        'teacher_enforcing': 1.0,\n",
        "        'downsampling': 2,\n",
        "        'dropout': 0.0,\n",
        "        'locked_dropout': 0.4\n",
        "        },  \n",
        "    'weight_tying' : True,\n",
        "    'label_smoothing' : 0.15,\n",
        "    'save': True,\n",
        "    'log': True,\n",
        "    'randomize': False,\n",
        "}\n",
        "\n",
        "try:\n",
        "    del attentionsr \n",
        "except Exception as e:\n",
        "    print(\"Couldn't delete! \", e)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "folder_path = r'/content/cmudrive/IDL/hw4-ablations'\n",
        "attentionsr = ModelSetup(config, save_path = folder_path, toy=False)\n",
        "attentionsr.dataloaders()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "attentionsr.setup()\n",
        "\n",
        "# *Continue Training\n",
        "# attentionsr.setup(continue_train=True, chkpt=16)\n",
        "# attentionsr.training(continue_train=True)\n",
        "# attentionsr.save_model()\n",
        "# if attentionsr.log: wandb.finish()\n",
        "# latest_model_name = attentionsr.model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ae935ae5a1304c369411cd27d332cc28",
            "c82be09816fa489da1dc4b8c953c4e45",
            "d8d44907640c44b486b3e5bc089f5068",
            "b03f8b339d584092a6bee34d41409536",
            "0e801130e0aa4ac0bd9b7a69776d6451",
            "6d70b772d2a14060a5249201200f1232",
            "b50daa4515df4ec89ddc413943de3cba",
            "5694aa9ca2a84b85948a393910bf5cd8"
          ]
        },
        "id": "4Onbh0Vih-5u",
        "outputId": "17ab64a9-c9ba-40e7-87e0-a441ddcad68e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "\t\t\t\tTRAINING\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnefario7\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/wandb/run-20220501_172226-3662aiu9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/dl-study-group/hw4-chinmay/runs/3662aiu9\" target=\"_blank\">LAS-bigger-highlr-13-30-512-2-512-512-256-1.0-2-0.0-0.4_b64_e50_sMultiStep_oAdamP_lr0.001</a></strong> to <a href=\"https://wandb.ai/dl-study-group/hw4-chinmay\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 1/50 | Train Loss 1.4038 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 513.2613\n",
            "Time lapsed = 0:18:28.095490\n",
            "Time left = 15:04:56.679010\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 2/50 | Train Loss 1.1514 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 528.1199\n",
            "Time lapsed = 0:36:59.469874\n",
            "Time left = 14:47:47.276976\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 3/50 | Train Loss 1.1113 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 519.9233\n",
            "Time lapsed = 0:55:28.281077\n",
            "Time left = 14:29:03.070206\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 4/50 | Train Loss 1.0881 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 523.5607\n",
            "Time lapsed = 1:13:57.797670\n",
            "Time left = 14:10:34.673205\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 5/50 | Train Loss 1.0689 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 515.8456\n",
            "Time lapsed = 1:32:30.965177\n",
            "Time left = 13:52:38.686593\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 6/50 | Train Loss 1.0598 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 519.8786\n",
            "Time lapsed = 1:51:00.057700\n",
            "Time left = 13:34:00.423133\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 7/50 | Train Loss 1.0525 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 524.1490\n",
            "Time lapsed = 2:09:30.255151\n",
            "Time left = 13:15:31.567356\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 8/50 | Train Loss 1.0421 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 517.6152\n",
            "Time lapsed = 2:27:59.272852\n",
            "Time left = 12:56:56.182473\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  9\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 9/50 | Train Loss 1.0344 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 515.8590\n",
            "Time lapsed = 2:46:33.115009\n",
            "Time left = 12:38:44.190597\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 10/50 | Train Loss 1.0242 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 514.8129\n",
            "Time lapsed = 3:05:08.418569\n",
            "Time left = 12:20:33.674276\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  11\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 11/50 | Train Loss 1.0002 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 512.7591\n",
            "Time lapsed = 3:23:45.055302\n",
            "Time left = 12:02:23.377889\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 12/50 | Train Loss 0.7354 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 188.2395\n",
            "Time lapsed = 3:42:23.500688\n",
            "Time left = 11:44:14.418845\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  13\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 13/50 | Train Loss 0.5589 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 99.5338\n",
            "Time lapsed = 4:00:59.264831\n",
            "Time left = 11:25:53.292211\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 14/50 | Train Loss 0.5116 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 51.9575\n",
            "Time lapsed = 4:19:37.911529\n",
            "Time left = 11:07:37.486789\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  1.0\n",
            "Epoch 15/50 | Train Loss 0.4876 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 45.4473\n",
            "Time lapsed = 4:38:16.381510\n",
            "Time left = 10:49:18.223523\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  16\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.96\n",
            "Epoch 16/50 | Train Loss 0.4765 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 32.6900\n",
            "Time lapsed = 4:56:51.640249\n",
            "Time left = 10:30:49.735529\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  17\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.9216\n",
            "Epoch 17/50 | Train Loss 0.4680 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 30.0527\n",
            "Time lapsed = 5:15:20.672574\n",
            "Time left = 10:12:08.364408\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  18\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.884736\n",
            "Epoch 18/50 | Train Loss 0.4635 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 26.0145\n",
            "Time lapsed = 5:33:41.997047\n",
            "Time left = 9:53:14.661417\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  19\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.84934656\n",
            "Epoch 19/50 | Train Loss 0.4590 | Learning Rate 0.0010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 20.6722\n",
            "Time lapsed = 5:51:56.990322\n",
            "Time left = 9:34:14.036841\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.8153726976\n",
            "Epoch 20/50 | Train Loss 0.4530 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 21.7551\n",
            "Time lapsed = 6:10:06.573516\n",
            "Time left = 9:15:09.860274\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  21\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.782757789696\n",
            "Epoch 21/50 | Train Loss 0.4392 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 16.0247\n",
            "Time lapsed = 6:28:10.055966\n",
            "Time left = 8:56:02.458239\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  22\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.7514474781081599\n",
            "Epoch 22/50 | Train Loss 0.4333 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 13.5763\n",
            "Time lapsed = 6:46:08.764221\n",
            "Time left = 8:36:54.790827\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  23\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.7213895789838335\n",
            "Epoch 23/50 | Train Loss 0.4316 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 15.2039\n",
            "Time lapsed = 7:04:01.527306\n",
            "Time left = 8:17:46.140751\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  24\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.6925339958244802\n",
            "Epoch 24/50 | Train Loss 0.4306 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 15.0734\n",
            "Time lapsed = 7:21:47.817279\n",
            "Time left = 7:58:36.802052\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  25\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.6648326359915009\n",
            "Epoch 25/50 | Train Loss 0.4295 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 14.3183\n",
            "Time lapsed = 7:39:32.327807\n",
            "Time left = 7:39:32.327807\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  26\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.6382393305518408\n",
            "Epoch 26/50 | Train Loss 0.4316 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 14.7620\n",
            "Time lapsed = 7:57:14.847956\n",
            "Time left = 7:20:32.167344\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  27\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.6127097573297672\n",
            "Epoch 27/50 | Train Loss 0.4308 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 15.0905\n",
            "Time lapsed = 8:14:48.510997\n",
            "Time left = 7:01:30.213072\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  28\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.5882013670365764\n",
            "Epoch 28/50 | Train Loss 0.4310 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 14.2812\n",
            "Time lapsed = 8:32:19.282016\n",
            "Time left = 6:42:32.293013\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  29\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.5646733123551133\n",
            "Epoch 29/50 | Train Loss 0.4310 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 14.9844\n",
            "Time lapsed = 8:49:50.717638\n",
            "Time left = 6:23:40.864496\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  30\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.5420863798609088\n",
            "Epoch 30/50 | Train Loss 0.4303 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 12.9764\n",
            "Time lapsed = 9:07:14.881341\n",
            "Time left = 6:04:49.920894\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  31\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.5204029246664724\n",
            "Epoch 31/50 | Train Loss 0.4283 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 14.8608\n",
            "Time lapsed = 9:24:36.069362\n",
            "Time left = 5:46:02.752190\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  32\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 32/50 | Train Loss 0.4314 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 13.5124\n",
            "Time lapsed = 9:41:54.024388\n",
            "Time left = 5:27:19.138718\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  33\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 33/50 | Train Loss 0.4277 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 13.8427\n",
            "Time lapsed = 9:59:11.880799\n",
            "Time left = 5:08:40.665866\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  34\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 34/50 | Train Loss 0.4292 | Learning Rate 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 13.8350\n",
            "Time lapsed = 10:16:16.151918\n",
            "Time left = 4:50:00.542079\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  35\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 35/50 | Train Loss 0.4267 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 14.4953\n",
            "Time lapsed = 10:33:19.850940\n",
            "Time left = 4:31:25.650403\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  36\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 36/50 | Train Loss 0.4224 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 10.2696\n",
            "Time lapsed = 10:50:23.151340\n",
            "Time left = 4:12:55.669966\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  37\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 37/50 | Train Loss 0.4197 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 10.5607\n",
            "Time lapsed = 11:07:34.985841\n",
            "Time left = 3:54:33.373404\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  38\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 38/50 | Train Loss 0.4186 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 11.1519\n",
            "Time lapsed = 11:24:52.116417\n",
            "Time left = 3:36:16.457816\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  39\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 39/50 | Train Loss 0.4173 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 10.8874\n",
            "Time lapsed = 11:42:13.080962\n",
            "Time left = 3:18:03.689502\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  40\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 40/50 | Train Loss 0.4181 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 11.4658\n",
            "Time lapsed = 11:59:34.429204\n",
            "Time left = 2:59:53.607301\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  41\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 41/50 | Train Loss 0.4176 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 10.1926\n",
            "Time lapsed = 12:16:53.945465\n",
            "Time left = 2:41:45.500224\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  42\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 42/50 | Train Loss 0.4189 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 11.7191\n",
            "Time lapsed = 12:34:13.942932\n",
            "Time left = 2:23:39.798654\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  43\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 43/50 | Train Loss 0.4195 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 11.9259\n",
            "Time lapsed = 12:51:32.455402\n",
            "Time left = 2:05:35.981112\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  44\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 44/50 | Train Loss 0.4196 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 13.1871\n",
            "Time lapsed = 13:08:50.315293\n",
            "Time left = 1:47:34.133904\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  45\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 45/50 | Train Loss 0.4187 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 12.6246\n",
            "Time lapsed = 13:26:07.844049\n",
            "Time left = 1:29:34.204894\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  46\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 46/50 | Train Loss 0.4172 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 12.3234\n",
            "Time lapsed = 13:43:27.545695\n",
            "Time left = 1:11:36.308321\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  47\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 47/50 | Train Loss 0.4184 | Learning Rate 0.0003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation LD: 12.2478\n",
            "Time lapsed = 14:00:43.698540\n",
            "Time left = 0:53:39.810545\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 48/50 | Train Loss 0.4183 | Learning Rate 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation LD: 11.6708\n",
            "Time lapsed = 14:18:01.394513\n",
            "Time left = 0:35:45.058105\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 49/50 | Train Loss 0.4178 | Learning Rate 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation LD: 12.9411\n",
            "Time lapsed = 14:35:18.463898\n",
            "Time left = 0:17:51.805386\n",
            "Saving Model!\n",
            "Saving Checkpoint!\n",
            "\n",
            "Epoch  50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Teacher Enforcing Rate =  0.4995868076798135\n",
            "Epoch 50/50 | Train Loss 0.4195 | Learning Rate 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation LD: 12.5967\n",
            "Time lapsed = 14:52:33.647741\n",
            "Time left = 0:00:00\n",
            "Saving Checkpoint!\n",
            "Saving Model!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae935ae5a1304c369411cd27d332cc28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>████████████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Loss</td><td>█▆▆▆▆▆▅▅▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation LD</td><td>█████████▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>0.00025</td></tr><tr><td>Training Loss</td><td>0.41947</td></tr><tr><td>Validation LD</td><td>12.59666</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">LAS-bigger-highlr-13-30-512-2-512-512-256-1.0-2-0.0-0.4_b64_e50_sMultiStep_oAdamP_lr0.001</strong>: <a href=\"https://wandb.ai/dl-study-group/hw4-chinmay/runs/3662aiu9\" target=\"_blank\">https://wandb.ai/dl-study-group/hw4-chinmay/runs/3662aiu9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code></code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "attentionsr.training()\n",
        "attentionsr.save_model()\n",
        "\n",
        "if attentionsr.log: wandb.finish()\n",
        "latest_model_name = attentionsr.model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ZO_vhADcMB"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JD_C2EoGdzj"
      },
      "outputs": [],
      "source": [
        "class SpeechInference():\n",
        "    def __init__(self, bw=30):\n",
        "        self.drive_dir = r'/content/cmudrive/IDL'\n",
        "\n",
        "        # Dataset and dataloader\n",
        "        self.test_data = LibriSamples(partition='test')\n",
        "        self.test_loader = DataLoader(\n",
        "            self.test_data, \n",
        "            batch_size=128, \n",
        "            shuffle=False, \n",
        "            drop_last=False,\n",
        "            collate_fn = self.test_data.collate_fn\n",
        "            )\n",
        "        \n",
        "        # self.decoder = CTCBeamDecoder(\n",
        "        #         LETTER_LIST,\n",
        "        #         blank_id=0,\n",
        "        #         log_probs_input=True,\n",
        "        #         beam_width=bw, \n",
        "        #         cutoff_top_n= 40,\n",
        "        #         cutoff_prob= 1.0\n",
        "        # )\n",
        "        \n",
        "    def get_predictions(self, model):\n",
        "        predictions = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (x, x_len) in enumerate(tqdm(self.test_loader, desc=\"Test\")):\n",
        "                torch.cuda.empty_cache()\n",
        "                x = x.to(device)\n",
        "                output, _ = model(x, x_len, mode='eval')\n",
        "\n",
        "                # BEAM SEARCH\n",
        "                # beam_results, beam_scores, timesteps, out_lens = self.decoder.decode(output, seq_lens=output_len)\n",
        "                # for b in range(output.shape[0]):\n",
        "                #     predictions.append(convert_to_string(beam_results[b][0], out_lens[b][0], self.phoneme_map, self.phonemes))\n",
        "\n",
        "                # GREEDY SEARCH\n",
        "                greedy_output = output.argmax(-1).detach().cpu().numpy()\n",
        "                predictions.extend(convert_to_string(greedy_output))\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def load_model(self): \n",
        "        self.main_path = os.path.join(self.drive_dir, self.model_type, self.model_name)\n",
        "        meta_path = os.path.join(self.main_path, 'model_config.yaml')\n",
        "        model_path = os.path.join(self.main_path, self.model_file)\n",
        "        \n",
        "        with open(meta_path, 'r') as meta:\n",
        "            args = yaml.safe_load(meta)\n",
        "\n",
        "        model = Seq2Seq(**args[\"seq2seq\"]).to(device)\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        return model\n",
        "\n",
        "    def simple_inference(self, model_name, model_file, model_type):\n",
        "        print(\"Running inference...\")\n",
        "        self.model_type = model_type\n",
        "        self.model_name = model_name\n",
        "        self.model_file = model_file\n",
        "        self.timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        model = self.load_model()\n",
        "        preds = self.get_predictions(model)\n",
        "        return preds\n",
        "\n",
        "    def write_csv(self, path, preds):\n",
        "        with open(path, 'w') as f:\n",
        "            csvwrite = csv.writer(f)\n",
        "            csvwrite.writerow(['id', 'predictions'])\n",
        "            for i in range(len(preds)):\n",
        "                csvwrite.writerow([i, preds[i]])\n",
        "\n",
        "    def generate_submission(self, save_path, preds): \n",
        "        print(\"Generating Submission CSV...\")\n",
        "        sub_dir = os.path.join(self.drive_dir, save_path, self.timestamp)\n",
        "        mod_dir = os.path.join(self.main_path, self.timestamp)\n",
        "\n",
        "        try:\n",
        "            os.mkdir(sub_dir)\n",
        "            os.mkdir(mod_dir)\n",
        "        except:\n",
        "            print(\"Couldn't create folder for submission.csv\")\n",
        "            \n",
        "        sub_path = os.path.join(sub_dir, 'submission.csv')\n",
        "        mod_path = os.path.join(mod_dir, 'submission.csv')\n",
        "\n",
        "        self.write_csv(sub_path, preds)\n",
        "        self.write_csv(mod_path, preds)\n",
        "\n",
        "        print(f\"File saved at : {sub_path}\")\n",
        "        print(f\"File saved in model at : {mod_path}\")\n",
        "        return sub_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msfaIJAEDfEu",
        "outputId": "2d3ef561-017c-43ae-bba4-33cded426f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MFCC: 100%|██████████| 2620/2620 [00:01<00:00, 1942.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 21/21 [00:42<00:00,  2.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Submission CSV...\n",
            "File saved at : /content/cmudrive/IDL/hw4-submission/2022-05-03_06-53-54/submission.csv\n",
            "File saved in model at : /content/cmudrive/IDL/hw4-ablations/LAS-bigger-highlr-13-30-512-2-512-512-256-1.0-2-0.0-0.4_b64_e50_sMultiStep_oAdamP_lr0.001/2022-05-03_06-53-54/submission.csv\n",
            "\n",
            "\n",
            "Submission File :  /content/cmudrive/IDL/hw4-submission/2022-05-03_06-53-54/submission.csv\n"
          ]
        }
      ],
      "source": [
        "model_name = 'LAS-bigger-highlr-13-30-512-2-512-512-256-1.0-2-0.0-0.4_b64_e50_sMultiStep_oAdamP_lr0.001'\n",
        "# model_name = latest_model_name\n",
        "\n",
        "model_type = r'hw4-ablations'\n",
        "sub_path = r'hw4-submission'\n",
        "model_file = 'model_40.pth'\n",
        "\n",
        "inference = SpeechInference()\n",
        "\n",
        "predictions = inference.simple_inference(model_name, model_file, model_type)\n",
        "submission_path = inference.generate_submission(sub_path, predictions)\n",
        "\n",
        "print(\"\\n\\nSubmission File : \", submission_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "qQQaBKeptUTW",
        "outputId": "55ea3e6f-b26b-4959-f472-deed354b8b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preview of submission.csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'HE BEGAN A CONFUSED COMPLAINT AGAINST THE WISTARD WHO ADVANISHED BEHIND THE CURT AND ON THE LEFT COMPLAINED AGAINST BEHIND THE CURT AND ON THE LEFT COMPLAINED AGAINST BEHIND THE CURT AND ON THE LEFT COMPLETE AGAINST BEHIND THE CURT AND ON THE LEFT COMPLETE AGAINST BEHIND THE CURT AND ON THE LEFT COMPLETE AGAINST THE WISTARD WHO ADVANCI BEHIND THE CURT AND ON THE LEFT COMPLAINED AGAINST BEHIND THE CURT AND ON THE LEFT COMPLETE AGAINST THE WISTARD WHO ADVANCI BEHIND THE CURT AND ON THE LEFT COMPLAINED AGAINST BEHIND THE CURT AND ON THE LEFT COMPLETE AGAINST THE WISTARD WHO ADVANCI BEHIND THE CUR'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Preview of submission.csv\")\n",
        "df = pd.read_csv(submission_path)\n",
        "df.head(10)\n",
        "df.iloc[0]['predictions']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXjEazPoXKcA",
        "outputId": "7dc2ff3c-953c-4949-e73e-c88f8e505c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 302k/302k [00:04<00:00, 65.1kB/s]\n",
            "Successfully submitted to Attention-Based Speech Recognition"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw4p2 -f $submission_path -m \"LAS-bigger\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMaMRHjBilAR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ufLFWkqEh-5e",
        "RBrQ8OHjW6qC",
        "i5ioyn6ldQB9",
        "I--VjKlEhwi8",
        "ZpSduz4H_f4O",
        "e_i4nnrZ_h0P",
        "kr2L3gvJ_vqH",
        "czdBv4XB_xHR",
        "DOUZJUE6_0zv",
        "r4ZO_vhADcMB"
      ],
      "machine_shape": "hm",
      "name": "HW4P2 Early",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae935ae5a1304c369411cd27d332cc28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c82be09816fa489da1dc4b8c953c4e45",
              "IPY_MODEL_d8d44907640c44b486b3e5bc089f5068"
            ],
            "layout": "IPY_MODEL_b03f8b339d584092a6bee34d41409536"
          }
        },
        "c82be09816fa489da1dc4b8c953c4e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e801130e0aa4ac0bd9b7a69776d6451",
            "placeholder": "​",
            "style": "IPY_MODEL_6d70b772d2a14060a5249201200f1232",
            "value": "0.281 MB of 0.281 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "d8d44907640c44b486b3e5bc089f5068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b50daa4515df4ec89ddc413943de3cba",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5694aa9ca2a84b85948a393910bf5cd8",
            "value": 1
          }
        },
        "b03f8b339d584092a6bee34d41409536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e801130e0aa4ac0bd9b7a69776d6451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d70b772d2a14060a5249201200f1232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b50daa4515df4ec89ddc413943de3cba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5694aa9ca2a84b85948a393910bf5cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}